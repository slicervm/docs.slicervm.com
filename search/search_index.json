{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction to Slicer","text":"<p>Slicer turns any machine into your own private cloud from a Raspberry Pi to a mini PC to a cloud server in minutes.</p> <p>It's ideal for learning &amp; experimentation - and powerful enough for R&amp;D, customer support, and even production workloads</p> <p>How we use Slicer at OpenFaaS Ltd</p> <ul> <li>To boot up a Kubernetes cluster in ~ 1min (large or small) to test new versions of Helm charts</li> <li>To reproduce customer support tickets - sometimes running multiple environments at once</li> <li>For introducing chaos testing into Kubernetes - where the network can be turned off whilst retaining a shell</li> <li>For R&amp;D with AI / LLM models in isolated environments</li> <li>Replacing slower cloud VMs with much faster hardware - desktop CPUs burst up to 5.5Ghz, have local NVMes</li> <li>Reducing cloud costs of R&amp;D/permanent environments - using mini PCs, self-built servers, and bare-metal in the cloud i.e. Hetzner</li> </ul> <p>Where can you run it?</p> <p>When commodity cloud vendors charge ~ 200 USD / mo for as little as 8vCPU and 32GB of RAM, working at scale becomes prohibitively expensive. You can obtain a brand new mini PC, or used server from eBay for a similar amount as a one-off cost. Or pay a bare-metal cloud provider like Hetzner Robot 33-100 EUR / mo for a powerful server for a public IP with reliable power and Internet. If you're tied to the cloud - Slicer also runs on DigitalOcean, Azure and GCP via nested virtualisation.</p> <p>Slicer is also fully functional on WSL2!</p> <p>Learn more in Alex's blog post Preview: Slice Up Bare-Metal with Slicer.</p> <p>See initial customer interest via this X/Twitter post</p> <p></p> <p>Slicer running on a Raspberry Pi 5 with NVMe. Click above to watch the video.</p>"},{"location":"#target-users-for-slicer","title":"Target users for Slicer","text":""},{"location":"#the-learner-the-experimenter-lets-make-vms-fun-again","title":"The learner, the experimenter - let's make VMs fun again","text":"<p>For the hobbyist, Slicer Home Edition is available at a low cost, making it accessible for personal projects and experimentation.</p> <p>Take that N100, that Raspberry Pi 5, that Beelink that DDH made you buy, that Dell PowerEdge in your basement, that Mac Mini M1 that's gathering dust, and make it into your lab.</p> <p>Slicer is probably the easiest, and most versatile way to learn about microVMs, virtualization and storage, whilst getting the most out of the hardware you already have your hands on.</p> <p>You can have fun, whilst learning and experimenting. You could even set up a permanent K3s cluster with K3sup Pro which you'll get as a free add-on.</p> <p>All at less than the cost of a weekly coffee.</p> <p>Think of Slicer like your own fast and private AWS EC2 region, without the surprise bill because you left something running.</p> <p>After running <code>slicer activate</code>, you'll get a link to join our Slicer Discord Server.</p> <p>Sign up now</p>"},{"location":"#the-power-user-and-technologist-of-the-team","title":"The power user and technologist of the team","text":"<p>For the power user or the technologist of the team, Slicer offers a quick and easy way to experiment with ideas and new technology in isolation, on fast hardware that you control.</p> <p>Whether you work on a Kubernetes product, support customers who do, or are looking for a way to start experimenting with agentic flows, LLMs with GPU acceleration, or a way to get the absolute most out of a large bare-metal host, Slicer will get you there and fast.</p> <p>The cost for commercial use starts at 250 USD / seat. A seat can be taken by a developer, or a production deployment on a server. For larger plans that scale for your needs, contact us and we can set up a demo.</p>"},{"location":"#use-cases","title":"Use-cases","text":""},{"location":"#super-fast-kubernetes-at-large-scale","title":"Super fast Kubernetes at large scale","text":"<p>Test at scale in minutes, not hours.</p> <p>We had to reproduce a customer support case in Kubernetes which only happened at the transition between 7000 to 7001 Pods. The time to create a 3-node cluster on AWS EKS is approximately 30 minutes, without even thinking about all those nodes. With Slicer, we reduced the time to testing to single digit minutes.</p> <p>What's more, the cluster can be destroyed near instantly, whilst CloudFormation hasn't even computed a plan yet.</p>"},{"location":"#chaos-testing-customer-support","title":"Chaos testing &amp; customer support","text":"<p>When writing Kubernetes operators, failure conditions are often overlooked. And no, you don't need a special framework on the CNCF landscape to do this.</p> <p>We had a controller that failed intermittently, and one day a customer gave us a log that pointed us in the right direction. The network was going down in AWS, and then its cached informers stopped - meaning it got no new events.</p> <p>We used Slicer's Serial Over SSH console to connect to the machine running the leader-elected Pod, and take down the network, whilst watching the logs of the container on disk. Within a couple of minutes the issue was patched, and a new release was shipped to the customer.</p>"},{"location":"#great-value-production","title":"Great value production","text":"<p>The first way we deployed Slicer into a permanent setup was by taking a Hetzner host with a 8-core AMD Ryzen and 64GB of RAM, and local NVMe. We created a HA, 3-node K3s cluster capable of running 300 Pods and deployed our long term testing environments to it, later adding production workloads SaaS like Inlets Cloud - running them there instead of creating myriad of costs in AWS.</p>"},{"location":"#gpu-powered-ai-and-agentic-workflows","title":"GPU-powered AI and agentic workflows","text":"<p>Slicer isn't tied to Firecracker. With the Cloud Hypervisor support, any kind of Nvidia GPU from a 3060 RTX, to a 3090 RTX, to an A100 can be used run local LLMs using Ollama, or a tool of your choice.</p> <p>Ephemeral VMs can be launched for agents via API, or command line - supplying a userdata script for bootstrap. You can then set up your own agent for command &amp; control.</p>"},{"location":"#better-than-misusing-containers","title":"Better than misusing containers","text":"<p>Docker is convenient and ubiquitous, it's totally the right option most of the time. But if you're having to give it all kinds of privileged flags just to make something work like Wireguard, or Docker In Docker, then you have the wrong tool for the job.</p> <p>A microVM gives you an isolated guest Kernel. You can even build new modules and load them in for R&amp;D, or for testing new frameworks based upon eBPF - all without compromising the host.</p> <p>Our second production instance of Slicer has three VMs in its group: 1 hosts a Ghost blog using Docker, 2 hosts the control-plane for Actuated along with all its requisite storage and state, 3 hosts an OpenFaaS Edge instance running in containerd. Everything is completely isolated and portable.</p>"},{"location":"#next-steps","title":"Next steps","text":"<ul> <li>Install Slicer and get started today.</li> <li>Read the launch blog post</li> <li>Get in touch for a commercial demo or order</li> </ul>"},{"location":"contact/","title":"Contact us","text":"<p>Slicer \u2122 is a trademark of OpenFaaS Ltd.</p>"},{"location":"contact/#for-your-team-or-organisation","title":"For your team or organisation","text":"<p>Would you like to contact us about Slicer for your team or organisation?</p> <p>Send an email to contact@openfaas.com</p>"},{"location":"contact/#discord-server-for-slicer-home-edition","title":"Discord Server for Slicer Home Edition","text":"<p>After running <code>slicer activate</code>, you'll get a link to join the Discord server for Slicer Home Edition.</p>"},{"location":"examples/buildkit/","title":"Remote Docker builds over SSH","text":"<p>BuildKit is a modern backend that powers Docker's build engine. This example shows how to set up a dedicated BuildKit instance in a Slicer VM for isolated, remote container builds.</p> <p>Note</p> <p>Whilst Slicer could be used to design a multi-tenant container builder this example is not intended for that use case.</p> <p>The setup in this example is intended for use by a developer for building images with a faster remote machine, or a different OS/architecture to their machine.</p> <p>Using a remote BuildKit instance in a Slicer VM provides several benefits:</p> <ul> <li>Isolation: Build processes are completely isolated from your host system</li> <li>Resource control: Dedicated CPU and memory resources for builds</li> <li>Security: Builds run in a sandboxed environment</li> <li>Flexibility: Easy to scale up resources or create multiple build environments</li> <li>Clean state: Each VM can be easily reset to a clean state</li> </ul> <p>This example is very minimal and covers the basic setup. You can expand it according to your needs. In the next steps we are going to:</p> <ul> <li>Install and configure BuildKit automatically on first boot with a userdate script.</li> <li>Set up Docker buildx to use the remote BuildKit instance.</li> </ul>"},{"location":"examples/buildkit/#vm-configuration","title":"VM configuration","text":"<p>The Slicer configuration is adapted from the walkthrough. When you create the YAML file, name it <code>buildkit.yaml</code>.</p> <p>We are going to use a userdata script to install and configure BuildKit on the first VM boot.</p> <p>Add the <code>userdata_file</code> to the hostgroup section:</p> <pre><code>  host_groups:\n  - name: buildkit\n    userdata_file: ./buildkit.sh\n</code></pre> <p>For better build performance, consider increasing the VM resources:</p> <pre><code>    vcpu: 4\n    ram_gb: 8\n    storage_size: 25G\n</code></pre> <p>Customize the <code>ssh_keys</code> or <code>github_user</code> fields so you can connect to the BuildKit instance over SSH. The Docker buildx remote driver supports connection to a remote BuildKit instance over SSH.</p>"},{"location":"examples/buildkit/#buildkit-installation-script","title":"BuildKit installation script","text":"<p>Create the <code>buildkit.sh</code> userdata script that will automatically install and configure BuildKit:</p> <pre><code>#!/usr/bin/env bash\n# BuildKit installation and configuration script\n# This script installs buildkitd, configures the buildkit group, and creates a systemd service\n\n#!/usr/bin/env bash\nset -euxo pipefail\n\n# Install buildkit\narkade system install buildkitd\n\n# Add a buildkit group\nsudo groupadd buildkit\n\n# Add ubuntu user to buildkit group\nsudo usermod -aG buildkit ubuntu\n\n# Systemd service for buidlkit (daemonized under systemd)\ncat &lt;&lt;'EOF' | sudo tee /etc/systemd/system/buildkitd.service &gt; /dev/null\n[Unit]\nDescription=BuildKit Daemon\nAfter=network.target\n\n[Service]\nType=simple\nExecStart=/usr/local/bin/buildkitd --addr unix:///run/buildkit/buildkitd.sock --group buildkit\nRestart=always\nUser=root\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsudo systemctl daemon-reload\nsudo systemctl enable --now buildkitd\n</code></pre>"},{"location":"examples/buildkit/#start-the-vm","title":"Start the VM","text":"<p>Start the VM with the following command:</p> <pre><code>sudo -E slicer up ./buildkit.yaml\n</code></pre>"},{"location":"examples/buildkit/#configure-docker-buildx","title":"Configure Docker buildx","text":"<p>Once your VM is running and BuildKit is installed, you can configure Docker buildx to use it as a remote builder. For more information about Docker builders, see the Docker builders documentation.</p>"},{"location":"examples/buildkit/#add-vm-to-known-hosts","title":"Add VM to known hosts","text":"<p>First, add the Slicer VM to your SSH known hosts to avoid connection issues:</p> <pre><code># Replace with your actual VM IP if different\nssh-keyscan 192.168.137.2 &gt;&gt; ~/.ssh/known_hosts\n</code></pre>"},{"location":"examples/buildkit/#create-the-remote-builder","title":"Create the remote builder","text":"<p>Create a new buildx builder instance that connects to your Slicer VM:</p> <pre><code># Create a new builder named 'slicer' using the remote driver\ndocker buildx create \\\n  --name slicer \\\n  --driver remote \\\n  ssh://ubuntu@192.168.137.2\n</code></pre>"},{"location":"examples/buildkit/#verify-the-builder","title":"Verify the builder","text":"<p>Check that your new builder is available and working:</p> <pre><code># List available builders\ndocker buildx ls\n\n# Inspect the slicer builder for detailed information\ndocker buildx inspect slicer\n</code></pre> <p>When buildx can successfully connect to the builder, the status should show as <code>running</code>:</p> <pre><code>slicer          remote\n \\_ slicer0      \\_ ssh://ubuntu@192.168.137.2    running   v0.24.0    linux/amd64 (+3), linux/386\n</code></pre> <p>The <code>inspect</code> command will show additional details about supported platforms, driver configuration, and connection status.</p>"},{"location":"examples/buildkit/#use-the-remote-builder","title":"Use the remote builder","text":"<p>Once your builder is configured and running, all builds executed with <code>--builder slicer</code> will run on the remote BuildKit instance instead of your local machine.</p> <p>Create a simple Dockerfile:</p> <pre><code>FROM alpine:3.19\n\nCMD [\"echo\", \"Hello, BuildKit!\"]\n</code></pre> <p>Build a container using your remote BuildKit instance:</p> <pre><code># Build and tag an image using the remote builder\ndocker buildx build \\\n  --builder slicer \\\n  -t hello-buildkit \\\n  .\n</code></pre> <p>The remote builder supports all BuildKit features including multi-platform builds, build secrets, cache mounts, and advanced Dockerfile syntax. Build outputs, logs, and any artifacts are handled seamlessly as if building locally, but with the security and resource isolation benefits of the dedicated VM.</p> <p>You can also set the remote builder as your default to avoid specifying <code>--builder</code> on every command:</p> <pre><code>docker buildx use slicer\n</code></pre>"},{"location":"examples/buildkit/#troubleshooting","title":"Troubleshooting","text":"<p>If you're having trouble connecting to the remote builder:</p> <ol> <li> <p>Check VM status: Ensure the VM is running and BuildKit service is active    <pre><code># SSH into the VM and check service status\nssh ubuntu@192.168.137.2\nsudo systemctl status buildkitd\n</code></pre></p> </li> <li> <p>Verify SSH connectivity: Test SSH connection directly    <pre><code>ssh ubuntu@192.168.137.2 \"echo 'SSH connection successful'\"\n</code></pre></p> </li> <li> <p>Check BuildKit socket: Verify the socket is accessible    <pre><code>ssh ubuntu@192.168.137.2 \"sudo -u ubuntu -g buildkit buildctl --addr unix:///run/buildkit/buildkitd.sock debug info\"\n</code></pre></p> </li> </ol>"},{"location":"examples/buildkit/#further-thoughts","title":"Further thoughts","text":"<p>This was a basic example to demonstrate how to run BuildKit isolated in a Slicer VM. Some ideas to explore further:</p> <ul> <li>TCP connection: Run BuildKit over TCP with mTLS in the VM to avoid SSH key management</li> <li>Ephemeral VMs: Use the Slicer REST API to provision temporary VMs for each build, then destroy them.</li> <li>BuildKit pools: Create pools of BuildKit instances for native multi-platform builds (avoiding QEMU emulation) and shared persistent caching.</li> </ul> <p>The isolated nature of microVMs makes this approach particularly well-suited for enterprise build infrastructure where security, resource control, and clean environments are priorities.</p>"},{"location":"examples/cursor-cli-agent/","title":"Cursor CLI Agent","text":"<p>Cursor's CLI can be used in a non-interactive way within a script or CI job. That makes it ideal for use as a one-shot task with a SlicerVM, or a longer running VM that you connect to with a remote VSCode IDE.</p> <p>Using this flow, you could create a powerful agent that uses your existing Cursor subscription, running with the \"auto\" model so it remains within the free tier.</p> <p>Example use-cases:</p> <ul> <li>Web scraping, data extraction and analysis</li> <li>Code or content generation</li> <li>Analyze user-submitted content such as code, comments, or documents</li> <li>Image or video conversion</li> <li>Chat bots, and webhook receivers / responders - i.e. from GitHub, GitLab, Discord, Slack, etc.</li> </ul> <p>Running within a microVM means you can run the agent in a secure, isolated environment, and you can easily discard the VM when done.</p> <p>Note: Overall, Cursor's CLI has some rough edges, and you may prefer to use opencode, which at time of writing worked better headless and was more polished.</p> <p>The CLI may require an initial interactive session to enable MCP usage and to \"trust\" the working directory.</p> <p>The CLI can also hang indefinitely after responding to a prompt, even in --print mode.</p> <p>The CLI seems to have no practical way to read a prompt from a file or a stdin pipe, so you have to use <code>$(cat prompt.txt)</code> which can end up reading subsequent commands into the prompt.</p>"},{"location":"examples/cursor-cli-agent/#quick-overview","title":"Quick overview","text":"<ul> <li>Create a Cursor API key and save it for use with the CLI.</li> <li>Create a userdata script to pre-install the CLI, Playwright for accessing the web, and your Cursor API key.</li> <li>Boot up the VM, optionally, collect the results from the logs or from the disk via SCP or by mounting the disk image after shutdown.</li> </ul>"},{"location":"examples/cursor-cli-agent/#userdata-script","title":"Userdata script","text":"<p>Populate the prompt section, and the API key with your own inputs.</p> <p>Save as cursor.sh:</p> <pre><code>#!/usr/bin/env bash\nset -euo pipefail\n\nTARGET_USER=ubuntu\n\n# If we're root, re-exec this script as $TARGET_USER (login shell so HOME/env are correct)\nif [[ $EUID -eq 0 ]]; then\n  if ! id -u \"$TARGET_USER\" &gt;/dev/null 2&gt;&amp;1; then\n    echo \"User '$TARGET_USER' does not exist\" &gt;&amp;2\n    exit 1\n  fi\n\n  # Option A: run the file path directly (keeps $0 correct; requires the user can read the file)\n  # exec sudo -u \"$TARGET_USER\" -i bash \"$0\" \"$@\"\n\n  # Option B: stream the script via stdin (works even if the file isn't readable by $TARGET_USER)\n  exec sudo -u \"$TARGET_USER\" -i bash -s -- \"$@\" &lt; \"$0\"\nfi\n\n# From here on, we are the ubuntu user\necho \"Running as $(whoami), HOME=$HOME\"\n# cd to HOME\ncd\n\ncurl https://cursor.com/install -fsS | sudo -E bash\n\nmkdir -p ~/.cursor/\n\necho 'export PATH=\"$HOME/.local/bin:$PATH\"' &gt;&gt; ./.bashrc\n\n# Set your CURSOR_API_KEY below - obtained from: https://cursor.com/dashboard?tab=background-agents\necho \"export CURSOR_API_KEY=\" | tee -a ~/.bashrc\n\n# Install playwright mcp tool for browsing web / fetching content\n# These steps could be built into a custom Slicer image to speed up boot time.\nsudo -E arkade system install node\nnpm i -D @playwright/test\nnpm i -D @playwright/mcp@latest\n\nnpx playwright install --with-deps chromium\n#npx playwright install --with-deps webkit\n\ncat &lt;&lt; EOF &gt; ~/.cursor/mcp.json\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@playwright/mcp@latest\",\n        \"--browser\", \"chromium\",\n        \"--headless\"\n        ]\n    }\n  }\n}\n\nEOF\n\ncat &lt;&lt; EOF &gt; ~/.cursor/cli-config.json\n{\n  \"permissions\": {\n    \"allow\": [\n      \"*\"\n    ],\n    \"deny\": []\n  }\n}\n\nEOF\n\nmkdir -p ~/.cursor/projects/home-ubuntu/\n\ncat &lt;&lt; EOF &gt; ~/.cursor/projects/home-ubuntu/mcp-approvals.json\n[\n  \"playwright-aced25c5e5b87b1c\"\n]\n\nEOF\n\nmkdir -p ~/.cache/ms-playwright/\n\ncat &lt;&lt; EOF &gt; ~/prompt.txt\nVisit the slicervm.com website and generate a JSON summary of the key elements, value-proposition,\nand target audience. Use playwright-mcp, this has been pre-installed and is ready for your use.\nEOF\n\nsource ~/.bashrc\n\n# Belt and braces..\nsudo chown -R \"$USER:$USER\" \"$HOME\"\n\ncursor-agent mcp list\ncursor-agent mcp list-tools playwright\n\ntime cursor-agent --model auto --force --output-format=text --print \"$(cat ./prompt.txt)\" &gt; ~/cursor.log\n</code></pre> <p>Note: at time of writing, a <code>mcp-approvals.json</code> file had to be created to get the MCP tool calls to work. Hopefully the cursor team will negate this requirement for headless use in a future release.</p> <p>The approval file is not required if you do not use MCP tools.</p>"},{"location":"examples/cursor-cli-agent/#config-file","title":"Config file","text":"<p>Save the config file from the walkthrough.</p> <p>Then add the userdata field within the hostgroup:</p> <pre><code>config:\n  hostgroups:\n    - name: vm\n+     userdata: ./cursor.sh\n</code></pre> <p>Save the resulting file as <code>cursor.yaml</code>.</p>"},{"location":"examples/cursor-cli-agent/#give-it-a-test-run","title":"Give it a test run","text":"<p>Now run:</p> <pre><code>sudo slicer up ./cursor.yaml\n</code></pre>"},{"location":"examples/cursor-cli-agent/#taking-it-further","title":"Taking it further","text":"<p>You can learn more about the Cursor CLI here and its headless usage here</p> <p>If your agent doesn't need to browse the web, then you can speed up the boot by removing the Playwright instructions.</p> <p>If you have additional MCP servers, you can add them into the userdata and ~/.cursor/mcp.json file.</p> <p>To make the system boot up even quicker, you can derive your own custom image with the CLI and playwright pre-installed. Then the userdata will only be used to set up the API key, and to provide the prompt.</p> <p>Finally, if you are automating access, you could automate the workflow by creating a microVM via the REST API, and using <code>scp</code> to collect the generated code or results from the agent.</p>"},{"location":"examples/docker/","title":"Run a Docker container in Slicer","text":"<p>There are three ways to try out a container or Docker within Slicer:</p> <ol> <li>Start your VM as per the walkthrough, then connect via <code>ssh</code> and install Docker inside the VM.</li> <li>Use Userdata to install Docker and pull your image on first boot.</li> <li>Create a custom base image with Docker pre-installed, and your image pre-pulled, with a systemd service to start it on every boot.</li> </ol> <p>On this page we'll cover options 2 and 3.</p> <p>Option 2 is the easiest to use, and most portable. The boot up time will be delayed whilst your userdata runs. In option 3, that time is spent on the host once, rather than on the first boot of each VM.</p> <p>We'll show you a generic approach here, but you can adapt it to your needs, or use another tool like nerdctl or Podman instead of Docker.</p> <p>The container we're going to use is a Docker Registry, but likewise you could run your own applications, a database, Ollama, Grafana, or any other container.</p>"},{"location":"examples/docker/#install-docker-and-a-container-via-userdata","title":"Install Docker and a container via Userdata","text":"<p>You can use Userdata to install Docker and pull your image on first boot.</p> <p>The below is only a partial snippet to show you the relevant changes:</p> <pre><code>config:\n    host_groups:\n    - name: vm\n      userdata: |\n        #!/bin/bash\n        # Install Docker\n        curl -fsSL https://get.docker.com | sh\n\n        # Add user to docker group\n        usermod -aG docker ubuntu\n\n        docker run -d -p 5000:5000 --restart=always --name registry registry:3\n</code></pre>"},{"location":"examples/docker/#create-a-custom-base-image-for-a-docker-image","title":"Create a custom base image for a Docker image","text":"<p>Create a one-shot systemd service to run your container on boot-up.</p> <p>Call this file <code>docker-registry.service</code>:</p> <pre><code>[Unit]\nDescription=Run a Docker registry\nAfter=docker.service\nRequires=docker.service\nWants=network-online.target\nAfter=network-online.target\nStartLimitIntervalSec=0\nStartLimitBurst=3\n\n[Service]\nType=oneshot\nRemainAfterExit=yes\nExecStart=/usr/bin/docker run -d -p 5000:5000 --restart=always --name registry registry:3\nTimeoutStartSec=0\nRestart=on-failure\nRestartSec=10s\nStandardOutput=journal\nStandardError=journal\nSyslogIdentifier=docker-registry\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <pre><code>FROM ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-latest\n\nRUN curl -fsSL https://get.docker.com | sh &amp;&amp; \\\n    usermod -aG docker ubuntu\n\nCOPY docker-registry.service /etc/systemd/system/docker-registry.service\n\nRUN systemctl enable docker-registry\n</code></pre> <p>Then build the image, and push it to a registry.</p> <p>Next, customise your <code>config.yaml</code> to use your new image:</p> <pre><code>config:\n   image: \"docker.io/alexellis2/slicer-docker-registry:latest\"\n</code></pre> <p>In order to pre-pull the container image during the Docker build, you'd have to use crane to export the image to a local tarball, and then to import that during userdata. This is because you can't run <code>docker pull</code> during a Docker build.</p>"},{"location":"examples/docker/#try-out-your-container","title":"Try out your container","text":"<p>If you deployed a Docker registry, you can push and pull images to it from your host.</p> <pre><code>docker pull alpine\ndocker tag alpine 192.168.137.1:5000/alpine\ndocker push 192.168.137.1:5000/alpine\n</code></pre> <p>You can use this custom registry for your Slicer images too, by adding <code>insecure_registry: true</code> to the <code>config</code> section of your VM's YAML file.</p>"},{"location":"examples/gpu-ollama/","title":"Run a microVM with a GPU mounted for Ollama","text":"<p>Ollama can run in any microVM using its CPU, however a GPU is the best option for a high token throughput and faster response times.</p> <p>You'll need a PC with VFIO support, this allows a PCI device such as a GPU to be passed through to the microVM for exclusive access.</p> <p>What if you have multiple GPUs? Let's imagine you have a ATX tower PC with 2x Nvidia RTX 3090 or 3060 GPUs.</p> <ul> <li>Allocate both GPUs to one machine</li> <li>Allocate each GPU to its own microVM</li> <li>Start up with zero microVMs and launch up to two short-lived tasks at once</li> </ul> <p></p> <p>Ollama running the qwen3 model to generate a story about a microVM's first day at school.</p> <p>GPU / VFIO mounting only works with Slicer running on <code>x86_64</code> at present.</p>"},{"location":"examples/gpu-ollama/#set-up-your-vm-configuration","title":"Set up your VM configuration","text":"<p>There a three differences to the other examples we've seen so far:</p> <ol> <li><code>gpu_count: N</code> is added to the hostgroup, N is a the number of GPUs to allocate to each VM</li> <li><code>hypervisor: cloud-hypervisor</code> - Cloud Hypervisor is used instead of Firecracker, to enable VFIO passthrough of a PCI device</li> <li><code>image</code> - a separate Kernel and root filesystem is required for Cloud Hypervisor</li> </ol> <p>The following to <code>gpu.yaml</code>, make sure you update <code>vcpu</code> and <code>ram_gb</code></p> <pre><code>config:\n  host_groups:\n  - name: gpu\n    storage: image\n    storage_size: 80G\n    count: 1\n    vcpu: 16\n    ram_gb: 64\n    gpu_count: 1\n    network:\n      bridge: brgpu0\n      tap_prefix: gputap\n      gateway: 192.168.137.1/24\n\n  github_user: alexellis\n\n  image: \"ghcr.io/openfaasltd/slicer-systemd-ch:5.10.240-x86_64-latest\"\n\n  hypervisor: cloud-hypervisor\n</code></pre> <p>Boot the VM(s) with:</p> <pre><code>sudo -E slicer up ./ollama-gpu.yaml\n</code></pre> <p>Note: adding PCI devices will add a boot delay of a few seconds vs. microVMs without any PCI devices. To monitor this, run <code>ping</code> in the background or <code>sudo fstail /var/log/slicer/</code> to see when the dmesg messages start appearing. </p> <p>Then, as usual, add the route on your workstation so you can connect via SSH.</p> <pre><code>ssh ubuntu@192.168.137.2\n</code></pre> <p>View the PCI devices:</p> <pre><code>$ lspci\n00:00.0 Host bridge: Intel Corporation Device 0d57\n00:01.0 Unassigned class [ffff]: Red Hat, Inc. Virtio console (rev 01)\n00:02.0 Mass storage controller: Red Hat, Inc. Virtio block device (rev 01)\n00:03.0 Mass storage controller: Red Hat, Inc. Virtio block device (rev 01)\n00:04.0 Ethernet controller: Red Hat, Inc. Virtio network device (rev 01)\n00:05.0 Unassigned class [ffff]: Red Hat, Inc. Virtio RNG (rev 01)\n00:06.0 VGA compatible controller: NVIDIA Corporation GA102 [GeForce RTX 3090] (rev a1)\n00:07.0 Audio device: NVIDIA Corporation GA102 High Definition Audio Controller (rev a1)\n00:08.0 VGA compatible controller: NVIDIA Corporation GA102 [GeForce RTX 3090] (rev a1)\n00:09.0 Audio device: NVIDIA Corporation GA102 High Definition Audio Controller (rev a1)\n</code></pre> <p>You can see that I mounted 2x Nvidia RTX 3090 GPUs into this microVM.</p> <p>You can install the Nvidia drivers using our utility script:</p> <pre><code>curl -SLsO https://raw.githubusercontent.com/self-actuated/nvidia-run/refs/heads/master/setup-nvidia-run.sh\nchmod +x ./setup-nvidia-run.sh\nsudo bash ./setup-nvidia-run.sh\n</code></pre> <p>Compilation can take a minute or two, but can be sped up by caching all changed files and untaring them over the top of the root filesystem in userdata, or by building a custom VM image with the generated tar expanded.</p> <p>If you run into an error, you can edit the script and uncomment the line <code>--no-unified-memory</code>.</p> <p>Other commands to start a custom agent, or to install frameworks/tools can be added easily via userdata directly within the YAML.</p> <p>Check the status of the driver with <code>nvidia-smi</code>:</p> <pre><code>ubuntu@gpu-1:~$ nvidia-smi\n\nMon Sep  1 11:28:45 2025\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 580.76.05              Driver Version: 580.76.05      CUDA Version: 13.0     |\n+-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 3090        Off |   00000000:00:06.0 Off |                  N/A |\n| 30%   40C    P0            108W /  350W |       0MiB /  24576MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA GeForce RTX 3090        Off |   00000000:00:08.0 Off |                  N/A |\n| 30%   32C    P0             99W /  350W |       0MiB /  24576MiB |      2%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n</code></pre> <p>Then install Ollama:</p> <pre><code>curl -fsSL https://ollama.com/install.sh | sh\n</code></pre> <p>Next, pull a model and try out a prompt:</p> <pre><code>ollama run qwen3:latest\n</code></pre> <p>You can also connect to the Ollama API from your host machine by using the VM's IP directly:</p> <pre><code>curl -SLs http://192.168.137.2:11434/api/generate -d '{\n  \"model\": \"qwen3:latest\",\n  \"prompt\":\"Why is the sky blue?\"\n}'\n</code></pre> <p>Since we're using a persistent disk image, any models you download will be available if you restart or shutdown Slicer.</p>"},{"location":"examples/ha-k3s/","title":"Highly Available (HA) Kubernetes with K3s","text":"<p>The following example sets up a 3x Node Kubernetes cluster using K3s. As an optional step, you can set up a Load Balancer running on the Slicer host to distribute traffic across the nodes for the API server and HTTP/HTTPS.</p> <p>If you would like to try out a smaller cluster first, you can simply change the <code>count</code> from <code>3</code> to <code>1</code> when saving the file below.</p> <p>Create <code>k3s-ha.yaml</code>:</p> <pre><code>config:\n  host_groups:\n  - name: k3s\n    storage: image\n    storage_size: 25G\n    count: 3\n    vcpu: 2\n    ram_gb: 4\n    network:\n      bridge: brk3s0\n      tap_prefix: k3stap\n      gateway: 192.168.137.1/24\n\n  github_user: alexellis\n\n  api:\n    port: 8080\n    bind_address: \"127.0.0.1:\"\n\n\n# Comment out the image depending on your system's architecture\n\n  # image: \"ghcr.io/openfaasltd/slicer-systemd-arm64:6.1.90-aarch64-latest\"\n  image: \"ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-latest\"\n\n  hypervisor: firecracker\n</code></pre> <p>The IP addresses of the VMs will be as follows:</p> <ul> <li><code>192.168.137.2</code></li> <li><code>192.168.137.3</code></li> <li><code>192.168.137.4</code></li> </ul>"},{"location":"examples/ha-k3s/#setup-kubernetes-with-k3sup-pro","title":"Setup Kubernetes with K3sup Pro","text":"<p>Download K3sup Pro:</p> <pre><code>curl -sSL https://get.k3sup.dev | PRO=true sudo -E sh\n</code></pre> <p>If you want to leave off <code>sudo</code>, then just move the <code>k3sup-pro</code> binary into your <code>$PATH</code> variable manually.</p> <p>Next, on the host where slicer is running, get the devices file from Slicer's API:</p> <pre><code>curl -sLS http://127.0.0.1:8080/nodes &gt; devices.json\n</code></pre> <p>Copy devices.json back to your workstation.</p> <p>On your workstation, add any routes that are specified so you can access the VMs on their own network.</p> <p>Check the options like disabling Traefik, so that you can install Ingress Nginx or Istio instead.</p> <pre><code>k3sup-pro plan --help\n\nk3sup-pro plan --traefik=false --user ubuntu\n</code></pre> <p>This will generate a plan.yaml file, you can review and edit it.</p> <p>Next, run <code>k3sup-pro apply</code>.</p> <p>This will install the first server, then server 2 and 3 in parallel.</p> <p>Finally run:</p> <pre><code>mkdir -p ~/.kube\ncp ~/.kube/config ~/.kube/config.bak || true\n\nk3sup-pro get-config \\\n --local-path ~/.kube/config \\\n --merge \\\n --context slicer-k3s-ha\n</code></pre> <p>Then you can run <code>kubectx slicer-k3s-ha</code>, and start using kubectl.</p> <p>Your cluster is running in HA mode.</p>"},{"location":"examples/ha-k3s/#create-a-ha-loadbalancer-for-the-vms","title":"Create a HA LoadBalancer for the VMs","text":"<p>If you would like to create a load balancer for the microVMs, you can do so using the mixctl add-on.</p> <pre><code>arkade get mixctl\n</code></pre> <p>Create a config named <code>k3s.yaml</code>:</p> <pre><code>version: 0.1\n\nrules:\n- name: k3s-api\n  from: 127.0.0.1:6443\n  to:\n    - 192.168.137.2:6443\n    - 192.168.137.3:6443\n    - 192.168.137.4:6443\n\n- name: k3s-http\n  from: 127.0.0.1:80\n  to:\n    - 192.168.137.2:80\n    - 192.168.137.3:80\n    - 192.168.137.4:80\n\n- name: k3s-tls\n  from: 127.0.0.1:443\n  to:\n    - 192.168.137.2:443\n    - 192.168.137.3:443\n    - 192.168.137.4:443\n</code></pre> <p>Then run <code>mixctl ./k3s.yaml</code></p> <p>Finally, revisit your plan so each server obtains a TLS certificate for the Kubernetes API server for the IP address of the Slicer host.</p> <p>So if the Slicer host were 192.168.1.100:</p> <pre><code>k3sup-pro plan --tls-san 192.168.1.100 \\\n  --update\n\nk3sup-pro apply\n</code></pre> <p>Then edit your <code>~/.kube/config</code> file and replace <code>192.168.137.2:6443</code> with <code>192.168.1.100:6443</code>.</p> <p>Now every time you run <code>kubectl</code>, you'll see mixctl balance traffic across all three servers.</p>"},{"location":"examples/k3s-gpu/","title":"Kubernetes with GPUs","text":"<p>In this example, we'll adapt elements of the HA Kubernetes example and the GPU Ollama setup to work together, so you can launch Pods with GPU acceleration.</p>"},{"location":"examples/k3s-gpu/#set-up-a-config","title":"Set up a config","text":"<p>Create the <code>k3s-gpu.yaml</code> file as below:</p> <pre><code>config:\n  host_groups:\n  - name: gpu\n    storage: image\n    storage_size: 30G\n    count: 1\n    vcpu: 4\n    ram_gb: 16\n    gpu_count: 1\n    network:\n      bridge: brgpu0\n      tap_prefix: gputap\n      gateway: 192.168.139.1/24\n\n  github_user: alexellis\n\n  image: \"ghcr.io/openfaasltd/slicer-systemd-ch:5.10.240-x86_64-latest\"\n\n  hypervisor: cloud-hypervisor\n</code></pre> <p>Feel free to customise the vCPU, RAM, and disk sizes.</p> <p>Boot up the VM:</p> <pre><code>sudo -E slicer up ./k3s-gpu.yaml\n</code></pre> <p>Now, run the route commands so you can SSH into the host from your workstation.</p> <p>Next, log into each VM via SSH:</p> <pre><code>ssh ubuntu@192.168.139.2\n</code></pre> <p>Next, install K3s using K3sup Pro or K3sup CE.</p> <p>For CE:</p> <pre><code>k3sup install --host 192.168.139.2 --user ubuntu\n</code></pre> <p>You'll get a kubeconfig returned, run the commands to export it so kubectl uses it.</p> <p>Install the Nvidia driver:</p> <pre><code>curl -SLsO https://raw.githubusercontent.com/self-actuated/nvidia-run/refs/heads/master/setup-nvidia-run.sh\nchmod +x ./setup-nvidia-run.sh\nsudo bash ./setup-nvidia-run.sh\n</code></pre> <p>Install the Nvidia Container Toolkit using the official instructions. Use the instructions for \"apt: Ubuntu, Debian\".</p> <p>Confirm that the nvidia container runtime has been found by K3s:</p> <pre><code>sudo grep nvidia /var/lib/rancher/k3s/agent/etc/containerd/config.toml\n</code></pre> <p>Apply a new runtime class for Nvidia:</p> <pre><code>cat &gt; nvidia-runtime.yaml &lt;&lt;EOF\napiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: nvidia\nhandler: nvidia\nEOF\n\nkubectl create -f nvidia-runtime.yaml\n</code></pre> <p>Run a test Pod to show the output from nvidia-smi:</p> <pre><code>cat &gt; nvidia-smi-pod.yaml &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nvidia-smi\nspec:\n  runtimeClassName: nvidia\n  restartPolicy: OnFailure\n  containers:\n    - name: nvidia-smi\n      image: nvidia/cuda:12.1.0-base-ubuntu22.04\n      command: ['sh', '-c', \"nvidia-smi\"]\nEOF\n\nkubectl create -f nvidia-smi-pod.yaml\n</code></pre> <p>For the time-being, this Pod uses only the <code>runtimeClassName</code> to request a GPU. Adding the usual <code>limits</code> section as below, does not work at present, and may require additional configuration in K3s or containerd:</p> <pre><code>+     resources:\n+       limits:\n+           nvidia.com/gpu: \"1\"\n</code></pre> <p>Fetch the logs:</p> <pre><code>kubectl logs nvidia-smi\n</code></pre> <pre><code>$ kubectl logs pod/nvidia-smi\nMon Sep  1 15:04:11 2025\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 580.76.05              Driver Version: 580.76.05      CUDA Version: 13.0     |\n+-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 3090        Off |   00000000:00:07.0 Off |                  N/A |\n| 30%   46C    P0            110W /  350W |       0MiB /  24576MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"examples/k3s-gpu/#enable-device-plugin","title":"Enable Device Plugin","text":"<p>Install Nvidia's Device Plugin for Kubernetes:</p> <pre><code>kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.17.1/deployments/static/nvidia-device-plugin.yml\n</code></pre> <p>Patch it so it works with K3s:</p> <pre><code># add runtimeClassName: nvidia to the DS pod spec\nkubectl -n kube-system patch ds nvidia-device-plugin-daemonset \\\n  --type='json' \\\n  -p='[{\"op\":\"add\",\"path\":\"/spec/template/spec/runtimeClassName\",\"value\":\"nvidia\"}]'\n\nkubectl -n kube-system rollout status ds/nvidia-device-plugin-daemonset -n kube-system\n</code></pre> <p>Then run the Pod from earlier, but with the <code>limits</code> in place:</p> <pre><code>cat &gt; nvidia-smi-pod.yaml &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nvidia-smi\nspec:\n  runtimeClassName: nvidia\n  restartPolicy: OnFailure\n  containers:\n    - name: nvidia-smi\n      image: nvidia/cuda:12.1.0-base-ubuntu22.04\n      command: ['sh', '-c', \"nvidia-smi\"]\n      resources:\n        limits:\n            nvidia.com/gpu: \"1\"\nEOF\n\nkubectl create -f nvidia-smi-pod.yaml\n</code></pre>"},{"location":"examples/large-scale-k3s/","title":"Large Scale Kubernetes (k3s)","text":"<p>Since the default limit for Kubernetes is around 100 Pods per node, installing Kubernetes directly to a host will not allow for large scale deployments.</p> <p>This blog post is intended for creating large clusters for testing/experimentation, R&amp;D, and customer support purposes.</p> <p>It uses a snapshotting filesystem with CoW, so that only a minimal amount of disk space needs to be allocated, and launch times are kept very fast.</p> <p>You can of course install any Kubernetes distribution in two ways:</p> <ul> <li>Via SSH (K3sup Pro, Ansible, etc)</li> <li>Via a userdata script written in bash</li> </ul> <p>Since K3sup Pro runs in parallel, is designed to work with Slicer's API, and is included for free for Slicer customers, we'll use it here for a speedy installation.</p> <p>Video demo of setting up a large scale K3s cluster:</p>"},{"location":"examples/large-scale-k3s/#setup-the-vm-configuration","title":"Setup the VM configuration","text":"<p>Consider the target hardware, and the specification you require for your nodes.</p> <p>If your system like ours has a 96-core Arm processor, with 196GB of RAM, you could split it up in any number of ways.</p> <p></p> <p>The Adlink Ampere Developer Platform loaded up with 96 cores and 196GB RAM.</p> <ul> <li>12 nodes with 8 cores and 16GB RAM each</li> <li>24 nodes with 4 cores and 8GB RAM each</li> <li>32 nodes with 3 cores and 6GB RAM each</li> <li>48 nodes with 2 cores and 4GB RAM each</li> <li>96 nodes with 1 core and 2GB RAM each</li> </ul> <p>Even if you don't have a large machine like this, you could run Slicer on multiple machines to get to a similar size.</p> <p>Let's go for the mid-ground with 24 nodes.</p> <p>Create <code>k3s-scale.yaml</code>:</p> <pre><code>config:\n  host_groups:\n  - name: k3s\n    storage: zfs\n    persistent: true\n    count: 24\n    vcpu: 4\n    ram_gb: 8\n    network:\n      bridge: brscale0\n      tap_prefix: sctap\n      gateway: 192.168.137.1/24\n\n  github_user: alexellis\n\n  api:\n    port: 8080\n    bind_address: \"127.0.0.1:\"\n\n# Comment out the image depending on your system's architecture\n\n  image: \"ghcr.io/openfaasltd/slicer-systemd-arm64:6.1.90-aarch64-latest\"\n  # image: \"ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-latest\"\n\n  hypervisor: firecracker\n</code></pre> <p>If you need to run the same test with a fresh cluster over and over again, you can change the <code>persistent: true</code> configuration to <code>persistent: false</code>. Then the snapshots will be discarded when Slicer exits. Otherwise, they are retained indefinitely.</p> <p>Now launch Slicer in a tmux window, so you can come back to it later:</p> <pre><code>tmux new-session -s slicer\n\nsudo slicer up ./k3s-scale.yaml\n</code></pre> <p>The Root filesystem will be downloaded and unpacked, then all the VMs will be launched.</p>"},{"location":"examples/large-scale-k3s/#install-kubernetes-with-k3sup-pro","title":"Install Kubernetes with K3sup Pro","text":"<p>Access the API on the server to download the devices file:</p> <pre><code>curl -LsS http://127.0.0.1:8080/devices -o devices.json\n</code></pre> <p>On your workstation, add any routes that are specified so you can access the VMs on their own network.</p> <p>Download the devices.json file to your local computer, and install K3sup Pro:</p> <pre><code>curl -sSL https://get.k3sup.dev | PRO=true sudo -E sh\n</code></pre> <p>Next, create a K3sup Pro plan using the devices file. Simply run the command in the same folder as <code>devices.json</code>.</p> <p>Pick an amount of nodes to dedicate to being servers, the rest will be agents.</p> <p>Always use an odd number, with a minimum of three. In K3s, servers can also run workloads by default.</p> <pre><code>k3sup pro plan \\\n  --servers 3 \\\n  --traefik=false \\\n  --user ubuntu\n</code></pre> <p>The plan command creates or updates a plan.yaml file in the local directory. You can view it or edit it.</p> <p>Apply the plan, the first server is created, then all other nodes are added in parallel based upon the <code>--parallel</code> flag.</p> <pre><code>k3sup pro apply --parallel 8\n</code></pre> <p>After a short period of time, your cluster will be ready.</p>"},{"location":"examples/large-scale-k3s/#get-access-to-the-kubeconfig","title":"Get access to the kubeconfig","text":"<p>Merge it into your KUBECONFIG file:</p> <pre><code>mkdir -p ~/.kube\ncp ~/.kube/config ~/.kube/config.bak || true\n\nk3sup-pro get-config \\\n --local-path ~/.kube/config \\\n --merge \\\n --context slicer-k3s-scale\n</code></pre> <p>Next switch into the context and install something to try out the cluster, like OpenFaaS CE.</p> <pre><code>arkade get kubectx\nkubectx slicer-k3s-scale\n\nkubectl get nodes -o wide\nkubectl top pod\n\narkade install openfaas\n</code></pre>"},{"location":"examples/multiple-machine-k3s/","title":"Multiple machine Kubernetes","text":"<p>In the HA Kubernetes and Large scale Kubernetes examples, we showed a single Kubernetes cluster running on a single Slicer host.</p> <p>In this example, we'll show you spread Kubernetes clusters over multiple machines, each running Slicer.</p> <ol> <li>You want a multi-arch Kubernetes cluster - with Slicer on an x86_64 and Arm64 machine</li> <li>You want a much larger cluster than you can fit on any single machine</li> </ol> <p>The basic idea is that you run Slicer on two or more machines, which manages its own set of VMs. These are put on distinct subnets to avoid IP address conflicts, then a rule is added to the routing table to enable inter-connectivity. This isn't limited to just Kubernetes, you could run any network-based product or service you like across multiple machines with this technique.</p> <p></p> <p>If I need a very large Kubernetes cluster I'll often run the control-plane on an x86_64 Ryzen 9 7900X machine, and the worker nodes on an Arm64 Ampere Altra machine like the Adlink AADP pictured above. The machine above has 96 Arm cores and 196GB RAM.</p>"},{"location":"examples/multiple-machine-k3s/#install-slicer-on-each-machine","title":"Install Slicer on each machine","text":"<p>First, install Slicer on each machine, following the installation instructions.</p> <p>For storage, we'll use the <code>image</code> setting, however if you're going to create many nodes, consider using ZFS for an instant clone of the VM filesystem, and reduced disk space consumption through ZFS's snapshots and Copy On Write (CoW) feature.</p>"},{"location":"examples/multiple-machine-k3s/#create-a-yaml-file-for-each-machine","title":"Create a YAML file for each machine","text":"<p>You need to dedicate one subnet per machine, so that there are no IP address conflicts.</p> <p>Then, you should ideally change the <code>name:</code> prefix of the host group, so that the machines have unique names.</p> <p>The first machine could use:</p> <pre><code>config:\n  host_groups:\n  - name: k3s-a\n    storage: image\n    storage_size: 25G\n    count: 3\n    vcpu: 2\n    ram_gb: 4\n    network:\n      bridge: brk3s0\n      tap_prefix: k3stap\n      gateway: 192.168.137.1/24\n\n  github_user: alexellis\n\n  api:\n    port: 8080\n    bind_address: \"127.0.0.1:\"\n\n  image: \"ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-latest\"\n\n  hypervisor: firecracker\n</code></pre> <p>Then the second would have a different <code>name:</code> prefix, and a different network section.</p> <pre><code>config:\n  host_groups:\n  - name: k3s-b\n    network:\n      bridge: brk3s0\n      tap_prefix: k3stap\n      gateway: 192.168.138.1/24\n</code></pre> <p>(Note the above is a partial snippet to show the differences only.)</p>"},{"location":"examples/multiple-machine-k3s/#start-slicer-on-each-machine","title":"Start Slicer on each machine","text":"<p>On each respective machine run:</p> <pre><code>sudo -E slicer up ./k3s-a.yaml\n</code></pre> <pre><code>sudo -E slicer up ./k3s-b.yaml\n</code></pre>"},{"location":"examples/multiple-machine-k3s/#enable-routing-between-the-machines-and-vms","title":"Enable routing between the machines and VMs","text":"<p>Now, you need to be careful here.</p> <p>Copy and paste the routing commands printed upon start-up.</p> <ol> <li>On machine A, run the command from machine B</li> <li>On machine B, run the command from machine A</li> <li>On your workstation, which needs to access all of the VMs, run both commands</li> </ol>"},{"location":"examples/multiple-machine-k3s/#install-kubernetes-with-k3sup-pro","title":"Install Kubernetes with K3sup Pro","text":"<p>On each host run the following:</p> <pre><code>curl -SLsf http://127.0.0.1:8080/nodes &gt; devices-a.json\n\ncurl -SLsf http://127.0.0.1:8080/nodes &gt; devices-b.json\n</code></pre> <p>If you enabled authentication for the API, include the <code>-H \"Authorization Bearer TOKEN\"</code> header as printed out when Slicer started up.</p> <p>Copy the two JSON files to your own workstation.</p> <p>Switch over to a terminal on your own workstation.</p> <p>K3sup Pro can support multiple devices.json files, the first file will be taken for the servers, the rest will be used as agents and installed in parallel.</p> <pre><code>PRO=1 curl -sSL https://get.k3sup.dev | sudo sh\n</code></pre> <pre><code>k3sup-pro plan --user ubuntu \\\n  --devices ./devices-a.json \\\n  --devices ./devices-b.json \\\n  --servers 3 \\\n  --traefik=false\n</code></pre> <p>After the plan.yaml file is generated, you can run <code>k3sup-pro apply</code> to setup the cluster.</p> <pre><code>k3sup-pro apply\n</code></pre> <p>After a few moments, you can get the KUBECONFIG and merge it into your existing kubeconfig file:</p> <pre><code>mkdir -p ~/.kube\ncp ~/.kube/config ~/.kube/config.bak || true\n\nk3sup-pro get-config \\\n    --local-path ~/.kube/config \\\n    --merge \\\n    --context slicer-multi\n</code></pre> <p>Then you can run <code>kubectx slicer-multi</code> to switch to your new cluster and explore it with kubectl.</p> <p>If your two machines were different kinds of architecture, explore the labels applied with:</p> <pre><code>kubectl get nodes --show-labels\n</code></pre> <p>Then you can deploy a Pod with a nodeSelector, or affinity/anti-affinity rule to control where your workloads run.</p> <p>For a number of quick Kubernetes applications to try out, see <code>arkade install --help</code>.</p> <p>The OpenFaaS CE app is quick to install and has a number of samples you can play with.</p>"},{"location":"examples/opencode-ai-agent/","title":"Run an AI Agent with OpenCode","text":"<p>We'll run an AI agent within a microVM using OpenCode, an open-source tool to generate code using large language models (LLMs).</p> <p>OpenCode occasionally supports free trials for popular LLMs. This guide was written whilst Grok's Coder model was available for free - and at no cost.</p> <p>OpenCode supports various model providers including GitHub CoPilot, OpenAI, Ollama (self-hosted models), and Anthropic.</p> <p>This example is very minimal and can be tuned and adapted in many ways to suit your workflow.</p> <p>The general idea is that everything is orchestrated via an initial userdata script:</p> <ul> <li>Install opencode CLI.</li> <li>Configure model authentication using a pre-defined config file <code>~/.local/share/opencode/auth.json</code> from another machine like your workstation.</li> <li>Set the agent working away as a systemd service, with a prompt set in the userdata file.</li> </ul> <p>This example is one-shot, so it's designed to run to completion once, without any further interaction.</p> <p></p> <p>Example of a one-shot execution for a sample prompt to create a Go HTTP server, and to test it via <code>curl</code></p>"},{"location":"examples/opencode-ai-agent/#example-config","title":"Example config","text":"<p>The slicer config will be adapted from the walkthrough. When you create the YAML, name it <code>opencode.yaml</code>.</p> <p>Now, just add the <code>userdata_file</code> in the hostgroup section.</p> <p><code>userdata_file: ./opencode.sh</code></p> <p>And customise the <code>ssh_keys</code> or <code>github_user</code> fields so you can connect via SSH to review the logs, and/or scp to pull out any generated code.</p> <p>On a computer where you've pre-installed and authenticated opencode, create a base64 representation of the auth file:</p> <pre><code># MacOS\nbase64 --wrap 9999 ~/.local/share/opencode/auth.json\n# Linux\nbase64 --cols 9999 ~/.local/share/opencode/auth.json\n</code></pre> <p>Then, create opencode.sh:</p> <pre><code>#!/usr/bin/env bash\n# Ubuntu only. Requires: curl + arkade preinstalled.\n# Installs opencode, sets pre-auth, creates a daemonized systemd unit \"opencode.service\".\n\nset -euo pipefail\n\n# === set this ===\n# set base64 of ~/.local/share/opencode/auth.json}\nexport OPENCODE_AUTH_JSON_B64=\"\"\n# =================\n\n# Install opencode -&gt; /usr/local/bin\narkade get opencode --path /usr/local/bin &gt;/dev/null\nchown ubuntu /usr/local/bin/opencode\nchmod +x /usr/local/bin/opencode\n\n# Prep dirs &amp; auth for user \"ubuntu\" (no group assumption)\nfor d in /home/ubuntu/workdir /home/ubuntu/.local/share/opencode /home/ubuntu/.local/state /home/ubuntu/.cache; do\n  mkdir -p \"$d\"\n  chown ubuntu \"$d\"\ndone\necho \"$OPENCODE_AUTH_JSON_B64\" | base64 -d &gt; /home/ubuntu/.local/share/opencode/auth.json\nchown ubuntu /home/ubuntu/.local/share/opencode/auth.json\nchmod 600 /home/ubuntu/.local/share/opencode/auth.json\n\n# Task payload (edit as needed)\ncat &gt;/home/ubuntu/workdir/task.txt &lt;&lt;'EOF'\nCreate a new Go program with an HTTP server.\nAdd GET /healthz returning 201 \"Created\".\nPrint a git diff at the end.\n\nUse \"arkade system install golang\" to install Go into the environment. Then test the program with \"go run main.go\" and curl localhost:8080/healthz - make sure you kill the program after confirming the correct HTTP code was returned.\n\nSet the PATH variable to include /usr/local/go/bin so that the go command is found.\nEOF\n\nchown ubuntu /home/ubuntu/workdir/task.txt\nchmod 600 /home/ubuntu/workdir/task.txt\n\n# systemd service (daemonized under systemd)\ncat &gt;/etc/systemd/system/opencode.service &lt;&lt;'EOF'\n[Unit]\nDescription=OpenCode one-shot (daemonized under systemd)\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=simple\nUser=ubuntu\nWorkingDirectory=/home/ubuntu/workdir\nEnvironment=HOME=/home/ubuntu\nEnvironment=XDG_STATE_HOME=/home/ubuntu/.local/state\nEnvironment=XDG_CACHE_HOME=/home/ubuntu/.cache\nEnvironment=XDG_DATA_HOME=/home/ubuntu/.local/share\nExecStart=/usr/bin/env bash -lc '/usr/local/bin/opencode run \"$(cat /home/ubuntu/workdir/task.txt)\" -m opencode/grok-code'\nRestart=no\nStandardOutput=journal\nStandardError=journal\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsystemctl daemon-reload\nsystemctl enable --now opencode.service\n</code></pre> <p>Just before you start up the VM, make sure you customise the prompt to have the agent do whatever it is you need.</p> <pre><code>cat &gt;/home/ubuntu/workdir/task.txt &lt;&lt;'EOF'\n\nPrompt goes here\nOver multiple lines\n\nEOF\n</code></pre> <p>Start up the VM i.e. <code>sudo -E slicer up ./opencode.yaml</code></p> <p>What if you want to copy in a private Git repository?</p> <p>One option is to include an SSH key for the agent/ubuntu user, so that it can clone the repository directly from GitHub or another Git server. To keep permissions tight, you could also simply <code>scp</code> the code into the microVM after it has booted, like below.</p> <p>If you want to work on a private Git repository, simply have the systemd unit wait until it finds a folder within the workdir folder, and then scp the code from your host after the microVM has booted.</p> <p>So if the directory was named i.e. <code>arkade</code>, and we'd cloned it locally, you could amend the <code>opencode</code> systemd unit like so:</p> <pre><code>[Service]\nExecStartPre=/usr/bin/env bash -c 'while [ ! -d /home/ubuntu/workdir/arkade ]; do sleep 5; done'\nExecStart=/usr/bin/env bash -lc '/usr/local/bin/opencode run \"$(cat /home/ubuntu/workdir/task.txt)\" -m opencode/grok-code'\n</code></pre>"},{"location":"examples/opencode-ai-agent/#view-the-results","title":"View the results","text":"<p>Once the VM is running, you can check the status of the <code>opencode</code> service.</p> <p>The code will be written to the <code>$HOME/workdir</code> directory.</p> <pre><code>ssh ubuntu@192.168.137.2\n\nsudo journalctl -u opencode.service -f\n\ncd workdir\nfind .\n\ngit diff\n</code></pre> <p>In the example of the preloaded prompt from above, we saw in <code>~/workdir/main.go</code>:</p> <pre><code>package main\n\nimport (\n        \"fmt\"\n        \"net/http\"\n)\n\nfunc main() {\n        http.HandleFunc(\"/healthz\", func(w http.ResponseWriter, r *http.Request) {\n                w.WriteHeader(http.StatusOK)\n                fmt.Fprint(w, \"ok\")\n        })\n        http.ListenAndServe(\":8080\", nil)\n}\n</code></pre> <p>To copy out the workdir, run something like this on your host:</p> <pre><code>mkdir -p agent-runs\ncd agent-runs\nscp -r ubuntu@192.168.137.2:/home/ubuntu/workdir .\n</code></pre>"},{"location":"examples/opencode-ai-agent/#further-thoughts","title":"Further thoughts","text":"<p>This was a very basic example to get you thinking - you could write your own program and use that to drive the whole interaction, rather than using systemd and bash.</p> <p>SSH could also be a better way to interact with the agent, rather than passing an initial prompt and token via the userdata file.</p>"},{"location":"examples/openfaas-edge/","title":"OpenFaaS Edge in slicer","text":"<p>Follow this guide to setup a pre-configured microVM with OpenFaas Edge deployed along with the OpenFaaS Function Builder API.</p> <p>After following the steps in this example you will be able to build and deploy functions to faasd directly from the VM or access the gateway and function builder from other machines by using the VM's IP.</p>"},{"location":"examples/openfaas-edge/#vm-config","title":"VM config","text":"<p>The slicer config will be adapted from the walkthrough. When you create the YAML, name it <code>openfaas-edge.yaml</code>.</p> <p>Change the hostsgroup name to e.g. <code>openfaas-edge</code> and add the <code>userdata_file</code> in and the hostgroup section.</p> <pre><code>config:\n  host_groups:\n- - name: vm\n+ - name: openfaas-edge\n+   userdata_file: ./openfaas-edge.sh\n</code></pre> <p>Customise the <code>ssh_keys</code> or <code>github_user</code> fields so you can connect via SSH to deploy functions, review the logs, and access check the status of the faasd services.</p> <p>The userdate script provided in this guide can be used on both ubuntu and rocky. You can change the <code>image</code> to switch to the your preferred OS. An overview of the available images can be found here.</p>"},{"location":"examples/openfaas-edge/#userdata","title":"Userdata","text":"<p>Create <code>openfaas-edge.sh</code>.</p> <p>By default the script installs OpenFaaS Edge a private registry and the OpenFaaS Function Builder API as additional services. Select which services to install by changing the environment variables in the script configuration section.</p> <p>Note that the function builder addon requires a separate license.</p> <p>Available configuration options:</p> Env Description Default INSTALL_REGISTRY Install a private registry <code>true</code> INSTALL_BUILDER Install the OpenFaaS Function Builder API <code>true</code> <pre><code>#!/usr/bin/env bash\n\n#==============================================================================\n# OpenFaaS Edge Installation Script\n#==============================================================================\n# This script installs OpenFaaS Edge, including optional\n# components like a private registry and function builder.\n#==============================================================================\n\nset -euxo pipefail\n\n#==============================================================================\n# CONFIGURATION\n#==============================================================================\n\n# Install additional services (registry and function builder)\nexport INSTALL_REGISTRY=true\nexport INSTALL_BUILDER=true\n\n#==============================================================================\n# SYSTEM PREPARATION and OpenFaaS Edge Installation\n#==============================================================================\n\nhas_dnf() {\n  [ -n \"$(command -v dnf)\" ]\n}\n\nhas_apt_get() {\n  [ -n \"$(command -v apt-get)\" ]\n}\n\necho \"==&gt; Configuring system packages and dependencies...\"\n\nif $(has_apt_get); then\n  export HOME=/home/ubuntu\n\n  sudo apt update -y\n\n  # Configure iptables-persistent to avoid interactive prompts\n  echo iptables-persistent iptables-persistent/autosave_v4 boolean false | sudo debconf-set-selections\n  echo iptables-persistent iptables-persistent/autosave_v6 boolean false | sudo debconf-set-selections\n\n  arkade oci install --path . ghcr.io/openfaasltd/faasd-pro-debian:latest\n  sudo apt install ./openfaas-edge-*-amd64.deb --fix-broken -y\n\n  if [ \"${INSTALL_REGISTRY}\" = \"true\" ]; then\n    sudo apt install apache2-utils -y\n  fi\nelif $(has_dnf); then\n  export HOME=/home/rocky\n\n  arkade oci install --path . ghcr.io/openfaasltd/faasd-pro-rpm:latest\n  sudo dnf install openfaas-edge-*.rpm -y\n\n\n  if [ \"${INSTALL_REGISTRY}\" = \"true\" ]; then\n    sudo dnf install httpd-tools -y\n  fi\nelse\n    fatal \"Could not find apt-get or dnf. Cannot install dependencies on this OS.\"\n    exit 1\nfi\n\n# Install faas-cli\narkade get faas-cli --progress=false --path=/usr/local/bin/\n\n# Create the secrets directory and touch the license file\nsudo mkdir -p /var/lib/faasd/secrets\ntouch /var/lib/faasd/secrets/openfaas_license\n\n#==============================================================================\n# PRIVATE REGISTRY AND FUNCTION BUILDER SETUP\n#==============================================================================\n\n# Always install registry if builder is installed\nif [ \"${INSTALL_BUILDER}\" = \"true\" ]; then\n INSTALL_REGISTRY=true\nfi\n\nif [ \"${INSTALL_REGISTRY}\" = \"true\" ]; then\n    echo \"==&gt; Setting up private container registry...\"\n\n    # Generate registry authentication\n    export PASSWORD=$(openssl rand -base64 16)\n    echo $PASSWORD &gt; $HOME/registry-password.txt\n\n    # Create htpasswd file for registry authentication\n    htpasswd -Bbc $HOME/htpasswd faasd $PASSWORD\n    sudo mkdir -p /var/lib/faasd/registry/auth\n    sudo mv $HOME/htpasswd /var/lib/faasd/registry/auth/htpasswd\n\n    # Create registry configuration\n    sudo tee /var/lib/faasd/registry/config.yml &gt; /dev/null &lt;&lt;EOF\nversion: 0.1\nlog:\n  accesslog:\n    disabled: true\n  level: warn\n  formatter: text\n\nstorage:\n  filesystem:\n    rootdirectory: /var/lib/registry\n\nauth:\n  htpasswd:\n    realm: basic-realm\n    path: /etc/registry/htpasswd\n\nhttp:\n  addr: 0.0.0.0:5000\n  relativeurls: false\n  draintimeout: 60s\nEOF\n\n    # Configure registry authentication for faas-cli\n    cat $HOME/registry-password.txt | faas-cli registry-login \\\n      --server http://registry:5000 \\\n      --username faasd \\\n      --password-stdin\n\n    # Setup Docker credentials for faasd-provider\n    sudo mkdir -p /var/lib/faasd/.docker\n    sudo cp ./credentials/config.json /var/lib/faasd/.docker/config.json\n\n    # Ensure pro-builder can access Docker credentials\n    sudo mkdir -p /var/lib/faasd/secrets\n    sudo cp ./credentials/config.json /var/lib/faasd/secrets/docker-config\n\n    # Configure local registry hostname resolution\n    echo \"127.0.0.1 registry\" | sudo tee -a /etc/hosts\n\n    echo \"==&gt; Adding registry services to docker-compose...\"\n\n    # Append additional services to docker-compose.yaml\n    sudo tee -a /var/lib/faasd/docker-compose.yaml &gt; /dev/null &lt;&lt;EOF\n\n  registry:\n    image: docker.io/library/registry:3\n    volumes:\n    - type: bind\n      source: ./registry/data\n      target: /var/lib/registry\n    - type: bind\n      source: ./registry/auth\n      target: /etc/registry/\n      read_only: true\n    - type: bind\n      source: ./registry/config.yml\n      target: /etc/docker/registry/config.yml\n      read_only: true\n    deploy:\n      replicas: 1\n    ports:\n      - \"5000:5000\"\nEOF\n\nfi\n\nif [ \"${INSTALL_BUILDER}\" = \"true\" ]; then\n    echo \"==&gt; Configuring function builder...\"\n\n    # Generate payload secret for function builder\n    openssl rand -base64 32 | sudo tee /var/lib/faasd/secrets/payload-secret\n\n    echo \"==&gt; Adding function builder services to docker-compose...\"\n\n    # Append additional services to docker-compose.yaml\n    sudo tee -a /var/lib/faasd/docker-compose.yaml &gt; /dev/null &lt;&lt;EOF\n\n  pro-builder:\n    depends_on: [buildkit]\n    user: \"app\"\n    group_add: [\"1000\"]\n    restart: always\n    image: ghcr.io/openfaasltd/pro-builder:0.5.3\n    environment:\n      buildkit-workspace: /tmp/\n      enable_lchown: false\n      insecure: true\n      buildkit_url: unix:///home/app/.local/run/buildkit/buildkitd.sock\n      disable_hmac: false\n      # max_inflight: 10 # Uncomment to limit concurrent builds\n    command:\n     - \"./pro-builder\"\n     - \"-license-file=/run/secrets/openfaas-license\"\n    volumes:\n      - type: bind\n        source: ./secrets/payload-secret\n        target: /var/openfaas/secrets/payload-secret\n      - type: bind\n        source: ./secrets/openfaas_license\n        target: /run/secrets/openfaas-license\n      - type: bind\n        source: ./secrets/docker-config\n        target: /home/app/.docker/config.json\n      - type: bind\n        source: ./buildkit-rootless-run\n        target: /home/app/.local/run\n      - type: bind\n        source: ./buildkit-sock\n        target: /home/app/.local/run/buildkit\n    deploy:\n      replicas: 1\n    ports:\n     - \"8088:8080\"\n\n  buildkit:\n    restart: always\n    image: docker.io/moby/buildkit:v0.23.2-rootless\n    group_add: [\"2000\"]\n    user: \"1000:1000\"\n    cap_add:\n      - CAP_SETUID\n      - CAP_SETGID\n    command:\n    - rootlesskit\n    - buildkitd\n    - \"--addr\"\n    - unix:///home/user/.local/share/bksock/buildkitd.sock\n    - --oci-worker-no-process-sandbox\n    security_opt:\n    - no-new-privileges=false\n    - seccomp=unconfined        # Required for mount(2) syscall\n    volumes:\n      # Runtime directory for rootlesskit/buildkit socket\n      - ./buildkit-rootless-run:/home/user/.local/run\n      - /sys/fs/cgroup:/sys/fs/cgroup\n      # Persistent state and cache directories\n      - ./buildkit-rootless-state:/home/user/.local/share/buildkit\n      - ./buildkit-sock:/home/user/.local/share/bksock\n    environment:\n      XDG_RUNTIME_DIR: /home/user/.local/run\n      TZ: \"UTC\"\n      BUILDKIT_DEBUG: \"1\"         # Enable for debugging\n      BUILDKIT_EXPERIMENTAL: \"1\"  # Enable experimental features\n    deploy:\n      replicas: 1\nEOF\n\nfi\n\n#==============================================================================\n# INSTALLATION EXECUTION\n#==============================================================================\n\necho \"==&gt; Installing faasd...\"\n\n# Execute the installation\nsudo /usr/local/bin/faasd install\n\n#==============================================================================\n# POST-INSTALLATION CONFIGURATION\n#==============================================================================\n\nif [ \"${INSTALL_BUILDER}\" = \"true\" ]; then\n    echo \"==&gt; Configuring insecure registry access...\"\n\n    # Configure faasd-provider to use insecure registry\n    sudo sed -i '/^ExecStart=/ s|$| --insecure-registry http://registry:5000|' \\\n        /lib/systemd/system/faasd-provider.service\n\n    # Reload systemd and restart faasd-provider\n    sudo systemctl daemon-reload\n    sudo systemctl restart faasd-provider\nfi\n\necho \"==&gt; OpenFaaS Edge installation completed successfully!\"\necho \"\"\necho \"1. Access the OpenFaaS gateway at http://localhost:8080\"\necho \"2. Get your admin password: sudo cat /var/lib/faasd/secrets/basic-auth-password\"\nif [ \"${INSTALL_REGISTRY}\" = \"true\" ]; then\n    echo \"3. Private registry available at http://localhost:5000\"\n    echo \"4. Registry password: cat $HOME/registry-password.txt\"\nfi\nif [ \"${INSTALL_BUILDER}\" = \"true\" ]; then\n    echo \"5. Pro-builder service available at http://localhost:8088\"\nfi\n</code></pre> <p>Start the VM:</p> <pre><code>sudo -E slicer up ./openfaas-edge.yaml`\n</code></pre> <p>Login to the VM and activate faasd using a static license key are by running faasd activate.</p> <p>Commercial users can create their license key as follows:</p> <pre><code>sudo nano /var/lib/faasd/secrets/openfaas_license\"\n</code></pre> <p>For personal, non-commercial use only, GitHub Sponsors of @openfaas can run:</p> <pre><code>sudo faasd github login\nsudo faasd activate\n</code></pre> <p>Restart faasd and the faad-provider after adding the license key:</p> <pre><code>sudo systemctl restart faasd faasd-provider\n</code></pre>"},{"location":"examples/openfaas-edge/#build-and-deploy-a-function-within-the-vm","title":"Build and deploy a function within the VM","text":"<p>Login in to the VM over ssh to perform a test build on the VM directly.</p> <p>Once you connected to the VM authenticate the faas-cli for gateway access:</p> <pre><code>sudo cat \"/var/lib/faasd/secrets/basic-auth-password\" \\\n | faas-cli login --password-stdin\n</code></pre> <p>Scaffold a new function for testing:</p> <pre><code>faas-cli new --lang python3-http \\\n  --prefix registry:5000/functions \\\n  pytest\n</code></pre> <p>The <code>--prefix</code> flag is used to set prefix for the function image to our local registry.</p> <p>Build the function using the function builder API and deploy it:</p> <pre><code># Get the payload secret\nsudo cp /var/lib/faasd/secrets/payload-secret ./payload-secret\n\nfaas-cli up \\\n    --remote-builder http://127.0.0.1:8088 \\\n    --payload-secret ./payload-secret \\\n    --tag=digest\n</code></pre>"},{"location":"examples/openfaas-edge/#access-openfaas-edge-from-your-own-workstation","title":"Access OpenFaaS Edge from your own workstation","text":"<p>Get the basic auth password from the VM and login with the faas-cli:</p> <pre><code>export OPENFAAS_URL=http://192.168.137.2:8080\nssh ubuntu@192.168.137.2 \"sudo cat /var/lib/faasd/secrets/basic-auth-password\" \\\n | faas-cli login --password-stdin\n</code></pre> <p>Note that the <code>OPENFAAS_URL</code> is configured to use the VM's IP address.</p> <p>Deploy a function form the function store:</p> <pre><code>faas-cli store deploy nodeinfo\n</code></pre> <p>Invoke the nodeinfo function:</p> <pre><code>curl -s http://192.168.137.2:8080/function/nodeinfo\n</code></pre> <p>If you have the function builder installed you can build and deploy a function using the function builder API:</p> <p>Scaffold a new function for testing:</p> <pre><code>faas-cli new --lang python3-http \\\n  --prefix registry:5000/functions \\\n  pytest\n</code></pre> <p>The <code>--prefix</code> flag is used to set the prefix for the function image to our local registry deployed on the VM.</p> <pre><code># Get the payload secret from the VM\nssh ubuntu@192.168.137.2 \"sudo cat /var/lib/faasd/secrets/payload-secret\" &gt; ./payload-secret\n\nfaas-cli up \\\n  --remote-builder http://192.168.137.2:8088 \\\n  --payload-secret ./payload-secret \\\n  --tag=digest\n</code></pre> <p>The <code>--remote-builder</code> flag points to the Function Builder API exposed on the VM.</p> <p>Test the new function:</p> <pre><code>curl -s http://192.168.137.2:8080/function/pytest\n</code></pre>"},{"location":"examples/openfaas-edge/#access-the-registry","title":"Access the registry","text":"<p>The be able to push and pull function from the image registry directly you need to add the VM IP address to your hosts file and login with docker.</p> <p>Add a new entry to your hosts file that points to the registry:</p> <pre><code>echo \"192.168.137.2 registry\" | sudo tee -a /etc/hosts\n</code></pre> <p>The registry has to be added as an insecure registry in the docker daemon configuration file, <code>/etc/docker/daemon.json</code>:</p> <pre><code>{\n  \"insecure-registries\": [ \"registry:5000\" ]\n}\n</code></pre> <p>Get get the registry password from the VM and login with docker:</p> <pre><code>ssh ubuntu@192.168.137.2 \"sudo cat ./registry-password.txt\" &gt; ./registry-password.txt\ncat registry-password.txt | docker login \\\n  --username faasd \\\n  --password-stdin \\\n  http://registry:5000\n</code></pre> <p>You should now be able to push and pull images from the registry.</p> <p>You can try this by building a function on your workstation and pushing it to the registry. M</p> <pre><code>faas-cli new --lang node22 \\\n  --prefix registry:5000/functions \\\n  --append stack.yaml \\\n  nodetest\n\nfaas-cli up\n</code></pre> <p>Test the new function:</p> <pre><code>curl -s http://192.168.137.2:8080/function/nodetest -d \"Hello World\"\n</code></pre>"},{"location":"examples/openfaas/","title":"OpenFaaS Pro and CE in Slicer","text":"<p>Boot up a pre-configured microVM with OpenFaaS Pro or CE setup along with <code>faas-cli</code>, <code>helm</code>, and <code>stern</code> for log tailing.</p> <p>For either edition, the URL for the gateway is set up for you for in the default user's <code>.bashrc</code> file:</p> <pre><code>export OPENFAAS_URL=http://127.0.0.1:31112\n</code></pre> <p>You can use that address directly on the VM during an SSH session, but you'll also be able to access the gateway from other machines by using the VM's IP i.e. <code>http://192.168.137.2:31112</code>.</p> <p>Get instructions to fetch the password via <code>arkade info openfaas</code>.</p> <p>For SSH access, customise the <code>github_user</code> field, or set the keys as a list via <code>ssh_keys</code>. Learn more about SSH in Slicer.</p> <p>A multi-node setup is possible, but in that case, it's better to set up K3s using K3sup Pro from outside the VMs, and to follow the HA K3s example followed by the OpenFaaS for Kubernetes instructions.</p>"},{"location":"examples/openfaas/#openfaas-pro","title":"OpenFaaS Pro","text":"<p>Just copy your license key into the YAML file below and run <code>sudo slicer up -f openfaas-pro.yaml</code>.</p> <p>Create <code>openfaas-pro.yaml</code> with the following content:</p> <pre><code>config:\n\n  host_groups:\n  - name: openfaas-pro\n    userdata: |\n              export LICENSE=\"\"\n              export HOME=/home/ubuntu\n              export USER=ubuntu\n\n              cd /home/ubuntu/\n\n              (\n              arkade get kubectl kubectx helm faas-cli k3sup stern --path /usr/local/bin\n              chown $USER /usr/local/bin/*\n\n              mkdir -p .kube\n              mkdir -p .openfaas\n\n              echo -n $LICENSE &gt; ./.openfaas/LICENSE\n              )\n\n              (\n              k3sup install --local\n              mv ./kubeconfig ./.kube/config\n              chown $USER .kube/config\n              )\n\n              (\n              kubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml\n\n              kubectl create secret generic \\\n                -n openfaas \\\n                openfaas-license \\\n                --from-file license=$HOME/.openfaas/LICENSE\n\n              helm repo add openfaas https://openfaas.github.io/faas-netes/\n              helm repo update &amp;&amp; \\\n                helm upgrade --install openfaas \\\n                --install openfaas/openfaas \\\n                --namespace openfaas \\\n                -f https://raw.githubusercontent.com/openfaas/faas-netes/refs/heads/master/chart/openfaas/values-pro.yaml\n\n              chown -R $USER $HOME\n\n              echo \"export OPENFAAS_URL=http://127.0.0.1:31112\" &gt;&gt; $HOME/.bashrc\n\n              )\n\n    storage: image\n    storage_size: 25G\n    count: 1\n    vcpu: 2\n    ram_gb: 4\n    network:\n      bridge: brvm0\n      tap_prefix: vmtap\n      gateway: 192.168.137.1/24\n      addresses:\n      - 192.168.137.2/24\n  github_user: alexellis\n\n  image: \"ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-latest\"\n\n  api:\n    port: 8080\n    bind_address: \"127.0.0.1:\"\n    auth:\n      enabled: true\n\n  ssh:\n    port: 2222\n    bind_address: \"0.0.0.0:\"\n\n  hypervisor: firecracker\n</code></pre> <p>Edit <code>export LICENSE=\"\"</code></p> <p>Then start it up:</p> <pre><code>sudo -E slicer up -f openfaas-pro.yaml\n</code></pre> <p>You can login with:</p> <pre><code>PASSWORD=$(kubectl get secret -n openfaas basic-auth -o jsonpath=\"{.data.basic-auth-password}\" | base64 --decode; echo)\necho -n $PASSWORD | faas-cli login --username admin --password-stdin\n</code></pre> <p>Deploy a function:</p> <pre><code>faas-cli store deploy figlet\n\n\nDescribe, then invoke the function:\n\n```bash\nfaas-cli describe figlet\nfaas-cli invoke &lt;&lt;&lt; \"SlicerVM.com\"\n</code></pre>"},{"location":"examples/openfaas/#openfaas-ce","title":"OpenFaaS CE","text":"<p>OpenFaaS CE is licensed for personal, non-commercial use or a single 60 day commercial trial per company.</p> <p>The above userdata script can be re-used, with a few options removed.</p> <p>Create <code>openfaas-ce.yaml</code> with the following content:</p> <pre><code>config:\n\n  host_groups:\n  - name: openfaas-ce\n    userdata: |\n              export HOME=/home/ubuntu\n              export USER=ubuntu\n\n              cd /home/ubuntu/\n\n              (\n              arkade get kubectl kubectx helm faas-cli k3sup stern --path /usr/local/bin\n              chown $USER /usr/local/bin/*\n\n              mkdir -p .kube\n              mkdir -p .openfaas\n              )\n\n              (\n              k3sup install --local\n              mv ./kubeconfig ./.kube/config\n              chown $USER .kube/config\n              )\n\n              (\n              kubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml\n\n              helm repo add openfaas https://openfaas.github.io/faas-netes/\n              helm repo update &amp;&amp; \\\n                helm upgrade --install openfaas \\\n                --install openfaas/openfaas \\\n                --namespace openfaas \\\n\n              chown -R $USER $HOME\n\n              echo \"export OPENFAAS_URL=http://127.0.0.1:31112\" &gt;&gt; $HOME/.bashrc\n\n              )\n\n    storage: image\n    storage_size: 25G\n    count: 1\n    vcpu: 2\n    ram_gb: 4\n    network:\n      bridge: brvm0\n      tap_prefix: vmtap\n      gateway: 192.168.137.1/24\n      addresses:\n      - 192.168.137.2/24\n  github_user: alexellis\n\n  image: \"ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-latest\"\n\n  api:\n    port: 8080\n    bind_address: \"127.0.0.1:\"\n    auth:\n      enabled: true\n\n  ssh:\n    port: 2222\n    bind_address: \"0.0.0.0:\"\n\n  hypervisor: firecracker\n</code></pre>"},{"location":"examples/pihole-adblock/","title":"PiHole Adblocker Example with Slicer","text":"<p>PiHole is a common adblocker for home networks. This example shows how to run PiHole in a Slicer VM.</p> <p>You can use a regular PC like an N100 or a Raspberry Pi.</p>"},{"location":"examples/pihole-adblock/#userdata","title":"Userdata","text":"<p>The automated installation of PiHole users a custom userdata script.</p> <p>Save this as <code>setup-pihole.sh</code>:</p> <pre><code>#!/usr/bin/env bash\nset -euxo pipefail\n\n# ---------- Tunables ----------\nUPSTREAMS=(\"1.1.1.1\" \"9.9.9.9\")   # change if you like\nENABLE_DNSSEC=\"true\"              # \"true\" or \"false\"\nADMIN_PASS=\"\"                     # leave empty to auto-generate\n# -------------------------------\n\nretry() { i=0; while true; do if \"$@\"; then return 0; fi; i=$((i+1)); [ \"$i\" -ge 30 ] &amp;&amp; return 1; sleep 2; done; }\nwait_default_route() { for _ in $(seq 1 60); do /usr/sbin/ip -4 route show default | /usr/bin/grep -q . &amp;&amp; return 0; sleep 1; done; return 1; }\n\n# 1) Network ready before any fetches\nwait_default_route\n\n# 2) Bring up systemd-resolved (no stub on :53 to avoid conflict)\n#    and point resolv.conf at resolved's generated file\n/usr/bin/mkdir -p /etc/systemd || :\n/usr/bin/tee /etc/systemd/resolved.conf &gt;/dev/null &lt;&lt;EOF\n[Resolve]\nDNS=${UPSTREAMS[*]}\nFallbackDNS=1.0.0.1 149.112.112.112\nDNSStubListener=no\nEOF\n/bin/systemctl unmask systemd-resolved || :\n/bin/systemctl enable --now systemd-resolved || :\nif [ -f /run/systemd/resolve/resolv.conf ]; then\n  chattr -i /etc/resolv.conf || :\n  chattr -i /etc/systemd/resolved.conf || :\n\n  /bin/ln -sf /run/systemd/resolve/resolv.conf /etc/resolv.conf\nelse\n  # fallback if resolved isn't providing it for some reason\n  /usr/bin/tee /etc/resolv.conf &gt;/dev/null &lt;&lt;EOF\n$(for s in \"${UPSTREAMS[@]}\"; do echo \"nameserver $s\"; done)\noptions timeout:2 attempts:2\nEOF\nfi\n\n# 3) Speed up apt (prefer IPv4)\n#    NOTE: do this BEFORE update to avoid AAAA timeouts\n/usr/bin/tee /etc/apt/apt.conf.d/99force-ipv4 &gt;/dev/null &lt;&lt;'EOF'\nAcquire::ForceIPv4 \"true\";\nEOF\n\n# 4) Base tools\nretry /usr/bin/apt-get update\nDEBIAN_FRONTEND=noninteractive retry /usr/bin/apt-get install -y \\\n  curl ca-certificates iproute2 procps e2fsprogs openssl dnsutils || :\n\n# 5) Detect primary interface + IP/CIDR\nIFACE=\"$(/usr/sbin/ip -o -4 route show to default | /usr/bin/awk '{print $5}' | /usr/bin/head -n1)\"\nIP_CIDR=\"$(/usr/sbin/ip -o -4 addr show dev \"${IFACE}\" | /usr/bin/awk '{print $4}' | /usr/bin/head -n1)\"\nVM_IP=\"$(echo \"$IP_CIDR\" | /usr/bin/cut -d/ -f1)\"\n\n# 6) Preseed Pi-hole v6 config (TOML + setupVars)\n/usr/bin/mkdir -p /etc/pihole\n# pihole.toml\n/usr/bin/tee /etc/pihole/pihole.toml &gt;/dev/null &lt;&lt;EOF\n[dns]\ninterface = \"0.0.0.0\"\nupstreams = [$(printf '\"%s\",' \"${UPSTREAMS[@]}\" | sed 's/,$//')]\ndnssec = ${ENABLE_DNSSEC}\nEOF\n\n# setupVars.conf (some installers still consult it)\n/usr/bin/tee /etc/pihole/setupVars.conf &gt;/dev/null &lt;&lt;EOF\nPIHOLE_INTERFACE=${IFACE}\nIPV4_ADDRESS=${IP_CIDR}\nIPV6_ADDRESS=\nDNSMASQ_LISTENING=single\nPIHOLE_DNS_1=${UPSTREAMS[0]}\nPIHOLE_DNS_2=${UPSTREAMS[1]:-${UPSTREAMS[0]}}\nDNSSEC=${ENABLE_DNSSEC}\nQUERY_LOGGING=true\nINSTALL_WEB_SERVER=true\nINSTALL_WEB_INTERFACE=true\nLIGHTTPD_ENABLED=true\nBLOCKING_ENABLED=true\nWEBPASSWORD=\nEOF\n\n# 7) Unattended install\nretry /usr/bin/curl -fsSL https://install.pi-hole.net -o /tmp/pihole-install.sh\nPIHOLE_SKIP_OS_CHECK=true /bin/bash /tmp/pihole-install.sh --unattended || :\n\n# 8) CLI path + admin password\nPIHOLE_BIN=\"/usr/local/bin/pihole\"; [ -x \"$PIHOLE_BIN\" ] || PIHOLE_BIN=\"/usr/bin/pihole\"\nif [ -z \"${ADMIN_PASS}\" ]; then ADMIN_PASS=\"$(/usr/bin/openssl rand -base64 18)\"; fi\n\"${PIHOLE_BIN}\" setpassword  \"${ADMIN_PASS}\" || :\n\"${PIHOLE_BIN}\" -g || :\n\n# 9) Ensure service enabled &amp; started\n/bin/systemctl enable pihole-FTL || :\n/bin/systemctl restart pihole-FTL || :\n\n# 10) Output &amp; quick hints\n/usr/bin/printf \"Pi-hole admin: http://%s/admin\\n\" \"${VM_IP}\"\n/usr/bin/printf \"Pi-hole admin password: %s\\n\" \"${ADMIN_PASS}\" &gt; /root/pihole-admin-pass.txt\n/usr/bin/printf \"Saved admin password to /root/pihole-admin-pass.txt\\n\"\necho \"Test locally: dig +short openfaas.com @127.0.0.1 || true\"\n</code></pre>"},{"location":"examples/pihole-adblock/#vm-configuration-file","title":"VM configuration file","text":"<p>Next, set up a modest VM configuration file - use the example given in the walkthrough.</p> <p>Then under <code>host_groups</code>, add the following, and update the hostgroup name to avoid clashes.</p> <pre><code>  host_groups:\n  - name: pihole\n    userdata_file: ./setup-pihole.sh\n</code></pre>"},{"location":"examples/pihole-adblock/#accessing-pihole","title":"Accessing PiHole","text":"<p>Watch the logs from the serial console to see when the install is complete.</p> <pre><code>arkade get fstail\nsudo fstail /var/log/slicer/\n</code></pre> <p>Next, the PiHole admin interface will be available on port 80 of the VM's IP address.</p> <p>This is typically <code>http://192.168.137.2/admin</code> and the admin password will have been printed to the logs found above.</p> <p>Failing that, you can log into the VM using the SOS console or regular SSH and read the file at <code>/root/pihole-admin-pass.txt</code>.</p> <p>Example DNS query:</p> <p>Add nslookup and dig to your Slicer system if not already present</p> <pre><code>sudo apt update &amp;&amp; \\\n    sudo apt install -qy dnsutils\n</code></pre> <p>Perform a lookup using PiHole as the DNS server:</p> <pre><code># Should work fine\nnslookup openfaas.com 192.168.137.2\n\n# Should be blocked\nnslookup doubleclick.net 192.168.137.2\n</code></pre> <p>To use PiHole from another machine, you have a couple of options:</p> <ol> <li>Log into your router and add a static route for the VM's IP address via the slicer host. Then set the DNS server under the router's DHCP settings to the PiHole VM's IP address.</li> <li>Enable DNAT via iptables from the Slicer host to the PiHole VM, then go into your home router and set the DNS server under the router's DHCP settings to the Slicer host's IP address.</li> </ol> <p>Example DNAT rule for iptables:</p> <pre><code># First, clear any existing DNS DNAT rules to avoid conflicts\n# Optional step..\n# sudo iptables -t nat -F PREROUTING\n\n# Set your PiHole VM IP and detect the network interface\nexport PIHOLE_IP=192.168.136.2  # Update this to match your actual PiHole VM IP\nexport SLICER_IFACE=$(ip -o -4 route show to default | awk '{print $5}' | head -n1)\n\necho \"Slicer host interface: $SLICER_IFACE\"\necho \"PiHole VM IP: $PIHOLE_IP\"\n\n# Add DNAT rules for DNS traffic coming from the external interface\nsudo iptables -t nat -A PREROUTING -i $SLICER_IFACE -p udp --dport 53 -j DNAT --to-destination $PIHOLE_IP:53\nsudo iptables -t nat -A PREROUTING -i $SLICER_IFACE -p tcp --dport 53 -j DNAT --to-destination $PIHOLE_IP:53\n\n# Add MASQUERADE rule so return traffic works properly\nsudo iptables -t nat -A POSTROUTING -j MASQUERADE\n</code></pre> <p>If you'd also like a rule for the admin dashboard, so it's also accessible from the host IP:</p> <pre><code># Set your PiHole VM IP and detect the network interface\nexport PIHOLE_IP=192.168.136.2  # Update this to match your actual PiHole VM IP\nexport SLICER_IFACE=$(ip -o -4 route show to default | awk '{print $5}' | head -n1)\n\nsudo iptables -t nat -A PREROUTING -i $SLICER_IFACE -p tcp --dport 80 -j DNAT --to-destination $PIHOLE_IP:80\n</code></pre>"},{"location":"examples/remote-vscode/","title":"Remote VSCode","text":"<p>Let's say that you work away from your computer often, or maybe just want to have a remote IDE that you can access from any kind of device like an iPad, or a Chromebook.</p> <p>You can run Microsoft's VSCode server in a VM using Slicer and connect to it remotely. Microsoft also offer free tunnels to connect to your server with authentication enabled via GitHub or Microsoft accounts.</p> <p>It's incredibly convenient.</p> <p>Alex recently migrated his blog from Ghost 1.x which had a built-in markdown editor, over to a static Next.js site. This example lets him edit the content of his blog without having a laptop with him. This whole setup could be run on a Raspberry Pi 5, or a cheap N100 mini computer.</p> <p></p> <p>VSCode tunneled to the web with built-in authentication and Copilot helping out</p>"},{"location":"examples/remote-vscode/#set-up-your-initial-vm","title":"Set up your initial VM","text":"<p>Use the walkthrough to get a basic VM with the specs you want, with your SSH keys pre-installed, and use a disk image as backing to make it persistent.</p> <p>When you create your config, you can add userdata to install the VSCode server, so it's already there by the time you get in.</p> <p>The below is based upon the Official VSCode Documentation.</p> <pre><code>config:\n  host_groups:\n  - name: vscode\n    userdata: |\n        #!/bin/bash\n\n        (\n        sudo apt-get install -qy curl gpg apt-transport-https\n\n        curl -sLS -o - https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor &gt; microsoft.gpg\n        sudo install -D -o root -g root -m 644 microsoft.gpg /usr/share/keyrings/microsoft.gpg\n\n        cat &gt; /etc/apt/sources.list.d/vscode.sources &lt;&lt; EOF\n        Types: deb\n        URIs: https://packages.microsoft.com/repos/code\n        Suites: stable\n        Components: main\n        Architectures: amd64,arm64,armhf\n        Signed-By: /usr/share/keyrings/microsoft.gpg\n        EOF\n\n        sudo apt install -qy apt-transport-https\n        sudo apt update\n        sudo apt install -qy code --no-install-recommends\n        )\n</code></pre> <p>You can watch the installation by adding the fstail tool via <code>arkade get fstail</code>.</p> <p>Then run the following to attach to any VM log files that are detected:</p> <pre><code>sudo fstail /var/log/slicer\n</code></pre> <p>It seemed to take about 16s to get all of the above installed and configured.</p>"},{"location":"examples/remote-vscode/#start-the-vscode-server","title":"Start the VSCode server","text":"<p>Log in via ssh i.e. <code>ssh ubuntu@192.168.137.2</code></p> <p>To use Microsoft's built-in tunneling service, you need to accept their license terms:</p> <pre><code>code tunnel --accept-server-license-terms \\\n    --name $(hostname)\n</code></pre> <p>You'll be presented with a URL to authenticate with GitHub using a device code.</p> <p>Then you'll get a URL for your remote VSCode server.</p>"},{"location":"examples/remote-vscode/#alternative-without-microsofts-tunneling-service","title":"Alternative without Microsoft's tunneling service","text":"<p>Alternatively, you can run the server without tunneling, and connect directly to it over HTTPS.</p> <pre><code>code serve-web\n</code></pre> <p>A link will be printed out pointing to <code>http://localhost:8080</code> including a token used to authenticate.</p> <p>Then port-forward the port 8000 to your local machine:</p> <pre><code>ssh -L 8000:127.0.0.1:8000 ubuntu@192.168.137.2\n</code></pre> <p>Or launch an inlets HTTPS tunnel via inletsctl to get a public URL with TLS enabled. You can enable auth using basic auth or OAuth2 with the inlets command.</p>"},{"location":"examples/remote-vscode/#customise-the-environment","title":"Customise the environment","text":"<p>Install any tooling you want such as Node, or Go, Python, Docker etc.</p> <p>If you use Copilot, log into your Microsoft account, and then you can enable the Copilot extension in your remote VSCode server.</p>"},{"location":"examples/remote-vscode/#another-approach-vscode-with-ssh","title":"Another approach: VScode with SSH","text":"<p>Instead of launching a browser-based VSCode server, you can use your local VSCode installation to connect to the microVM over SSH.</p> <ul> <li>Any agents you run can work in YOLO mode, without risking your content, confidential data, or any risk of breaking your main host.</li> <li>You can create, delete, and iterate on microVMs easily, and if you break something, just delete it and start again.</li> <li>You can get access to a full Linux host, whilst working on MacOS or Windows.</li> <li>You can test programs out on an Arm host, even if your main host is x86_64.</li> </ul> <p>You can also use VSCode's built-in SSH support to connect to your VM. This allows you to use your local VSCode installation to edit files on the remote machine, with all your familiar settings already available.</p> <ol> <li>Install the Remote - SSH extension in your local VSCode.</li> <li>Configure your SSH settings to connect to your VM.</li> <li>Open a remote window in VSCode and start editing files on your VM.</li> </ol> <p>This approach gives you the full power of your local VSCode environment whilst making sure any packages you install, or changes you make, do not affect your main host.</p>"},{"location":"examples/run-a-task/","title":"Run a task in a VM","text":"<p>This example shows how to run a one-shot task in a VM via API. The CLI can also act as a client to the API during testing.</p> <p>Use-cases could include:</p> <ul> <li>Running an AI coding agent in a contained environment without risking your whole workstation</li> <li>Starting on-demand IDEs for pull request development or review</li> <li>Autoscaling Kubernetes nodes - added and removed on demand</li> <li>Running a CI build or compiling untrusted customer code</li> <li>Starting a temporary service such as a database for end to end testing</li> <li>Cron jobs, batch jobs, and serverless functions</li> </ul> <p>One-shot tasks are VMs that are launched on demand for a specific purposes. But there's no limit on the lifetime of these VMs, they can run for any period of time - be that 250ms to process a webhook, 48 hours to run some fine-tuning, or several weeks. Just bear in mind that if you shut down or close Slicer, they will also be shut down and destroyed.</p> <p>Watch a demo of the tutorial to see how fast it is to launch microVMs for one-shot tasks.</p>"},{"location":"examples/run-a-task/#tutorial","title":"Tutorial","text":"<p>Create an empty hostgroup configuration.</p> <p>For the fastest possible boot times, use ZFS for storage.</p> <p>If you don't have ZFS set up yet, you can simply replace the storage option with something like:</p> <pre><code>storage: image\nstorage_size: 20G\n</code></pre> <p>Create <code>tasks.yaml</code>:</p> <pre><code>config:\n  host_groups:\n  - name: task\n    storage: zfs\n    count: 0\n    vcpu: 1\n    ram_gb: 2\n    network:\n      bridge: brtsk0\n      tap_prefix: tsktap\n      gateway: 192.168.138.1/24\n\n  api:\n    bind_address: 127.0.0.1\n    port: 8081\n\n  image: \"ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-latest\"\n\n  hypervisor: firecracker\n</code></pre> <p>Now start up slicer:</p> <pre><code>sudo -E slicer up ./tasks.yaml\n</code></pre> <p>Now set up a HTTP endpoint using a free service like ReqBin.com or webhook.site.</p> <p>Write a userdata script to send a POST request to your HTTP endpoint on boot-up, then have it exit.</p> <p>Save task.sh:</p> <pre><code>cat &gt; task.sh &lt;&lt;'EOF'\n#!/bin/bash\ncurl -i -X POST -d \"$(cat /etc/hostname) booted\\nUptime: $(uptime)\" \\\n    https://webhook.site/f38eddbf-6285-4ff8-ae3e-f2e782c73d8f\n\nsleep 1\nsudo reboot\nexit 0\nEOF\n</code></pre> <p>Then run your task by booting a VM with the script as its userdata:</p> <pre><code>curl -isLSf http://127.0.0.1:8081/hostgroup/task/nodes \\\n    -H \"Content-Type: application/json\" \\\n    --data-binary \"{\n  \\\"userdata\\\": $(cat ./task.sh | jq -Rs .)\n}\"\n</code></pre> <p>Check your HTTP bin for the results.</p> <p>You can also run this in a for loop:</p> <pre><code>for i in {1..5}\ndo\n  curl -sLSf http://127.0.0.1:8081/hostgroup/task/nodes \\\n      -H \"Content-Type: application/json\" \\\n      --data-binary \"{\\\"userdata\\\": $(cat ./task.sh | jq -Rs .)}\"\ndone\n</code></pre> <p></p> <p>Each of the 5 tasks that executed and exited, posted to the endpoint</p>"},{"location":"examples/run-a-task/#launch-a-task-from-the-cli","title":"Launch a task from the CLI","text":"<p>The slicer CLI can act as a HTTP client to the REST API, which makes it a bit easier for initial exploration:</p> <pre><code>for i in {1..5};\ndo\n  slicer vm add \\\n    task \\\n    --api http://127.0.0.1:8081 \\\n    --userdata-file ./task.sh\ndone\n</code></pre> <p>The output will be as follows:</p> <pre><code>VM created\n  Hostname: task-1\n  Group:    task\n  IP:       192.168.138.2/24\n  Specs:    1 vCPU, 2GB RAM, GPUs: 0\n  Persistent: false\n  Created:  2025-09-09T09:45:27+01:00\n</code></pre> <p>When using Cloud Hypervisor for GPU support, the <code>--gpus</code> flag can be passed to allocate a number of GPUs from the host into the guest VM.</p> <p>This would be useful for batch inference, or AI jobs that benefit from direct access to a local LLM.</p>"},{"location":"examples/run-a-task/#optimise-the-image-for-start-up-speed","title":"Optimise the image for start-up speed","text":"<p>After various Kernel modules are loaded, and the system has performed its self-checking, your code should be running at about the 2.5s mark, or a bit earlier depending on your machine.</p> <p>To optimise the boot time further for one-shot use-cases, the SSH host key regenerate step that is present on start-up. It can add a few seconds to the boot time, especially if entropy is low on your system.</p> <p>You can derive your own image to use, with this disabled:</p> <pre><code>FROM ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-latest\n\nRUN systemctl disable regen-ssh-host-keys &amp;&amp;\n    systemctl disable ssh &amp;&amp; \\\n    systemctl disable sshd &amp;&amp; \\\n    systemctl disable slicer-vmmeter\n</code></pre> <p>After SSH is disabled, the only way to debug a machine is via the Slicer agent using <code>slicer vm exec</code> to get a shell.</p> <p>You can also disable <code>slicer-ssh-agent</code> (not actually a full SSH daemon), however the <code>slicer vm</code> commands will no longer work.</p> <p>If you publish an image to the Docker Hub, make sure you include its prefix i.e. <code>docker.io/owner/repo:tag</code>.</p>"},{"location":"examples/run-a-task/#cache-the-kernel-to-a-local-file","title":"Cache the Kernel to a local file","text":"<p>Rather than downloading an extracting the Kernel on each run of Slicer, you can extract a given vmlinux file and tell the YAML file to use that.</p> <p>This is preferred for a long-running host, where we need to keep the root-filesystem image and Kernel in sync.</p> <p>The Kernel must agree with the root filesystem image, which means using a proper tag and not a <code>latest</code> tag.</p> <p>Why? The Kernel is built as a vmlinux, however its modules are packaged into the root filesystem image.</p> <p>Run the following:</p> <pre><code>$ arkade get crane\n$ crane ls ghcr.io/openfaasltd/actuated-kernel:5.10.240-x86_64-latest\n\n5.10.240-x86_64-3d7a67d1683b524b4128ad338f90b1da710f2fd9\n5.10.240-kvm-x86_64-3d7a67d1683b524b4128ad338f90b1da710f2fd9\n5.10.240-x86_64-ea04b63b9117c966a57e17e1bc1bfcf713cd6276\n5.10.240-x86_64-bb71bdd1cd06bad2cc11f8ab3f323c3f19d41c8b\n5.10.240-x86_64-2f2ebc0bbefe128aa3061e6ea6806cbcdc975208\n6.1.90-aarch64-2f2ebc0bbefe128aa3061e6ea6806cbcdc975208\n6.1.90-aarch64-5c59e9b9b08eea49499be8449099291c93469b80\n5.10.240-x86_64-5c59e9b9b08eea49499be8449099291c93469b80\n</code></pre> <p>Pick a stable tag for your architecture i.e. <code>x86_64-SHA</code> or <code>aarch64-SHA</code>, then run:</p> <pre><code>$ arkade oci install ghcr.io/openfaasltd/actuated-kernel:5.10.240-x86_64-2f2ebc0bbefe128aa3061e6ea6806cbcdc975208 --output ./\n$ ls\nvmlinux\n</code></pre> <p>Next, find the matching root filesystem image:</p> <pre><code>$ crane ls ghcr.io/openfaasltd/slicer-systemd\n</code></pre> <p>Use it in your YAML file, replacing <code>kernel_image</code> with <code>kernel_file</code>:</p> <pre><code>  kernel_file: \"./vmlinux\"\n  image: \"ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-2f2ebc0bbefe128aa3061e6ea6806cbcdc975208\"\n</code></pre>"},{"location":"getting-started/daemon/","title":"How to run Slicer as a Daemon","text":"<p>The term \"daemon\" is a well established phrase from UNIX meaning a process that runs in the background. Daemons are usually managed by an init system which can monitor and restart them.</p> <p>Not only can we monitor Slicer's logs via <code>journalctl</code>, but we can manage it with standard <code>systemctl</code> commands.</p> <p>Let's take the example from the walkthrough and create a systemd service for it:</p> <p>Create a service named i.e. <code>vm-image.service</code>:</p> <pre><code>[Unit]\nDescription=Slicer\n\n[Service]\nUser=root\nType=simple\nWorkingDirectory=/home/alex\nExecStart=sudo -E /usr/local/bin/slicer up \\\n  /home/alex/vm-image.yaml \\\n  --license-file /home/alex/.slicer/LICENSE\nRestart=always\nRestartSec=30s\nKillMode=mixed\nTimeoutStopSec=30\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Install the service, and set it to start up on reboots:</p> <pre><code>sudo cp ./vm-image.service /etc/systemd/system/\nsudo systemctl enable vm-image.service\n</code></pre> <p>Now before starting the service, make sure you shut down any existing Slicer process that is managing this particular VM.</p> <p>Then:</p> <pre><code>sudo systemctl start vm-image.service\n</code></pre> <p>To view the logs for the service run:</p> <pre><code># Page through all logs\nsudo journalctl --output=cat -u vm-image\n\n# Tail the latest logs\nsudo journalctl --output=cat -f -u vm-image\n\n# View all logs since today/yesterday\nsudo journalctl --output=cat --since today -u vm-image\nsudo journalctl --output=cat --since yesterday -u vm-image\n</code></pre> <p>To stop the service run <code>sudo systemctl stop vm-image</code>, and to prevent it loading on start-up run: <code>sudo systemctl disable vm-image</code>.</p> <p>You can create multiple Slicer services to run different sets of VMs or configurations on the same host.</p>"},{"location":"getting-started/install/","title":"Installation","text":"<p>Don't wait for the perfect system. Slicer can run practically anywhere.</p> <p>To activate Slicer, you'll need a Hobbyist or Pro subscription - pick according to your needs. These run month by month, so there's very little commitment or risk involved.</p> <p>After the installation, when you run <code>slicer activate</code> you'll get an invite link to the Discord server. We highly recommend joining.</p>"},{"location":"getting-started/install/#system-requirements","title":"System requirements","text":"<p>Any reasonably modern computer can run Slicer, the requirements are very low - x86_64 or Arm64 (including the Raspberry Pi).</p> <p>Sustainably sourced:</p> <ul> <li>Low powered N100 mini PC / Beelink / Minisforum</li> <li>Ampere Altra / Raspberry Pi 5 (with NVMe)</li> <li>Mac Mini M1 / M2 with Asahi Linux installed</li> <li>Self-built ATX tower</li> <li>Computer under your desk / old Thinkpad / Dell server from eBay</li> </ul> <p>Cloud-based bare-metal:</p> <ul> <li>Hetzner Robot (cheapest, best value)</li> <li>Phoenix NAP</li> <li>Latitude.sh</li> </ul> <p>Enterprise:</p> <ul> <li>On-premises datacenter</li> <li>OpenStack / VMware (with nested virtualisation)</li> <li>Azure, DigitalOcean, GCP VMs (with nested virtualisation)</li> </ul> <p>A Linux system with KVM is required (bare-metal or nested virtualisation), so if you see <code>/dev/kvm</code>, Slicer will work there.</p> <p>Ubuntu LTS is formally supported, whilst Arch Linux and RHEL-like Operating Systems should work - we won't be able to debug your system.</p> <p>Ideally, nothing else should be installed on a host that runs Slicer. It should be thought of as a basic appliance - a bare OS, with minimal packages.</p>"},{"location":"getting-started/install/#run-these-steps","title":"Run these steps","text":"<p>Slicer is evolving, and for the better. Today it uses part of the installation process for actuated.</p> <p>The installer sets up Firecracker, Cloud Hypervisor, containerd for storage, and a few networking options.</p> <pre><code># The installer usually looks for an actuated license, but you don't\n# need one to run the installation. We'll create a temporary file via touch.\nmkdir -p ~/.actuated\ntouch ~/.actuated/LICENSE\n\n(\n# Install arkade\ncurl -sLS https://get.arkade.dev | sudo sh\n\n# Use arkade to extract the agent from its OCI container image\narkade oci install ghcr.io/openfaasltd/actuated-agent:latest --path ./agent\nchmod +x ./agent/agent*\nsudo mv ./agent/agent* /usr/local/bin/\n)\n\n(\ncd agent\nsudo -E ./install.sh\n)\n</code></pre> <p>Then, get the Slicer binary:</p> <pre><code>sudo -E arkade oci install ghcr.io/openfaasltd/slicer:latest \\\n  --path /usr/local/bin\n</code></pre> <p>The same command can be repeated to update Slicer to future versions.</p> <p>For Home Edition/Hobbyist users, activate Slicer with your GitHub account to obtain a license key:</p> <pre><code>slicer activate --help\n\nslicer activate\n</code></pre> <p>Pro/Commercial should save their license key (received by email after checking out) to <code>~/.slicer/LICENSE</code> without running any additional commands.</p> <p>Next, start your first VM with the walk through.</p>"},{"location":"getting-started/install/#updating-slicer","title":"Updating slicer","text":"<p>To update Slicer, use the <code>slicer update</code> command:</p> <pre><code>sudo -E slicer update\n</code></pre> <p>Alternatively, if you're on an earlier version, repeat this command from the installation step:</p> <pre><code>sudo -E arkade oci install ghcr.io/openfaasltd/slicer:latest \\\n  --path /usr/local/bin\n</code></pre>"},{"location":"getting-started/walkthrough/","title":"Create a Linux VM","text":"<p>In this example we'll walk through how to create a Linux VM using Slicer on an x86_64 host, or an Arm64 host.</p> <p>The <code>/dev/kvm</code> device must exist and be available to continue.</p>"},{"location":"getting-started/walkthrough/#create-the-vm-configuration","title":"Create the VM configuration","text":"<p>Slicer is a long lived process that can be run in the foreground or daemonised with systemd.</p> <p>To create a VM using a Linux bridge, and a disk file for storage create the following <code>./vm-image.yaml</code> file:</p> <pre><code>config:\n  host_groups:\n  - name: vm\n    storage: image\n    storage_size: 25G\n    count: 1\n    vcpu: 2\n    ram_gb: 4\n    network:\n      bridge: brvm0\n      tap_prefix: vmtap\n      gateway: 192.168.137.1/24\n\n  github_user: alexellis\n\n  image: \"ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-latest\"\n\n  hypervisor: firecracker\n\n  api:\n    port: 8080\n    bind_address: \"127.0.0.1\"\n  ssh:\n    port: 2222\n    bind_address: \"0.0.0.0\"\n</code></pre> <p>The HTTP API is enabled by default and can be disabled with <code>enabled: false</code>.</p> <p>The SSH server is disabled by default and can be enabled by providing a port.</p> <p>The default <code>bind_address</code> for both the API and SSH is <code>127.0.0.1</code> - loopback on the host system.</p> <p>There's no need to provide the HTTP API section unless you plan to run Slicer more than once on the same host, in that case, it's useful to include it so you can change it to a different port on another slicer instance.</p> <p>Configuration for an Arm64 Slicer installation</p> <p>If you're deploying to an Arm64 host, then the only changes needed is to select a different <code>image</code> in the <code>vm.yaml</code> file:</p> <pre><code>-  image: \"ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-latest\"\n+  image: \"ghcr.io/openfaasltd/slicer-systemd-arm64:6.1.90-aarch64-latest\"\n</code></pre> <p>The <code>storage: image</code> setting means a disk image will be cloned from the root filesystem into a local file. It's not the fastest option for the initial setup, but it's the simplest, persistent and great for long-living VMs.</p> <p>Now, open a new terminal window, or ideally launch <code>tmux</code> so you can leave the binary running in the background.</p> <pre><code>sudo -E slicer up ./vm-image.yaml\n</code></pre> <p>Having customised the <code>github_user</code> to your own username, your SSH keys will have been fetched from your profile, and preinstalled into the VM.</p> <p>On your workstation, add any routes that are specified so you can access the VMs on their own network.</p> <p>Connect with:</p> <pre><code>ssh ubuntu@192.168.137.2\n</code></pre>"},{"location":"getting-started/walkthrough/#ignore-changing-ssh-host-keys","title":"Ignore changing SSH host keys","text":"<p>If, like the developers of Slicer, you'll be re-creating many hosts with the same IP addresses, you have two options:</p> <ul> <li>Memorise and get familiar with the <code>ssh-keygen -R &lt;ip-address&gt;</code> command</li> <li>Or add the following to your <code>~/.ssh/config</code> file to stop it complaining</li> </ul> <pre><code>Host 192.168.137.*\n    StrictHostKeyChecking no\n    UserKnownHostsFile /dev/null\n    GlobalKnownHostsFile /dev/null\n    CheckHostIP no\n    LogLevel QUIET\n    User ubuntu\n</code></pre> <p>Repeat it once for each IP range you use with Slicer.</p> <p>And bear in mind that you should not do this for production or long-running hosts.</p>"},{"location":"getting-started/walkthrough/#view-the-serial-console","title":"View the serial console","text":"<p>The logs from the serial console including the output from the boot process are available on disk:</p> <pre><code>$ sudo tail -f /var/log/slicer/vm-1.txt\n\n         Starting OpenBSD Secure Shell server...\n[  OK  ] Started OpenBSD Secure Shell server.\n[  OK  ] Reached target Multi-User System.\n[  OK  ] Reached target Graphical Interface.\n         Starting Record Runlevel Change in UTMP...\n[  OK  ] Finished Record Runlevel Change in UTMP.\n\nUbuntu 22.04.5 LTS vm-1 ttyS0\n\nvm-1 login:\n</code></pre> <p>If you want to tail the logs from all available VMs at once, use <code>fstail</code> via <code>arkade get fstail</code>:</p> <pre><code>sudo -E fstail /var/log/slicer/\n</code></pre>"},{"location":"getting-started/walkthrough/#launch-a-second-vm","title":"Launch a second VM","text":"<p>Edit the <code>count</code> field, and set it to <code>2</code>.</p> <p>Then hit Control + C and launch slicer again.</p> <p>You'll see the second VM come online and can connect to it over SSH.</p>"},{"location":"getting-started/walkthrough/#enable-the-http-api","title":"Enable the HTTP API","text":"<p>The API is used by the <code>slicer vm</code> commands, and can also be used directly via <code>curl</code>.</p> <pre><code>  api:\n    port: 8080\n    bind_address: \"127.0.0.1:\"\n    auth:\n      enabled: true\n</code></pre> <p>The auth token will be created at <code>/var/lib/slicer/token</code> and can be used via a <code>Authorization: Bearer</code> header.</p> <p>i.e.</p> <pre><code>curl -s http://127.0.0.1:8080/nodes/ -H \"Authorization: Bearer $(sudo cat /var/lib/slicer/token)\" | jq\n</code></pre>"},{"location":"getting-started/walkthrough/#enable-the-serial-over-ssh-console","title":"Enable the Serial Over SSH Console","text":"<p>The Serial Over SSH (SOS) console can be used to log into a VM without a password, and without any form of networking enabled.</p> <pre><code>  ssh:\n    port: 2222\n    bind_address: \"0.0.0.0:\"\n</code></pre> <p>Example:</p> <pre><code>ssh -p 2222 ubuntu@localhost\n</code></pre>"},{"location":"reference/api/","title":"Slicer REST API","text":"<p>This page documents the HTTP API exposed by Slicer for managing micro-VMs, images, disks, and operations.</p>"},{"location":"reference/api/#authentication","title":"Authentication","text":"<p>No authentication, ideal for local/dev work:</p> <pre><code>config:\n  api:\n    bind_address: 127.0.0.1\n    port: 8080\n</code></pre> <p>For production:</p> <pre><code>config:\n  api:\n    bind_address: 127.0.0.1\n    port: 8080\n    auth:\n      enabled: true\n</code></pre> <p>The token will be saved to: <code>/var/lib/slicer/auth/token</code>.</p> <p>Send an <code>Authorization: Bearer TOKEN</code> header for authenticated Slicer daemons.</p> <p>If you intend to expose the Slicer API over the Internet using something like a self-hosted inlets tunnel, or Inlets Cloud, then make sure you use the TLS termination option.</p> <p>i.e.</p> <pre><code>DOMAIN=example.com\ninletsctl create \\\n    slicer-api \\\n    --letsencrypt-domain $DOMAIN \\\n    --letsencrypt-email webmaster@$DOMAIN\n</code></pre> <p>Alternatively, if Slicer is on a machine that's public facing, i.e. on Hetzner or another bare-metal cloud, you can use Caddy to terminate TLS. Make sure the API is bound to 127.0.0.1:</p> <p>Caddyfile:</p> <pre><code>{\n    email \"webmaster@example.com\"\n    # Uncomment to try the staging certificate issuer first\n    #acme_ca https://acme-staging-v02.api.letsencrypt.org/directory\n    # The production issuer:\n    acme_ca https://acme-v02.api.letsencrypt.org/directory\n }\n\nslicer-1.example.com {\n reverse_proxy 127.0.0.1:8080\n}\n</code></pre> <p>You can install Caddy via <code>arkade system install caddy</code></p>"},{"location":"reference/api/#conventions","title":"Conventions","text":"<ul> <li>Content-Type: application/json unless noted.</li> <li>Timestamps: RFC3339.</li> <li>IDs: opaque strings returned by the API.</li> <li>Errors: JSON: { \"error\": \"message\", \"code\": \"optional_code\" } with appropriate HTTP status.</li> </ul>"},{"location":"reference/api/#get-healthz","title":"GET /healthz","text":"<p>Check service liveness.</p> <p>Response 200: <pre><code>{ \"status\": \"ok\" }\n</code></pre></p>"},{"location":"reference/api/#list-nodes","title":"List nodes","text":"<p>HTTP GET</p> <p><code>/nodes</code></p> <pre><code>[{\"hostname\":\"vm-1\",\"ip\":\"192.168.137.2\",\"created_at\":\"2025-09-02T08:32:37.667253315+01:00\"}]\n</code></pre>"},{"location":"reference/api/#list-host-groups","title":"List Host Groups","text":"<p>HTTP GET</p> <p><code>/hostgroup</code></p> <p>Response 200:</p> <pre><code>[{\"name\":\"vm\",\"count\":1,\"ram_gb\":4,\"vcpu\":2}]\n</code></pre>"},{"location":"reference/api/#create-a-new-host-within-a-host-group","title":"Create a new host within a Host Group","text":"<p>HTTP POST</p> <p><code>/hostgroup/NAME/nodes</code></p> <p>Add a host with the defaults:</p> <pre><code>{\n}\n</code></pre> <p>Or add a host with userdata:</p> <p>Make sure the string for userdata is JSON encoded.</p> <pre><code>{\n  \"userdata\": \"sudo apt update &amp;&amp; sudo apt upgrade\"\n}\n\nAdd a host with a custom GitHub user to override the SSH keys:\n\n```json\n{\n    \"github_user\": \"your_github_username\"\n}\n</code></pre>"},{"location":"reference/api/#get-serial-console-logs-from-a-vm","title":"Get serial console logs from a VM","text":"<p>HTTP GET</p> <p><code>GET /vm/{hostname}/logs?lines=&lt;n&gt;</code></p> <p>When <code>n</code> is empty, a default of 20 lines will be read from the end of the log.</p> <p>When <code>n</code> is <code>0</code>, the whole contents will be returned.</p> <p>Response 200:</p> <pre><code>[  OK  ] Started slicer-ssh-agent.\n[  OK  ] Started slicer-vmmeter.\n[  OK  ] Reached target Multi-User System.\n         Starting Record Runlevel Change in UTMP...\n[  OK  ] Finished Record Runlevel Change in UTMP.\n</code></pre>"},{"location":"reference/api/#delete-a-host-within-a-host-group","title":"Delete a host within a Host Group","text":"<p>HTTP DELETE</p> <p><code>/hostgroup/NAME/nodes/HOSTNAME</code></p> <p>Response 204: No Content</p>"},{"location":"reference/api/#get-node-consumption-details","title":"Get node consumption details","text":"<p>Providing that the <code>slicer-vmmeter</code> service is running in your VM, detailed usage and consumption metrics can be obtained.</p> <p>HTTP GET</p> <p><code>/nodes/stats</code></p> <pre><code>[\n  {\n    \"hostname\": \"vm-1\",\n    \"ip\": \"192.168.137.2\",\n    \"created_at\": \"2025-09-02T08:32:37.667253315+01:00\",\n    \"snapshot\": {\n      \"hostname\": \"vm-1\",\n      \"arch\": \"x86_64\",\n      \"timestamp\": \"2025-09-02T07:39:32.388239809Z\",\n      \"uptime\": \"6m53s\",\n      \"totalCpus\": 2,\n      \"totalMemory\": 4024136000,\n      \"memoryUsed\": 220184000,\n      \"memoryAvailable\": 3803952000,\n      \"memoryUsedPercent\": 5.471584459372148,\n      \"loadAvg1\": 0,\n      \"loadAvg5\": 0,\n      \"loadAvg15\": 0,\n      \"diskReadTotal\": 62245888,\n      \"diskWriteTotal\": 7815168,\n      \"networkReadTotal\": 44041,\n      \"networkWriteTotal\": 19872,\n      \"diskIOInflight\": 0,\n      \"openConnections\": 0,\n      \"openFiles\": 480,\n      \"entropy\": 256,\n      \"diskSpaceTotal\": 26241896448,\n      \"diskSpaceUsed\": 820826112,\n      \"diskSpaceFree\": 24062115840,\n      \"diskSpaceUsedPercent\": 3.12792222782572\n    }\n  }\n]\n</code></pre>"},{"location":"reference/api/#execute-a-shell","title":"Execute a shell","text":"<p>HTTP GET</p> <p>This is an internal endpoint used by <code>slicer vm exec</code> to obtain a shell. It requires the <code>slicer-ssh-agent</code> service to be running within the guest microVM.</p> <p><code>/vm/{hostname}/exec</code></p>"},{"location":"reference/images/","title":"Images for Slicer MicroVMs","text":"<p>Images for Slicer are built with a Dockerfile, and can be extended with additional layers.</p> <p>It is also possible to build completely different images from scratch, however this is not recommended or documented at this time. So if you need a different OS, reach out to the team via Discord and request it there.</p> <p>Cloud Hypervisor is required to mount a PCI device such as an Nvidia GPU into a microVM. In all other cases, Firecracker is preferred.</p> <p>Generally, Slicer makes use of systemd to run, tune, and manage services and background agents. It is technically possible to use other init systems, but is not supported at this time.</p> <p>Image availability:</p> Operating System Firecracker (x86_64) Firecracker (arm64) Cloud Hypervisor (x86_64) Cloud Hypervisor (arm64) Ubuntu 22.04 Ubuntu 24.04 Rocky Linux 9 <p>Ubuntu 22.04 is supported by Cannoncial is supported until April 2027.</p> <p>Table of image tags:</p> Operating System Firecracker (x86_64) Firecracker (arm64) Cloud Hypervisor (x86_64) Cloud Hypervisor (arm64) Ubuntu 22.04 <code>ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-latest</code> <code>ghcr.io/openfaasltd/slicer-systemd-arm64:6.1.90-aarch64-latest</code> <code>ghcr.io/openfaasltd/slicer-systemd-ch:5.10.240-x86_64-latest</code> Ubuntu 24.04 <code>ghcr.io/openfaasltd/slicer-systemd-2404:5.10.240-x86_64-latest</code> Rocky Linux 9 <code>ghcr.io/openfaasltd/slicer-systemd-rocky9:5.10.240-x86_64-latest</code>"},{"location":"reference/sos/","title":"Slicer's Serial Over SSH console (SOS)","text":"<p>Slicer's Serial Over SSH (SOS) console allows you manage VMs without having an OpenSSH server running, or an active network connection. SOS works over VSOCK, which is a virtual serial console provided by Firecracker and Cloud Hypervisor.</p> <p>SSH can be convenient for remote access, but depends on an OpenSSH server being installed and running within the VM. On first boot, an operational OpenSSH server will need to go through a host-keys generation process which can take anywhere from a few hundred milliseconds to a few seconds, if entropy is low.</p> <p>SSH also depends on a network connection being available, and the system having fully booted.</p>"},{"location":"reference/sos/#set-up-sos-in-your-config-file","title":"Set up SOS in your config file","text":"<p>You'll need to specify the source for authorized SSH keys.</p> <p>This can be one of either, or both of:</p> <ul> <li><code>github_user</code> - a GitHub username to fetch public keys from</li> <li><code>ssh_keys</code> - a list of public keys to add to the VM</li> </ul> <pre><code>config:\n  github_user: &lt;your-github-username&gt;\n  ssh_keys:\n  - &lt;your-ssh-public-key&gt;\n  - &lt;another-ssh-public-key&gt;\n</code></pre> <p>Secondly, you need to bind a specific port and adapter for SOS to listen on.</p> <pre><code>config:\n  ssh:\n    bind_address: `127.0.0.1:`\n    port: 2222\n</code></pre> <p>To limit connections to only those coming from the Slicer host, use <code>127.0.0.1:</code>, but to enable remote SOS access from other hosts use <code>0.0.0.0:</code>.</p> <p>The port number can be anything, but it's suggested that you use a higher port i.e. <code>2221</code> or <code>2222</code>, etc to avoid conflicts.</p> <p>Each Slicer process can only bind to one port, so if you want to run multiple Slicer instances on the same host, you'll need to use different ports.</p>"},{"location":"reference/sos/#connect-to-a-vm-using-sos","title":"Connect to a VM using SOS","text":"<p>When connecting to the SOS, you'll be presented with a menu.</p> <p>This menu lists all running VMs, hit up or down arrows and then \"Enter\" on the target VM.</p> <pre><code>Select a VM (use \u2191\u2193 arrows, Enter to select):\n----------------------------------------\n&gt; vm-1 &lt;\n  Quit\n----------------------------------------\nPress Enter to select, q to quit\n</code></pre> <p>From within the VM sub-menu, you can:</p> <ul> <li>Connect - launch a shell like an SSH session as the root user, but without any need for OpenSSH</li> <li>Pause the VM - with Firecracker microVMs, the VM's vCPUs are stopped, but memory is still allocated</li> <li>Resume the VM - if the VM was paused, this will allow it to continue running from where it left off</li> <li>Shutdown the VM - this is a graceful shutdown, equivalent to running <code>sudo shutdown now</code> within the VM</li> </ul> <pre><code>Actions for VM: vm-1\n----------------------------------------\n&gt; Connect &lt;\n  Pause\n  Shutdown\n  Back\n----------------------------------------\nPress Enter to select, Esc or 'b' to go back\n</code></pre> <p>In order for the shell be be operational, you'll need to make sure the <code>slicer-ssh-agent.service</code> is running within the VM using systemd. This is pre-installed in all the official VM images, but can be disabled for a slightly faster boot time, if you do not need to use SOS.</p> <p>When you connect, you'll be logged in as the <code>root</code> user.</p> <pre><code>Connecting to: vm-1\nWelcome to Ubuntu 22.04.5 LTS (GNU/Linux 5.10.240 x86_64)\n\n * Documentation:  https://help.ubuntu.com\n * Management:     https://landscape.canonical.com\n * Support:        https://ubuntu.com/pro\n\nThis system has been minimized by removing packages and content that are\nnot required on a system that users do not log into.\n\nTo restore this content, you can run the 'unminimize' command.\nroot@vm-1:/root# \n</code></pre> <p>Type <code>exit</code> or press <code>Ctrl+D</code> to exit the shell session.</p> <p>To enter the menu again, reconnect with the SSH command you used earlier.</p>"},{"location":"reference/ssh/","title":"Log into a VM using SSH","text":"<p>This page covers two concepts:</p> <ol> <li>SSH access to a running VM over the network (covered on this page)</li> <li>Serial Over SSH console (SOS)</li> </ol>"},{"location":"reference/ssh/#ssh-access-to-a-running-vm-over-the-network","title":"SSH access to a running VM over the network","text":"<p>Unless you have optimised an image to turn off the bundled OpenSSH server, then it will start when the VM boots.</p> <p>You can configure VMs within a hostgroup with your SSH keys in two ways.</p>"},{"location":"reference/ssh/#using-the-github_user-field","title":"Using the <code>github_user</code> field","text":"<p>The simplest option is to use GitHub. Set your SSH keys on your profile, then they'll be available at <code>https://github.com/USER.keys</code></p> <pre><code>config:\n  github_user: alexellis\n</code></pre> <p>Only one username can be specified within the <code>github_user</code> field.</p>"},{"location":"reference/ssh/#using-the-ssh_keys-field","title":"Using the <code>ssh_keys</code> field","text":"<p>The <code>ssh_keys</code> field removes the dependency on GitHub, and speeds up the boot by avoiding a call over the Internet to a remote server.</p> <p>This method supports multiple keys.</p> <pre><code>config:\n  ssh_keys:\n    - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC3... user@host\n    # For a multi-line key, use YAML's pipe syntax\n    - |\n        ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIB3... user@host\n</code></pre>"},{"location":"reference/ssh/#using-the-userdata-field-to-set-up-ssh-keys","title":"Using the <code>userdata</code> field to set up SSH keys","text":"<p>You can also use the <code>userdata</code> field to set up SSH keys, as shown in the Userdata for Slicer VMs page.</p> <pre><code>config:\n    host_groups:\n    - name: vm\n      userdata: |\n        #!/bin/bash\n        mkdir -p /home/ubuntu/.ssh\n        echo \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC3... user@host\" &gt;&gt; /home/ubuntu/.ssh/authorized_keys\n        chmod 600 /home/ubuntu/.ssh/authorized_keys\n        chown -R ubuntu:ubuntu /home/ubuntu/.ssh\n</code></pre> <p>If you're running a different OS image such as Rocky Linux, make sure you change the home directory and user/group name accordingly.</p> <p>The <code>TARGET_USER</code> environment variable will also be set within the context of the <code>userdata</code> script, so you could use that instead of hard-coding <code>/home/ubuntu</code> or <code>/home/rocky</code>.</p>"},{"location":"reference/troubleshooting/","title":"Troubleshooting Slicer","text":"<p>The best place to get help with the Home Edition is via Discord. For Pro users, email support is available, check the welcome email for details.</p>"},{"location":"reference/troubleshooting/#doesnt-boot-right-or-has-a-networking-issue-perhaps-github-keys-arent-being-imported","title":"Doesn't boot right or has a networking issue - perhaps GitHub keys aren't being imported?","text":"<p>Look in the log file outputted from slicer. So if you are running i.e. <code>k3s</code> as the host group and VM 1 isn't booting:</p> <pre><code>sudo cat /var/log/slicer/k3s-1.txt\n</code></pre> <p>If you can get into the VM via the SOS console, then run the following:</p> <pre><code>sudo journalctl -u mount-config --no-pager\n</code></pre> <p>Look for networking issues, or a bad routing.</p> <p>If your networking equipment is forcing the microVMs to use IPv6, but you do not have IPv6 connectivity, then you could try to disable IPv6.</p> <p>The host is easier to fix and may mean you can leave the microVMs as they are.</p> <pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.lo.disable_ipv6=1\n</code></pre> <p>To make it permanent:</p> <p>Then add the following lines to <code>/etc/sysctl.conf</code>:</p> <pre><code>net.ipv6.conf.all.disable_ipv6 = 1\nnet.ipv6.conf.default.disable_ipv6 = 1\nnet.ipv6.conf.lo.disable_ipv6 = 1\n</code></pre> <p>Alternatively, you could try disabling only within the microVM on first boot:</p> <pre><code>config:\n    host_groups:\n    - name: k3s\n      userdata: |\n        #!/bin/bash\n        sysctl -w net.ipv6.conf.all.disable_ipv6=1\n        sysctl -w net.ipv6.conf.default.disable_ipv6=1\n        sysctl -w net.ipv6.conf.lo.disable_ipv6=1\n\n        echo \"net.ipv6.conf.all.disable_ipv6 = 1\" |tee -a /etc/sysctl.conf\n        echo \"net.ipv6.conf.default.disable_ipv6 = 1\" |tee -a /etc/sysctl.conf\n        echo \"net.ipv6.conf.lo.disable_ipv6 = 1\" |tee -a /etc/sysctl.conf\n\n        # Cause the SSH keys to re re-imported from GitHub on the next boot\n\n        rm -rf /home/ubuntu/.ssh/github_keys_imported\n\n        # Optionally, reboot the VM to re-import the keys from GitHub\n        # reboot\n</code></pre> <p>If you're having issues reaching GitHub for your SSH keys, you can set them manually in the config or userdata.</p>"},{"location":"reference/troubleshooting/#the-problem-may-be-fixed-by-upgrading-slicer","title":"The problem may be fixed by upgrading Slicer","text":"<p>You can upgrade the Slicer binary by running the instructions at the end of the installation page.</p>"},{"location":"reference/troubleshooting/#i-cant-connect-over-ssh","title":"I can't connect over SSH","text":"<p>Have a look at the VM's serial console, written to: <code>/var/log/slicer/vm-1.txt</code> where <code>1</code> is the VM number, and vm is the host group name. </p>"},{"location":"reference/troubleshooting/#the-vm-cant-connect-to-the-internet","title":"The VM can't connect to the Internet","text":"<p>This can occur when there are old or stable routes in place, for instance, if you added a route for <code>192.168.137.0/24</code> on another host, but are now running Slicer on your own workstation.</p> <p>Check routes with:</p> <pre><code>sudo ip route\n</code></pre> <p>Then delete one with i.e. <code>sudo ip route del 192.168.137.0/24</code>.</p>"},{"location":"reference/troubleshooting/#ive-run-out-of-disk-space","title":"I've run out of disk space","text":"<p>There are three places to perform a prune/clean-up.</p> <ol> <li>Check there are not a lot of unused <code>.img</code> files from various launches of VMs</li> </ol> <pre><code>sudo find / -name *.img\n</code></pre> <p>Delete the ones you no longer require. Beware of deleting .img files for VMs that you still need.</p> <ol> <li>If you've been working with custom images, prune them from the containerd library:</li> </ol> <pre><code>sudo ctr -n slicer i ls\n\nsudo ctr -n slicer i ls -q |xargs sudo ctr -n slicer i rm\n</code></pre> <p>If you are using a snapshotter like ZFS or Devmapper, the above command will delete all images and their snapshots.</p> <p>So to be more selective, you can delete individual images by name:</p> <p>For instance:</p> <pre><code>sudo ctr -n slicer i rm docker.io/library/ubuntu:22.04\n</code></pre> <ol> <li>Remove the /var/run folder for Slicer</li> </ol> <p>The <code>/var/run</code> and <code>/var/log</code> folders contain logs, sockets, temporary disks, and extracted Kernel files. This can build up over time. <code>/var/run</code> is generally ephemeral, and removed on each reboot.</p> <pre><code>sudo rm -rf /var/run/slicer\nsudo rm -rf /var/log/slicer\n</code></pre> <p>For the nuclear option, delete all of the containerd's data, this will remove all images, snapshots, including any from containers that you run via Docker, if that's also installed.</p> <pre><code>sudo systemctl stop containerd\nsudo rm -rf /var/lib/containerd\nsudo systemctl restart containerd\n</code></pre>"},{"location":"reference/vfio/","title":"VFIO Passthrough","text":""},{"location":"reference/vfio/#vfio-virtual-function-io-passthrough","title":"VFIO (Virtual Function I/O) Passthrough","text":"<p>VFIO (Virtual Function I/O) is a Linux kernel framework that allows a PCI device to be directly assigned to a virtual machine (VM). This enables the VM to have direct access to the hardware device, providing near-native performance and functionality.</p> <p>Any device such as a GPU or NIC that is passed through to a VM must first be unbound from its current driver on the host, and is exclusively bound to the VFIO driver.</p> <p>Slicer supports VFIO passthrough for Nvidia GPUs when using Cloud Hypervisor as the hypervisor as per the example Ollama with a GPU.</p> <p>Support for NICs for router appliances will be coming shortly.</p> <p>VFIO is limited to x86_64 systems with hardware support for IOMMU (Intel VT-d or AMD-Vi).</p>"},{"location":"reference/vfio/#enable-vfio-on-start-up","title":"Enable VFIO on start-up","text":"<p>You must edit the <code>cmdline</code> argument of your bootloader to include the following parameters.</p> <p>Intel:</p> <pre><code>intel_iommu=on iommu=pt\n</code></pre> <p>AMD:</p> <pre><code>amd_iommu=on iommu=pt\n</code></pre> <p>If you're using Grub as a bootloader, then edit the <code>/etc/default/grub</code> file and add the parameters to the <code>GRUB_CMDLINE_LINUX_DEFAULT</code> variable. For example:</p> <pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash intel_iommu=on iommu=pt\"\n</code></pre> <p>Then update Grub:</p> <pre><code>sudo update-grub\n</code></pre> <p>Then update your initramfs:</p> <pre><code>sudo update-initramfs -c -k $(uname -r)\n</code></pre> <p>Now reboot your system.</p>"},{"location":"reference/vfio/#optional-bind-devices-to-vfio-on-start-up","title":"Optional: Bind devices to VFIO on start-up","text":"<p>We recommend unbinding and rebinding devices to VFIO using the <code>slicer pci</code> commands either as and when they're required, or via a systemd unit on start-up.</p> <p>But, you can also bind devices to VFIO via the <code>cmdline</code> argument in your bootloader configuration.</p> <p>Note: That this method only works at the vendor/device ID level, so if you have multiple GPUs, or multiple NICs of the same time, it's very unlikely that you'll want to bind them all to VFIO because they will not be accessible to the host.</p> <p>Use <code>sudo -E slicer pci list</code> to find the vendor and device IDs of the devices you want to bind to VFIO.</p> <p>Then add them to the <code>cmdline</code> argument as follows, replacing the example IDs below with your own:</p> <pre><code>vfio-pci.ids=10de:2204,10de:1aef\n</code></pre>"},{"location":"reference/vfio/#view-pci-devices-and-their-iommu-groups","title":"View PCI Devices and their IOMMU Groups","text":"<p>To view the PCI devices and their IOMMU groups, you can use the following command:</p> <pre><code>sudo -E slicer pci list\n</code></pre> <p>Example output:</p> <pre><code>ADDRESS      CLASS    VENDOR   DEVICE   DESCRIPTION                              IOMMU GROUP  VFIO GROUP   DRIVER\n-------      -----    ------   ------   ---------------------------------------- ----------   ----------   ------\n0000:0b:00.0 0300     10de     2204     VGA compatible controller: NVIDIA Cor... 28           28           vfio-pci\n0000:0b:00.1 0403     10de     1aef     Audio device: NVIDIA Corporation GA10... 28           28           vfio-pci\n0000:0c:00.0 0300     10de     2204     VGA compatible controller: NVIDIA Cor... 29           29           vfio-pci\n0000:0c:00.1 0403     10de     1aef     Audio device: NVIDIA Corporation GA10... 29           29           vfio-pci\n</code></pre>"},{"location":"reference/vfio/#bind-a-device-to-vfio","title":"Bind a device to VFIO","text":"<p>You can bind a device to VFIO in two ways:</p> <ol> <li>By specifying the device's vendor and device ID in the cmdline arguments for the Kernel.</li> <li>By using the <code>slicer pci</code> command to bind the device to VFIO.</li> </ol> <p>We recommend only using 2. because 1. is unable to differentiate between multiple devices of the same type such as two or more NICs or two or more GPUs. You often need at least one of these to be available for the host.</p> <p>View PCI devices via <code>sudo -E slicer pci list</code>.</p> <p>Then run <code>sudo -E slicer pci bind &lt;PCI_ADDRESS&gt;</code> to bind the device to VFIO. Replace <code>&lt;PCI_ADDRESS&gt;</code> with the actual PCI address of the device you want to bind.</p>"},{"location":"reference/vfio/#unbind-a-device-from-vfio","title":"Unbind a device from VFIO","text":"<p>To unbind a device from VFIO, you can use the following command:</p> <pre><code>sudo -E slicer pci unbind &lt;PCI_ADDRESS&gt;\n</code></pre>"},{"location":"reference/vfio/#rebind-a-device-to-its-original-driver","title":"Rebind a device to its original driver","text":"<p>Once unbound, you can use <code>bind</code> with the <code>--driver</code> flag to re-bind it to the original driver on your host system. Or reboot if that's easier.</p> <p>In the case of the GPU example above, you may want to allocate one of the bound GPUs back to the host for display purposes.</p> <pre><code>sudo -E slicer pci bind 0000:0b:00.0 --driver=nvidia\nsudo -E slicer pci bind 0000:0b:00.1 --driver=snd_hda_intel\n</code></pre>"},{"location":"reference/vfio/#troubleshooting","title":"Troubleshooting","text":"<ol> <li>Ensure that your CPU and motherboard support IOMMU and that it is enabled in the BIOS/UEFI settings.</li> <li>Also double-check your bootloader i.e. Grub configuration for the command line that's passed to the Linux Kernel. Did you skip <code>update-grub</code> or <code>update-initramfs</code>?</li> <li>Check the output of <code>sudo dmesg | grep -e DMAR -e IOMMU</code> for any errors related to IOMMU initialization.</li> <li>Ensure that the device you are trying to passthrough is not being used by the host system and that it's not already bound to a specific driver.</li> <li>Verify that the device is in its own IOMMU group using <code>sudo -E slicer pci list</code> - you typically have to bind every device within an IOMMU group otherwise they cannot be used in a VM.</li> </ol> <p>After checking all of the above, if you find your devices are all mixed into the same IOMMU group, that means your system is not designed for VFIO.</p> <p>As an alternative, you can deploy/run the so called \"ACS patches\", but they may also have certain security or stability implications. Use at your own risk.</p>"},{"location":"storage/devmapper/","title":"Devmapper storage for Slicer","text":"<p>Devmapper is one of the options for storage with Slicer.</p> <p>We generally recommend using disk images for long-running VMs, or ZFS for launching many short-lived VMs.</p> <p>That said, you can install Devmapper as an alternative, with a backing drive set up for it, just like we do for ZFS.</p>"},{"location":"storage/devmapper/#installation","title":"Installation","text":"<p>The setup for Devmapper is different to ZFS, you will need to run the installation script again, but this time, with additional arguments.</p> <p>First, install a drive, or make a partition available for Devmapper to use.</p> <p>Run <code>lsblk</code> to identify your drives.</p> <pre><code>$ lsblk\nNAME                             MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS\nnvme1n1                          259:0    0 931.5G  0 disk \n\u251c\u2500nvme1n1p1                      259:1    0     1G  0 part /boot/efi\n\u251c\u2500nvme1n1p2                      259:2    0     2G  0 part /boot\n\u2514\u2500nvme1n1p3                      259:3    0 928.5G  0 part \n  \u2514\u2500ubuntu--vg-ubuntu--lv        253:0    0   500G  0 lvm  /\nnvme0n1                          259:4    0   1.8T  0 disk \n</code></pre> <p>In this instance, you can see that my 2TB NVMe SSD is called nvme0n1 and is currently not allocated.</p> <p>Head over to the installation page and run the installation script again, this time include the <code>VM_DEV</code> environment variable.</p> <p>The default size for any unpacked VM is <code>30GB</code>, so if you want to alter that size do it now.</p> <pre><code>(\ncd agent\nBASE_SIZE=35GB VM_DEV=/dev/nvme0n1 sudo -E ./install.sh\n)\n</code></pre> <p>Be very careful that you specify the correct drive or partition. This operation cannot be reserved, and will destroy any existing contents.</p>"},{"location":"storage/devmapper/#configure-slicer-to-use-devmapper","title":"Configure Slicer to use Devmapper","text":"<pre><code>config:\n  host_groups:\n  - name: vm\n-   storage: image\n-   storage_size: 25G\n+   storage: devmapper\n</code></pre> <p>The <code>storage_size</code> field cannot be specified for Devmapper. So whatever size was setup for snapshots during the installation will be the size used for all VMs.</p>"},{"location":"storage/overview/","title":"Storage for Slicer","text":"<p>In the walkthrough, we show how to use disk images for storage.</p> <p>Disk images were not the first choice for Slicer, initially, only snapshotting filesystems were supported. With a snapshot, the initial filesystem is \"unpacked\" once - taking anywhere between 30 and 90 seconds, then can be cloned instantly.</p> <ul> <li>Disk Images - static files allocated on start-up, can take a few seconds to clone the initial disk</li> <li>Snapshotting / Copy On Write (CoW) - dynamic filesystems that only store changes and allow for instant cloning</li> </ul> <p>The current installation supports Disk Images only, and we will add the instructions for ZFS shortly.</p>"},{"location":"storage/overview/#disk-images","title":"Disk images","text":"<p>Disk images are similar to loopback filesystems which you may have created in the past via <code>fallocate</code> and <code>mkfs.ext4</code>.</p> <p>Pros:</p> <ul> <li>Convenient, with a custom disk size</li> <li>Easy to manage and migrate between systems</li> </ul> <p>Cons:</p> <ul> <li>No deduplication, as is possible with snapshots/Copy On Write (CoW) systems</li> <li>Launch/clone time slower than CoW when launching many VMs</li> </ul>"},{"location":"storage/overview/#zfs","title":"ZFS","text":"<p>ZFS is an advanced filesystem that supports CoW snapshots.</p> <p>Pros:</p> <ul> <li>Built-in support for snapshots and deduplication</li> <li>Instant clone of base snapshot - ideal for launching many VMs</li> <li>Easier to troubleshoot and understand than devmapper</li> </ul> <p>Cons:</p> <ul> <li>More complex to set up and manage than disk images</li> <li>Requires an additional disk or partition</li> <li>Additional setup required</li> <li>No custom sizing - must match the base snapshot</li> </ul> <p>Setup ZFS storage for Slicer</p>"},{"location":"storage/overview/#devmapper","title":"Devmapper","text":"<p>Devmapper is available for Slicer, but not set up by default and requires additional setup.</p> <p>Generally, we'd recommend using ZFS instead of devmapper unless you have a specific need for it.</p> <p>Pros:</p> <ul> <li>Reasonably well known from the Docker space</li> <li>Instant clone of base snapshot</li> </ul> <p>Cons:</p> <ul> <li>Requires an additional disk or partition</li> <li>No custom size for VMs - the size of any VM must match the base snapshot</li> <li>Difficult to debug and troubleshoot - it's easier to recreate the whole storage pool</li> </ul> <p>Setup devmapper storage for Slicer</p>"},{"location":"storage/zfs/","title":"ZFS Storage Pools for Slicer","text":"<p>ZFS is an advanced filesystem that was originally developed at Sun Microsystems. The modern equivalent for Linux was ported as the OpenZFS project and has a license that is incompatible with the Linux Kernel, for that reason, it is kept out of tree, and must be installed separately.</p> <p>Whilst ZFS can run on a loopback device, this is not recommended and may be unstable. Instead, dedicated either a partition or a drive to running ZFS.</p>"},{"location":"storage/zfs/#use-zfs-for-vm-storage","title":"Use ZFS for VM storage","text":"<p>Before you can start using <code>zfs</code> storage for your VMs, you'll have to run through the installation steps below.</p> <p>Let's customise the walkthrough example for ZFS.</p> <p>1) Change the <code>storage</code> type from <code>image</code> to <code>zfs</code>:</p> <pre><code>config:\n  host_groups:\n  - name: vm\n-   storage: image\n+   storage: zfs\n-   storage_size: 20G\n</code></pre> <p>See the note below on <code>storage_size</code>.</p> <p>2) Customise the <code>storage_size</code></p> <p>The <code>storage_size</code> field is optional for ZFS.</p> <p>If not specified, the default size of the base snapshot will be used. A custom size can be given, so long as it is equal to or larger than the base snapshot size.</p> <pre><code>config:\n  host_groups:\n  - name: vm\n    storage: zfs\n+   storage_size: 40G\n</code></pre> <p>If the base snapshot size is large for any existing VMs, then you can find its lease and remove it before having it re-created with the new settings for the vzol-snapshotter.</p> <pre><code>$ sudo ctr -n slicer leases ls\nID            CREATED AT           LABELS \nslicer/k3s-1  2025-09-04T13:53:07Z -      \n</code></pre> <p>Then, find the lease ID for the VM in question, and delete it. The lease ID is the hostname of the VM i.e.<code>k3s-1</code>.</p> <pre><code>sudo ctr -n slicer leases rm slicer/k3s-1\n</code></pre> <p>To delete all leases:</p> <pre><code>sudo ctr -n slicer leases ls -q | xargs -n1 sudo ctr -n slicer leases rm\n</code></pre>"},{"location":"storage/zfs/#install-packages-for-zfs","title":"Install packages for ZFS","text":"<pre><code>sudo apt update\nsudo apt install -y zfsutils-linux\n</code></pre> <p>The two commands you are likely to need are:</p> <ul> <li><code>zpool</code> - create and explore pools to be used by ZFS</li> <li><code>zfs</code> - manage ZFS filesystems and snapshots</li> </ul>"},{"location":"storage/zfs/#create-a-pool-for-slicer","title":"Create a pool for Slicer","text":"<p>Let's say that you have a local NVMe i.e. <code>/dev/nvme1n1</code>.</p> <p>Within that device, you may have <code>/dev/nvme1n1p1</code> for your operating system, and some free space at <code>/dev/nvme1n1p2</code>.</p> <p>Use the following command to enroll that free space into ZFS:</p> <pre><code>sudo zpool create slicer /dev/nvme1n1p2\n</code></pre> <p>ZFS has many different options such as checksuming, compression, deduplication, encryption, and the ability to run in different RAID-like modes across multiple drives.</p> <p>We recommend that you do the simplest thing first, to get it working before tinkering further.</p>"},{"location":"storage/zfs/#create-a-filesystem-for-slicer","title":"Create a filesystem for Slicer","text":"<pre><code>sudo zfs create slicer/snapshots\n</code></pre>"},{"location":"storage/zfs/#install-the-zvol-snapshotter-for-containerd","title":"Install the zvol-snapshotter for containerd","text":"<p>The containerd project already has a ZFS snapshotter, however it is unsuitable for use for VMs, therefore we needed to implement our own snapshotter which can present ZFS volumes as block devices to a microVM.</p> <p>The zvol-snapshotter can be installed using arkade:</p> <pre><code>arkade system install zvol-snapshotter \\\n  --dataset slicer/snapshots\n</code></pre> <p>You can specify the size of any VM drive that will be created by the snapshotter using the <code>--size</code> flage, e.g. <code>--size=40G</code>. The default size is <code>20G</code>.</p> <p>Configure containerd to enable Zvol snapshotter, edit <code>/etc/containerd/config.toml</code> and add:</p> <pre><code>[proxy_plugins]\n  [proxy_plugins.zvol]\n    type = \"snapshot\"\n    address = \"/run/containerd-zvol-grpc/containerd-zvol-grpc.sock\"\n</code></pre> <p>Restart containerd:</p> <pre><code>sudo systemctl restart containerd\n</code></pre> <p>Check if the snapshotter is running OK:</p> <pre><code>sudo journalctl -u zvol-snapshotter -f\n</code></pre> <p>Now, test the Zvol snapshotter:</p> <pre><code>(\nsudo -E ctr images pull --snapshotter zvol docker.io/library/hello-world:latest\nsudo -E ctr run --snapshotter zvol docker.io/library/hello-world:latest test\n)\n</code></pre>"},{"location":"storage/zfs/#adjust-the-base-snapshot-size","title":"Adjust the base snapshot size","text":"<p>The base snapshot size can be configured when installing or updating the snapshotter with <code>arkade system install</code> by including the <code>--size</code> flag.</p> <p>You can also manually edit the snapshotter configuration file.</p> <p>Edit <code>/etc/containerd-zvol-grpc/config.toml</code> and replace the volume size with the desired value, e.g <code>40G</code>:</p> <pre><code>root_path=\"/var/lib/containerd-zvol-grpc\"\ndataset=\"your-zpool/snapshots\"\n-volume_size=\"30G\"\n+volume_size=\"40G\"\nfs_type=\"ext4\"\n</code></pre> <p>Finally reload and restart the service:</p> <pre><code>sudo systemctl restart zvol-snapshotter\n</code></pre>"},{"location":"storage/zfs/#footnote-on-zfs-on-a-loopback-file","title":"Footnote on ZFS on a loopback file","text":"<p>It is absolutely not recommended to use a loopback file for ZFS for Slicer.</p> <pre><code>$ fallocate -l 250G zfs.img\n$ sudo zpool create slicer $HOME/zfs.img\n\nzpool list\nNAME     SIZE  ALLOC   FREE  CKPOINT  EXPANDSZ   FRAG    CAP  DEDUP    HEALTH  ALTROOT\nslicer   248G   105K   248G        -         -     0%     0%  1.00x    ONLINE  -\n\n$ zfs list\nNAME     USED  AVAIL     REFER  MOUNTPOINT\nslicer   105K   240G       24K  /slicer\n</code></pre>"},{"location":"tasks/custom-image/","title":"Build a custom root filesystem","text":"<p>You can customise a Slicer VM in two ways:</p> <ol> <li>Via userdata on first boot (a bash script included via the config file)</li> <li>By extending an existing root filesystem with Docker and adding various <code>COPY</code> and <code>RUN</code> statements</li> </ol> <p>When building within Docker, you must bear in mind that not all of the files shown to you will persist into the final image.</p> <p>Additionally, if you're copying and pasting commands from installation guides, certain <code>systemctl</code> commands will need to be adapted.</p> <p>If a guide gave you <code>systemctl enable --now bind</code> for a custom DNS server, you'd simply remove the <code>--now</code> flag. The meaning of <code>--now</code> is to instruct the build container to start a systemd service immediately - that's not possible or desirable during a build.</p> <p>In short, if you are trying to extend a Slicer image, treat it as if it were a Docker container, and only run commands that you know work in a regular Dockerfile.</p> <p>For anything that does not run or work within a Dockerfile, either use userdata to run the steps on first boot, or add a one-shot systemd unit file that will run your custom steps on first boot.</p>"},{"location":"tasks/custom-image/#build-a-custom-image","title":"Build a custom image","text":"<p>First, refer to the image you want to customise - whether it's for aarch64 or x86_64.</p> <p>Then create a Dockerfile with a <code>FROM</code> line specifying the base image you want to use. For example:</p> <pre><code>FROM ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-latest\n</code></pre> <p>If you wanted to install Nginx and have it start automatically, with a website you've created, you could add the following lines to your Dockerfile.</p> <p>Nginx will run in systemd, we should not try to change the CMD instruction.</p> <pre><code>+RUN apt-get update &amp;&amp; apt-get install -y nginx\n+RUN systemctl enable nginx\n+COPY ./my-website /var/www/html\n</code></pre> <p>Then build and publish the image to your own registry:</p> <pre><code>docker build -t docker.io/alexellis2/slicer-nginx:5.10.240-x86_64 .\ndocker push docker.io/alexellis2/slicer-nginx:5.10.240-x86_64\n</code></pre> <p>Then edit your Slicer YAML and replace the <code>image:</code> with <code>docker.io/alexellis2/slicer-nginx:5.10.240-x86_64</code>.</p> <p>If you wanted Docker to be pre-installed into all your VMs, with the default user already set up for access, you could write:</p> <pre><code>+RUN curl -sLS https://get.docker.com | sh\n+RUN usermod -aG docker ubuntu\n</code></pre> <p>If you want to run a local registry, without TLS authentication enabled, you can do so with the following within your YAML file:</p> <pre><code>config:\n  insecure_registry: true\n</code></pre> <p>Then if you want to run a temporary Docker registry on another machine on your network:</p> <pre><code>docker run -d -p 5000:5000 --restart always \\\n  --name registry registry:3\n</code></pre>"},{"location":"tasks/monitoring/","title":"Logs &amp; Monitoring","text":""},{"location":"tasks/monitoring/#logs","title":"Logs","text":"<p>The output of the serial console which contains boot-up messages and output from the init system is available at <code>/var/log/slicer</code>.</p> <p>So for the quickstart, with 1x VM in a hostgroup named vm, you can view the log with:</p> <pre><code>sudo tail -f /var/log/slicer/vm-1.txt\n</code></pre> <p>These logs are also available via the REST API or the CLI:</p> <pre><code>sudo -E slicer logs vm-1               # Tail the last 20 lines (default)\nsudo -E slicer logs vm-1 --lines 50    # Tail the last 50 lines\nsudo -E slicer logs vm-1 --lines 0     # Print the whole file\n</code></pre>"},{"location":"tasks/monitoring/#monitoring","title":"Monitoring","text":"<p>When the <code>slicer-vmmeter.service</code> is loaded and running, then system utilization data can be collected via the <code>slicer vm top</code> command.</p> <p>If you need more monitoring that is available, feel free to let us know what you're looking for.</p>"},{"location":"tasks/monitoring/#view-utilization-via-slicer-vm-top","title":"View utilization via <code>slicer vm top</code>","text":"<pre><code>$ slicer vm top\n\nHOST   IP             CPU  L1    L5    L15   MEM_USED  MEM_FREE  DISK_USED%  DISK_FREE  NET_RX/s  NET_TX/s  DR/s  DW/s  UP\nk3s-2  192.168.136.3  2    0.00  0.00  0.00  159MB     3669MB    2.2         29.2GB     0B/s      0B/s      0B/s  0B/s  7m45s\nk3s-1  192.168.136.2  2    0.00  0.00  0.00  154MB     3674MB    2.2         29.2GB     0B/s      0B/s      0B/s  0B/s  7m45s\n</code></pre> <p>Breakdown:</p> <ul> <li>CPU - vCPU allocated</li> <li>L1, L5, L15 - Load average over 1, 5 and 15 minutes</li> <li>MEM_USED - Memory used by the VM</li> <li>MEM_FREE - Memory free in the VM</li> <li>DISK_USED% - Percentage of disk used in the VM</li> <li>DISK_FREE - Disk space free in the VM</li> <li>NET_RX/s - Network received per second</li> <li>NET_TX/s - Network transmitted per second</li> <li>DR/s - Disk read per second</li> <li>DW/s - Disk write per second</li> <li>UP - Uptime of the VM</li> </ul> <p>To simulate resource usage, you could download Geekbench 6 and run a benchmark. Note that the Arm preview for Geekbench 6 may not fully complete on an Arm system. </p> <p>If slicer is running remotely:</p> <pre><code>$ slicer vm top --api http://192.168.1.114:8080\n</code></pre> <p>If slicer is running remotely but requires authentication:</p> <pre><code>$ slicer vm top --api http://192.168.1.114:8080 --token TOKEN_VALUE\n$ slicer vm top --api http://192.168.1.114:8080 --token-file TOKEN_VALUE\n</code></pre> <p>When slicer is running locally, and authentication is enabled, use <code>sudo -E</code> and slicer will attempt to read the auth token from the default location of <code>/var/lib/slicer/token</code>.</p> <pre><code>$ sudo slicer vm top\n</code></pre>"},{"location":"tasks/monitoring/#automated-monitoring-with-node_exporter","title":"Automated monitoring with <code>node_exporter</code>","text":"<p>The Open Source node_exporter project from the Prometheus project can be used to collect system metrics from each VM, you can install the agent as a systemd unit file through userdata, or a custom base image.</p> <p>Prometheus or the Grafana Agent can then be run on the host, or somewhere else to collect the metrics and store them in a time-series database.</p>"},{"location":"tasks/nested-virtualization/","title":"Nested Virtualization with Slicer","text":"<p>Nested virtualization refers to running a virtual machine within another.</p> <p>Generally, you should always aim to run microVMs directly on bare-metal for the best performance and lowest overheads. There are a few use-cases where that is not possible, so nested virtualization provides an alternative at the trade-off of some additional latency.</p> <p>There are three main use-cases for nested virtualization with Slicer.</p> <ol> <li>Your primary OS is MacOS or Windows, which means you have to run Slicer within a Linux VM.</li> <li>You only have access to cloud VMs from DigitalOcean, Azure, or Google Cloud Platform (GCP) rather than bare-metal servers.</li> <li>You want to run Slicer within Slicer for testing and experimentation.</li> </ol> <p>For the first two use-cases, there's nothing extra for you to do. You're simply running all the Slicer commands within an existing VM.</p> <p>For the third use-case, to run Slicer within Slicer, you just need to pay careful attention to the IP addresses and routing configurations if you want the host to be accessible on the outer network.</p> <p></p> <p>This image shows an example of nested virtualization setup running on my Intel N100.</p>"},{"location":"tasks/nested-virtualization/#slicer-within-slicer","title":"Slicer within Slicer","text":"<p>Pictured: Connecting to a VM within a nested Slicer instance from an outer host on the same physical network as the virtualization host.</p> <p>We are assuming you want to access VMs from your workstation IP <code>192.168.1.10</code>, and that the virtualization host running Slicer is on <code>192.168.1.11</code> (it may be an Intel N100, or an old Intel NUC for instance).</p> <p>On the virtualization host (192.168.1.11), there will be two slicer instances running, one within the other.</p> <p>The outer slicer's IP range will be <code>192.168.137.0/24</code>.</p> <p>The nested or inner slicer's IP range will be <code>192.168.130.0/24</code>.</p> <p>On the virtualization host, install Slicer, then copy and paste the example from the walkthrough and save it as <code>nested.yaml</code>.</p> <p>Change hostgroup's <code>name</code> field to <code>nested</code>, and take a note of the IP range i.e. <code>192.168.137.0/24</code>. If the IP range is not already set to <code>192.168.137.0/24</code>, update it.</p> <p>Start up slicer with the config file:</p> <pre><code>sudo -E slicer up ./nested.yaml\n</code></pre> <p>Once the VM is started-up, add a route on your workstation (<code>192.168.1.10</code>) using the output printed during VM launch.</p> <p>That will be something like:</p> <pre><code>sudo ip route add 192.168.137.0/24 via 192.168.1.11\n</code></pre> <p>Next, connect into the first VM launced by Slicer using SSH:</p> <pre><code>ssh ubuntu@192.168.137.2\n</code></pre> <p>Now perform an installation of Slicer within the VM.</p> <p>Copy the walkthrough example YAML from the walkthrough and save it as <code>nested.yaml</code>.</p> <p>Edit the IP address of the nested VM to <code>192.168.130.2</code>.</p> <p>Start Slicer within the VM:</p> <pre><code>sudo -E slicer up ./nested.yaml\n</code></pre> <p>Now add a route to the nested VM's IP range on your workstation (<code>192.168.1.10</code>):</p> <pre><code>sudo ip route add 192.168.130.0/24 via 192.168.1.11\n</code></pre> <p>Note how the gateway given is the IP of the virtualization host, and not the IP of the nested VM.</p> <p>Now connect to either of the two VMs using SSH:</p> <pre><code># Connect to the first VM\nssh ubuntu@192.168.137.2\n\n# Connect to the second VM\nssh ubuntu@192.168.130.2\n</code></pre>"},{"location":"tasks/share-files-with-nfs/","title":"Share files between host and VM with NFS","text":"<p>NFS (Network File System) provides a simple way to share files between your host machine and Slicer VMs. This is particularly useful for development workflows, data processing, or when you need persistent storage that survives VM restarts.</p> <p>This example shows you how to set up an NFS server on your host machine and mount it from within a Slicer VM.</p>"},{"location":"tasks/share-files-with-nfs/#prerequisites","title":"Prerequisites","text":"<p>You'll need a running Slicer VM. Follow the walkthrough to create one if you haven't already.</p>"},{"location":"tasks/share-files-with-nfs/#set-up-the-nfs-server-on-the-host","title":"Set up the NFS server on the host","text":"<p>Install the NFS kernel server on your host machine:</p> <pre><code>sudo apt update &amp;&amp; sudo apt install -y nfs-kernel-server\n</code></pre> <p>Create a directory to share with your VMs:</p> <pre><code>sudo mkdir -p /srv/slicer_share\nsudo chown nobody:nogroup /srv/slicer_share\n</code></pre> <p>Configure the NFS exports by editing <code>/etc/exports</code>:</p> <pre><code>sudo nano /etc/exports\n</code></pre> <p>Add the following line to allow your Slicer network access to the share. Replace the network CIDR with your actual Slicer network configuration (the default usded in the walkthrough is <code>192.168.137.0/24</code>):</p> <pre><code>/srv/slicer_share 192.168.137.0/24(rw,sync,no_subtree_check)\n</code></pre> <p>Apply the configuration changes:</p> <pre><code>sudo exportfs -ra\n</code></pre> <p>Start and enable the NFS server:</p> <pre><code>sudo systemctl enable --now nfs-server\n</code></pre> <p>You can verify the export is active:</p> <pre><code>sudo exportfs -v\n</code></pre>"},{"location":"tasks/share-files-with-nfs/#mount-the-nfs-share-in-your-vm","title":"Mount the NFS share in your VM","text":"<p>This section provides manual setup instructions. For an automated setup using userdate, see Automate NFS setup with userdata</p> <p>Connect to your VM via SSH:</p> <pre><code>ssh ubuntu@192.168.137.2\n</code></pre> <p>Inside the VM, install the NFS client utilities:</p> <pre><code>sudo apt update &amp;&amp; sudo apt install -y nfs-common\n</code></pre> <p>Create a mount point:</p> <pre><code>sudo mkdir -p /mnt/slicer_share\n</code></pre> <p>Mount the NFS share from your host (replace <code>192.168.137.1</code> with your actual host IP if different):</p> <pre><code>sudo mount -t nfs 192.168.137.1:/srv/slicer_share /mnt/slicer_share\n</code></pre> <p>Test the connection by creating a file:</p> <pre><code>echo \"Hello from VM!\" | sudo tee /mnt/slicer_share/test.txt\n</code></pre> <p>You should be able to see this file on your host at <code>/srv/slicer_share/test.txt</code>.</p>"},{"location":"tasks/share-files-with-nfs/#make-the-mount-persistent","title":"Make the mount persistent","text":"<p>To automatically mount the NFS share when the VM boots, add it to <code>/etc/fstab</code>:</p> <pre><code>echo \"192.168.137.1:/srv/slicer_share /mnt/slicer_share nfs defaults 0 0\" | sudo tee -a /etc/fstab\n</code></pre> <p>Test the fstab entry:</p> <pre><code>sudo umount /mnt/slicer_share\nsudo mount -a\n</code></pre>"},{"location":"tasks/share-files-with-nfs/#automate-nfs-setup-with-userdata","title":"Automate NFS setup with userdata","text":"<p>You can automate the NFS client setup by adding userdata to your VM configuration.</p> <pre><code>config:\n  host_groups:\n  - name: vm\n    userdata: |\n      #!/bin/bash\n      # Install NFS client\n      apt update &amp;&amp; apt install -y nfs-common\n\n      # Create mount point\n      mkdir -p /mnt/slicer_share\n\n      # Add to fstab for persistent mounting\n      echo \"192.168.137.1:/srv/slicer_share /mnt/slicer_share nfs defaults 0 0\" &gt;&gt; /etc/fstab\n\n      # Mount immediately\n      mount -a\n</code></pre> <p>With this configuration, you can increase the VM count in your host group and all VMs will automatically have the NFS share mounted.</p>"},{"location":"tasks/userdata/","title":"Userdata for Slicer VMs","text":"<p>Userdata can be used to customise VMs on first boot. Another option is to build a custom image.</p> <p>This can include anything from installing packages, configuring services, or setting up user accounts.</p> <p>If you wanted to run AI agents within Slicer, you could use userdata to install a custom AI framework, library, or agent for Remote Procedure Control (RPC).</p>"},{"location":"tasks/userdata/#inline-userdata-in-the-config-file","title":"Inline userdata in the config file","text":"<pre><code>config:\n  host_groups:\n  - name: agent\n+   userdata: |\n+     # Install Ollama\n+     curl -fsSL https://ollama.com/install.sh | sh\n+\n+     # Install a hard-coded SSH key for remote administration\n+\n+     echo \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC3... user@host\" | tee -a /home/ubuntu/.ssh/authorized_keys\n+     chmod 600 /home/ubuntu/.ssh/authorized_keys\n+     chown ubuntu:ubuntu /home/ubuntu/.ssh/authorized_keys\n</code></pre> <p>You could also install something like Docker like this:</p> <pre><code>config:\n  host_groups:\n  - name: agent\n+   userdata: |\n+     # Install Docker\n+     curl -fsSL https://get.docker.com | sh\n</code></pre>"},{"location":"tasks/userdata/#userdata-via-a-local-file","title":"Userdata via a local file","text":"<p>Instead of inlining long scripts in the config file, you can reference a local file.</p> <pre><code>config:\n  host_groups:\n  - name: agent\n+   userdata_file: ./userdata.sh\n</code></pre> <p>Then simply write the script in <code>userdata.sh</code>, here's one to set up Docker:</p> <pre><code>#!/bin/bash\n# Install Docker\ncurl -fsSL https://get.docker.com | sh\n# Add user to docker group\nusermod -aG docker ubuntu\n</code></pre>"},{"location":"tasks/userdata/#userdata-via-the-cli","title":"Userdata via the CLI","text":"<p>You can add and remove VMs via Slicer's API directly using HTTP, or using the Slicer CLI itself as a client.</p> <p>If you had a hostgroup named <code>agents</code>, you could run:</p> <pre><code>USERDATA=$(echo '# Install Ollama\\ncurl -fsSL https://ollama.com/install.sh | sh' | jq -Rs .)\n\ncurl -sLSf http://127.0.0.1:8080/hostgroup/agents/nodes \\\n    -H \"Content-Type: application/json\" \\\n    -d \"{\n      \\\"userdata\\\": $USERDATA\n    }\"\n</code></pre> <p>For longer userdata scripts, you can save them to a file and reference it:</p> <pre><code># Create userdata.sh\ncat &gt; userdata.sh &lt;&lt; EOF\n#!/bin/bash\n\n# Install Docker\ncurl -fsSL https://get.docker.com | sh\n\n# Add user to docker group\nusermod -aG docker ubuntu\nEOF\n\n# Use the file in your API call\ncurl -sLSf http://127.0.0.1:8080/hostgroup/agents/nodes \\\n    -H \"Content-Type: application/json\" \\\n    -d \"{\n      \\\"userdata\\\": $(cat userdata.sh | jq -Rs .)\n    }\"\n</code></pre> <p>Or you can use the CLI for more convenience:</p> <pre><code>sudo -E slicer vm add \\\n    --userdata ./userdata.sh\n</code></pre>"}]}