{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction to Slicer","text":"<p>SlicerVM gives you real Linux, in milliseconds.</p> <p>Full VMs with systemd and a real kernel, on your Mac, your servers, or your cloud. Slicer is built for teams that need isolation and control without moving code and data to third-party infrastructure.</p>"},{"location":"#where-slicer-fits","title":"Where Slicer fits","text":"<p>Slicer is useful for both one-off/ephemeral workloads and long-running Linux services.</p> <p>All Slicer VMs are called VMs both in the YAML, the API, and codebase, however, we talk about them in two different ways:</p> <ul> <li>Sandboxes \u2014 API-driven, disposable environments for short-lived execution and automation. These VMs are launched into a host group which has a <code>count: 0</code> on start-up.</li> <li>Services \u2014 persistent VMs where you need stable hosts for hosting, long-term testing, etc. These VMs are launched from a host group which has a <code>count: 1</code> or higher.</li> </ul> <p>Both are managed through the same CLI/API/SDK surfaces.</p> <p>In both modes you get a full Linux system where often containers are not enough: package managers, services that need to run as root, bespoke networking configurations, and predictable startup/tear-down times.</p> <p>Example of Slicer used for a Service: A remote (persistent) BuildKit daemon or HA K3s cluster</p> <p>Example of a Slicer used as a Sandbox: Run a task or Process video files with ffmpeg</p>"},{"location":"#our-own-use-cases","title":"Our own use-cases","text":"<p>Slicer was developed internally at OpenFaaS Ltd and has been used across product, platform, and support work.</p> <ul> <li>Booting Kubernetes clusters for Helm chart and platform validation in minutes.</li> <li>Reproducing customer issues quickly in disposable lab environments.</li> <li>Running the code review bot in isolated microVMs for safer, repeatable review flows.</li> <li>Running chaos and failure-mode testing with network and execution control.</li> <li>Building AI/LLM workflows in isolated Linux environments.</li> <li>Reducing R&amp;D cost and latency by replacing slower cloud VMs with local mini PCs, NUCs, and bare-metal hosts.</li> <li>Replacing Docker Desktop/Lima/UTM on macOS with a local-first Linux workflow.</li> <li>Creating quick demo environments for customer trials to reduce friction during B2B evaluations.</li> </ul>"},{"location":"#conceptual-architecture","title":"Conceptual architecture","text":"<p>Slicer for Linux</p> <ul> <li>On Linux, KVM is used with Firecracker by default. Cloud Hypervisor is an option when you need PCI passthrough for devices like NICs or GPUs.</li> <li>You can start Slicer with a fixed host-group count or zero hosts and create VMs on demand through the API.</li> </ul> <p>Slicer for Mac:</p> <ul> <li>On macOS, Slicer for Mac uses Apple\u2019s Virtualization Framework.</li> <li>Mac guests can use VirtioFS folder sharing for local paths.</li> <li> <p>Mac guests support Rosetta for x86_64 Linux binaries.</p> </li> <li> <p>Slicer runs as a daemon and exposes API, CLI, and SDK management for microVM host groups.</p> </li> <li>You define host groups and VM specs in YAML before launching VMs. These cannot be created via API at this point in time. Host groups need to have non-overlapping networking CIDRs defined.</li> <li>The guest agent <code>slicer-agent</code> enables deep integration without having to use networking for: file copies, command execution, secret syncing, shutdowns, and direct shell access to the VM.</li> </ul>"},{"location":"#where-you-can-run-it","title":"Where you can run it","text":"<p>Slicer runs on Linux, including x86_64 and arm64 systems, and on Apple Silicon via Slicer for Mac.</p> <p>If you need local Linux on macOS, check out Slicer for Mac.</p> <p>If you need production-hosted Linux microVMs, Slicer works well on bare-metal and in nested virtualisation on cloud providers.</p> <p>You can also run it on WSL2 for local experimentation, labs, and home office use.</p>"},{"location":"#next-steps","title":"Next steps","text":"<ul> <li>Install Slicer on Linux</li> <li>Install Slicer for Mac</li> <li>Try the walkthrough</li> <li>Run a one-shot task</li> <li>GPU workloads with Slicer</li> <li>Get in touch for commercial questions</li> </ul>"},{"location":"contact/","title":"Contact us","text":"<p>Slicer \u2122 is a trademark of OpenFaaS Ltd.</p>"},{"location":"contact/#for-your-team-or-organisation","title":"For your team or organisation","text":"<p>Would you like to contact us about Slicer for your team or organisation?</p> <p>Send an email to contact@openfaas.com</p>"},{"location":"contact/#discord-support-for-slicer","title":"Discord support for Slicer","text":"<p>After running <code>slicer activate</code>, you'll get a link to join the Slicer Discord server.</p>"},{"location":"examples/autoscaling-k3s/","title":"Autoscaling Kubernetes (k3s) with Cluster Autoscaler","text":"<p>The Cluster Autoscaler is a controller offered by the Kubernetes community to add and remove nodes to a Kubernetes cluster based upon demand.</p> <p>When run in the cloud such as AWS, Azure, GCP, etc, the autoscaler provisions nodes using the cloud provider's VM primitive, so EC2 for AWS, Google Compute Engine for GCP, etc.</p> <p>With Slicer it launches Firecracker microVMs on one or more Slicer hosts. So you can build massive clusters at home or in your own datacentre.</p> <ul> <li>Adds new Kubernetes nodes via Slicer's REST API</li> <li>Removes nodes when they are no longer needed</li> <li>Acts in a similar way to spot instances (ephemeral nodes)</li> </ul>"},{"location":"examples/autoscaling-k3s/#conceptual-overview","title":"Conceptual Overview","text":"<p>Slicer Autoscaler Conceptual Overview running across two separate machines.</p> <p>Control Plane</p> <p>The Control Plane nodes will be setup statically via YAML configuration on one Slicer host. The default is 3, but you could increase that if you wished.</p> <p>The Cluster Autoscaler will be deployed in-cluster using Helm, with a TOML file that defines the node groups and their scaling parameters, and which Slicer API and Token URL to use.</p> <p>Workers / Agents</p> <p>The workers or agents will be provisioned as required by the Cluster Autoscaler, pointing at one or most hosts running Slicer's REST API.</p>"},{"location":"examples/autoscaling-k3s/#demo-watch-a-video-walkthrough","title":"Demo: Watch a video walkthrough","text":"<p>If you'd like to see how it works, you can watch a walkthrough video on YouTube.</p>"},{"location":"examples/autoscaling-k3s/#step-1-setup-the-control-plane-nodes","title":"Step 1: Setup the Control Plane nodes","text":"<p>On the first Slicer host, the one that will run the control plane run the following steps.</p> <p>Create and deploy a 3-node K3s control plane. This will serve as the foundation for your autoscaling cluster. For detailed instructions on Highly Available K3s setup with Slicer, see the HA K3s example.</p> <p>Use <code>slicer new</code> to generate a configuration file for the control plane nodes.</p> <pre><code># Create control plane configuration\nslicer new k3s-cp \\\n  --cpu 2 \\\n  --ram 4 \\\n  --count=3 \\\n  --cidr 192.168.137.0/24 \\\n  &gt; k3s-cp.yaml\n</code></pre> <p>The values above are suggestions that we've tested, but you can customise the specifications to your own needs.</p> <ul> <li><code>--cpu</code> - the number of virtual CPUs to allocate to each control plane VM</li> <li><code>--ram</code> - the amount of RAM in GB to allocate to each control plane VM</li> <li><code>--count</code> - the number of control plane VMs to create</li> <li><code>--cidr</code> - the CIDR block to use for the control plane network (best not to change this until you've run the whole tutorial successfully)</li> </ul> <p>Then start up the control plane nodes:</p> <pre><code># Deploy control plane\nsudo slicer up ./k3s-cp.yaml\nsudo slicer vm list --json &gt; devices.json\n</code></pre>"},{"location":"examples/autoscaling-k3s/#step-11-install-k3s-on-the-control-plane-nodes","title":"Step 1.1: Install K3s on the control plane nodes","text":"<p>Move over to your workstation.</p> <p>Copy the devices.json file to your workstation.</p> <p>Add the route that you were given when starting up Slicer to your machine.</p> <p>Install K3s across all control plane nodes using K3sup Pro. K3sup Pro automates the installation process, setting up the first server and then joining the remaining servers in parallel:</p> <pre><code># Download K3sup Pro (included with Slicer)\ncurl -sSL https://get.k3sup.dev | PRO=true sudo -E sh\nk3sup-pro activate\n\n# Create K3s cluster\nk3sup-pro plan --user ubuntu ./devices.json\nk3sup-pro apply\n\n# Get kubeconfig and join token\nk3sup-pro get-config --local-path ~/k3s-cp-kubeconfig\nk3sup-pro node-token --user ubuntu --host 192.168.137.2 &gt; ~/k3s-join-token.txt\n</code></pre> <p>The kubeconfig is required so that you can use kubectl to manage the cluster. There are additional flags if you want to merge this into your existing kubeconfig file under a new context name such as:</p> <pre><code>k3sup-pro get-config \\\n  --local-path ~/.kube/config \\\n  --merge \\\n  --context slicer-k3s-cp\n</code></pre> <p>The <code>k3sup-pro node-token</code> command fetches the token that K3s will use to add new agents/workers into the cluster, and will be consumed by the Cluster Autoscaler when it is deployed.</p>"},{"location":"examples/autoscaling-k3s/#step-2-setup-worker-node-host-group","title":"Step 2: Setup Worker Node Host Group","text":"<p>Create a separate Slicer instance for autoscaled worker nodes. This could be done on the same machine as the control plane, or a different one.</p> <p>Note that the CIDR block must be different for each Slicer instance.</p> <pre><code># Create worker node host group (starting with 0 nodes)\nslicer new k3s-agents-1 \\\n  --cpu 2 \\\n  --ram 4 \\\n  --cidr 192.168.138.0/24 \\\n  --count=0 \\\n  --tap-prefix=\"k3sa1\" \\\n  --api-bind=\"0.0.0.0\" \\\n  --api-port=8081 \\\n  --find-ssh-keys=false \\\n  --ssh-port=0 \\\n  &gt; k3s-agents-1.yaml\n\n# Deploy worker host group\nsudo slicer up ./k3s-agents-1.yaml\n</code></pre> <p>This host group starts with zero nodes (<code>count=0</code>). The Cluster Autoscaler will call the Slicer API to dynamically add nodes to this group based on scheduling demands.</p> <p>Retrieve the API token from this Slicer instance. This token will be used by the Cluster Autoscaler to authenticate with the Slicer API when provisioning new worker nodes:</p> <pre><code># On the worker node host\nsudo cat /var/lib/slicer/auth/token\n</code></pre> <p>Save the value on your workstation as <code>~/slicer-token-1.txt</code>.</p> <p>You can repeat this process on multiple hosts to create additional worker node groups if needed.</p>"},{"location":"examples/autoscaling-k3s/#step-21-enroll-other-machines","title":"Step 2.1: Enroll other machines","text":"<p>If you have additional machines that could run Slicer and provide Kubernetes nodes, you can add them by following the instructions above.</p> <p>This could be an Arm64 machine (Ampere, Raspberry Pi, etc) or an x86_64 (Intel, AMD, etc) machine.</p> <p>For each, make sure you change:</p> <ul> <li>the CIDR block</li> <li>the tap-prefix</li> <li>the API port</li> <li>the name of the host group</li> </ul> <p>The API port doesn't strictly need to be changed if you run every Slicer instance on a different physical machine, however we've changed it since you may wish to try this out all on one computer.</p> <p>I.e. to add <code>k3s-agents-2</code>:</p> <pre><code>slicer new k3s-agents-2 \\\n  --cpu 2 \\\n  --ram 4 \\\n  --cidr 192.168.139.0/24 \\\n  --count=0 \\\n  --tap-prefix=\"k3sa2\" \\\n  --api-bind=\"0.0.0.0\" \\\n  --api-port=8082 \\\n  --find-ssh-keys=false \\\n  --ssh-port=0 \\\n  &gt; k3s-agents-2.yaml\n</code></pre> <p>Remember to save each Slicer API token to your workstation as <code>~/slicer-token-2.txt</code>, <code>~/slicer-token-3.txt</code>, etc.</p>"},{"location":"examples/autoscaling-k3s/#step-3-configure-networking","title":"Step 3: Configure Networking","text":"<p>In a Kubernetes cluster, every Node and Pod must be able to communicate with every other Node and Pod.</p> <p>That means you must configure a number of routes.</p> <p>Your workstation</p> <p>On your workstation, you can get away with only adding a route to the Control Plane Slicer instance.</p> <p>On each worker host</p> <p>Each worker host must have a route to:</p> <ul> <li>The Control Plane Slicer instance</li> <li>Every other worker node Slicer instance (other than its own)</li> </ul> <p>Making it permanent</p> <p>Routes added with <code>ip route</code> are not permanent, so if your network link goes down, you may have to run them again. If you reboot the machine, you'll need to run them again.</p> <p>When we run long-term K3s clusters with Slicer this way, we'll create a one-shot systemd unit that adds the correct routes on every boot.</p> <p>Alternatively, if running at home, you could add these routes via your ISP's router.</p> <p>For instance, this may be what runs on the control-plane for two separate worker nodes.</p> <p>Create a file called <code>slicer-routes.service</code>: <pre><code>[Unit]\nDescription=Add slicer routes\nAfter=network-online.target\nWants=network-online.target\n\n[Service]\nType=oneshot\nExecStart=/root/add-routes.sh\nRemainAfterExit=true\nUser=root\n\n[Install]\nWantedBy=multi-user.target\n</code></pre></p> <p>Then enable the service to run on every boot:</p> <pre><code>sudo cp ./slicer-routes.service /etc/systemd/system/\nsudo systemctl enable slicer-routes.service\n</code></pre> <p>Then create the <code>add-routes.sh</code> script at <code>/root/add-routes.sh</code>.</p> <p>Copy and paste in the <code>ip route</code> commands that you were given when starting up all your worker nodeSlicer instances.</p>"},{"location":"examples/autoscaling-k3s/#step-4-configure-cluster-autoscaler","title":"Step 4: Configure Cluster Autoscaler","text":"<p>Create the cloud configuration file that tells the Cluster Autoscaler how to connect to your K3s cluster and Slicer APIs.</p> <p>This INI file defines the node groups and their scaling parameters:</p> <pre><code>cat &gt; ./cloud-config.ini &lt;&lt;EOF\n[global]\nk3s-url=https://192.168.137.2:6443\nk3s-token=$(cat ~/k3s-join-token.txt)\ndefault-min-size=0\ndefault-max-size=10\n\n[nodegroup \"k3s-agents-1\"]\nslicer-url=http://192.168.138.1:8081\nslicer-token=$(cat ~/slicer-token-1.txt)\nEOF\n</code></pre> <p>Note that the setting of <code>default-max-size</code> will affect how many nodes can be added to a node group.</p> <p>If you were to have had two Slicer instances running worker nodes, the config would look like this:</p> <pre><code>cat &gt; cloud-config.ini &lt;&lt;EOF\n[global]\nk3s-url=https://192.168.137.2:6443\nk3s-token=$(cat ~/k3s-join-token.txt)\ndefault-min-size=0\ndefault-max-size=10\n\n[nodegroup \"k3s-agents-1\"]\nslicer-url=http://192.168.138.1:8081\nslicer-token=$(cat ~/slicer-token-1.txt)\n\n[nodegroup \"k3s-agents-2\"]\nslicer-url=http://192.168.139.1:8082\nslicer-token=$(cat ~/slicer-token-2.txt)\nEOF\n</code></pre> <p>In the above example, the Slicer host may be a Raspberry Pi 5, and you should limit how many nodes can run there with an extra setting under the <code>[nodegroup]</code> section:</p> <p>I set this to <code>4</code> (4x4=16) since the Raspberry Pi 5 has 4 physical cores and 16GB of RAM.</p> <pre><code>max-size=4\n</code></pre> <ul> <li>The <code>nodegroup</code> name comes from the the hostgroup name of the Slicer instance</li> <li>The <code>slicer-url</code> is the API URL of the Slicer instance</li> <li>The <code>slicer-token</code> is the API token of the Slicer instance</li> <li>The <code>k3s-url</code> is the API URL of the K3s control plane</li> <li>The <code>k3s-token</code> is the join token for the K3s control plane</li> <li>The <code>default-min-size</code> is the default minimum number of nodes to scale to</li> <li>The <code>default-max-size</code> is the default maximum number of nodes to scale to</li> </ul> <p>Overview of all available configuration options:</p> Key Description Required Default <code>global</code> Global configuration options No - <code>global/k3s-url</code> K3s control plane API server URL Yes - <code>global/k3s-token</code> K3s join token for new nodes Yes - <code>global/ca-bundle</code> Path to custom CA bundle file for Slicer API calls No - <code>global/default-min-size</code> Default minimum nodes per group No 1 <code>global/default-max-size</code> Default maximum nodes per group No 8 <code>nodegroup/slicer-url</code> Slicer API server URL Yes - <code>nodegroup/slicer-token</code> Slicer API authentication token Yes - <code>nodegroup/min-size</code> Group-specific minimum size No global default <code>nodegroup/max-size</code> Group-specific maximum size No global default"},{"location":"examples/autoscaling-k3s/#step-5-deploy-cluster-autoscaler","title":"Step 5: Deploy Cluster Autoscaler","text":"<p>Deploy the Cluster Autoscaler using Helm.</p> <p>If you don't have Helm, you can install it via arkade with: <code>arkade get helm</code>.</p> <p>First, create a Kubernetes secret containing the cloud configuration file:</p> <pre><code>kubectl create secret generic \\\n  -n kube-system \\\n  cluster-autoscaler-cloud-config \\\n  --from-file=cloud-config=./cloud-config.ini\n</code></pre> <p>Create a <code>values-slicer.yaml</code> file to configure the Cluster Autoscaler Helm chart. This configuration specifies the Slicer-compatible autoscaler image, mounts the cloud config secret, and sets appropriate scaling parameters:</p> <pre><code>fullnameOverride: slicer-cluster-autoscaler\n\nimage:\n  repository: ghcr.io/openfaasltd/cluster-autoscaler-slicer\n  tag: latest\n\ncloudProvider: slicer\n\nautoDiscovery:\n  clusterName: k3s-slicer\n\nextraVolumeSecrets:\n  cluster-autoscaler-cloud-config:\n    name: cluster-autoscaler-cloud-config\n    mountPath: /etc/slicer/\n    items:\n      - key: cloud-config\n        path: cloud-config\n\nextraArgs:\n  cloud-config: /etc/slicer/cloud-config\n  logtostderr: true\n  stderrthreshold: info\n  v: 4\n  scale-down-enabled: true\n  scale-down-delay-after-add: \"30s\"\n  scale-down-unneeded-time: \"30s\"\n  expendable-pods-priority-cutoff: -10\n  expander: random\n</code></pre> <p>You can find other settings for the Cluster Autoscaler in the official Kubernetes autoscaler Helm chart.</p> <p>Deploy the autoscaler using the official Kubernetes autoscaler Helm chart:</p> <pre><code>helm repo add autoscaler https://kubernetes.github.io/autoscaler\nhelm upgrade \\\n  --install \\\n  slicer-cluster-autoscaler autoscaler/cluster-autoscaler \\\n  --namespace=kube-system \\\n  --values=./values-slicer.yaml\n</code></pre> <p>Next, a minor patch is required to the ClusterRole so that it can remove nodes from the cluster.</p> <pre><code>kubectl patch clusterrole/slicer-cluster-autoscaler \\\n  --type='json' \\\n  -p='[{\"op\": \"add\", \"path\": \"/rules/4/verbs/-\", \"value\": \"delete\"}]'\n</code></pre> <p>You could also edit this manually via <code>kubectl edit clusterrole/slicer-cluster-autoscaler</code>, then add <code>delete</code> to the array of verbs under <code>resources</code> and <code>nodes</code>.</p> <p>If our fork gets merged into upstream, then this patch will no longer be needed.</p>"},{"location":"examples/autoscaling-k3s/#how-to-update-the-configuration","title":"How to update the configuration","text":"<p>If you got something wrong like a token, URL, or Slicer host group name, or perhaps want to add a new Slicer instance, you'll need to update the INI file.</p> <p>If you need to update the configuration, then do the following:</p> <ul> <li>Update your local INI file</li> <li>Delete the secret for the Cluster Autoscaler</li> <li>Re-create the secret</li> <li>Restart the Cluster Autoscaler</li> </ul> <pre><code>kubectl delete secret \\\n  -n kube-system \\\n  cluster-autoscaler-cloud-config\n\nkubectl create secret generic \\\n  -n kube-system \\\n  cluster-autoscaler-cloud-config \\\n  --from-file=cloud-config=./cloud-config.ini\n\nkubectl rollout restart deployment \\\n  -n kube-system \\\n  slicer-cluster-autoscaler\n</code></pre>"},{"location":"examples/autoscaling-k3s/#setting-the-expander-mode","title":"Setting the Expander mode","text":"<p>Check what expander you\u2019re using in the Cluster Autoscaler deployment:</p> <ul> <li> <p><code>--expander=price</code> or <code>--expander=least-waste</code> - will strongly bias towards any group with less RAM/CPU per node</p> </li> <li> <p><code>--expander=random</code> - will randomly pick between suitable groups</p> </li> <li> <p><code>--expander=most-pods</code> - picks the group that fits the most pending pods</p> </li> </ul> <p>If you just want both groups to see some traffic, switching to random or most-pods is usually the simplest.</p> <p>The expander flag can be set in values.yaml:</p> <pre><code>extraArgs:\n  expander: random\n</code></pre>"},{"location":"examples/autoscaling-k3s/#custom-ca-bundle-for-slicer-api","title":"Custom CA Bundle for Slicer API","text":"<p>If your Slicer API endpoints use self-signed certificates or certificates signed by a custom Certificate Authority, you can specify a custom CA bundle.</p> <p>The path the the CA bandle can be configured in the <code>[global]</code> section in the <code>cloud-config.ini</code>:</p> <pre><code>[global]\nca-bundle = /etc/ssl/certs/slicer-ca-bundle.pem\n</code></pre> <p>The CA bundle file should contain one or more PEM-encoded certificates. This configuration applies globally to all Slicer API calls across all node groups.</p> <p>When deploying with Helm, you can mount your CA bundle using <code>extraVolumeSecrets</code>.</p> <p>Create a secret for the CA bundle:</p> <pre><code>kubectl create secret generic \\\n  -n kube-system \\\n  slicer-ca-bundle \\\n  --from-file=ca-bundle=./ca-bundle.pem\n</code></pre> <p>Update your values.yaml file:</p> <pre><code>extraVolumeSecrets:\n  slicer-ca-bundle:\n    name: slicer-ca-bundle\n    mountPath: /etc/ssl/certs/slicer-ca-bundle.pem\n    items:\n      - key: ca-bundle\n        path: ca-bundle\n</code></pre>"},{"location":"examples/autoscaling-k3s/#test-the-cluster-autoscaler","title":"Test the Cluster Autoscaler","text":"<p>The autoscaler works by detecting unschedulable Pods and automatically provisioning new Worker Nodes through Slicer's REST API. Once workloads are removed or reduced, it will scale down unneeded nodes after the configured cooldown period.</p> <p>The capacity of the Control Plane will mean that you can already launch a large number of Pods without needing any scaling.</p> <p>Here's how you can force the Cluster Autoscaler to scale:</p> <ul> <li>Fill up the Control Plane nodes with Pods</li> <li>Use taints, tolerations, nodeselector labels, or other affinity rules to prevent Pods from running on the Control Plane nodes</li> </ul> <p>You can watch the logs of the autoscaler to understand what it's doing:</p> <pre><code># Watch autoscaler logs\nkubectl logs -n kube-system \\\n  deployment/slicer-cluster-autoscaler \\\n  -f\n</code></pre> <p>In another terminal, ideally a split tmux pane set up the below:</p> <pre><code># Pane 1 - watch nodes being added\nkubectl get nodes --watch --output wide\n\n# Pane 2 - watch which Pods are unschedulable/Not Ready - that means they're waiting for a node to be available\nkubectl get pods --watch --output wide --all-namespaces\n</code></pre> <p>Keep another eye out on Slicer's output on the worker node host. You should see VMs being booted up and / or shutting down.</p> <p>Simplest option: scale to hundreds of Pods</p> <p>Create a deployment for busybox's sleep image, and give it fairly big resource requests.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: sleep\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: sleep\n  template:\n    metadata:\n      labels:\n        app: sleep\n    spec:\n      terminationGracePeriodSeconds: 0\n      containers:\n        - name: sleep\n          image: docker.io/library/busybox:latest\n          command: [\"sleep\", \"infinity\"]\n          imagePullPolicy: IfNotPresent\n          resources:\n            requests:\n              cpu: 50m\n              memory: 50Mi\nEOF\n</code></pre> <p>Once the Deployment is created, you can scale it to a moderate or massive number of Pods. Remember that by default, Kubernetes limits each node so it can only run 100 Pods.</p> <p>Try running the below, and give the system a minute or two to catch up between each.</p> <p>The below will full up the Control Plane nodes, and worker nodes will be needed to support the excess.</p> <pre><code>kubectl scale deployment sleep --replicas=100\nkubectl scale deployment sleep --replicas=300\nkubectl scale deployment sleep --replicas=400\n</code></pre> <p>You'll see the a new Node being added to the cluster, and the Pods being scheduled to run on it.</p> <pre><code># Watch each Pod coming online, and see which node it gets scheduled to\nkubectl get pods --watch --output wide\n\n# Watch nodes coming online to support the new Pods\nkubectl get nodes --watch --output wide\n</code></pre> <p></p> <p>Example showing 600 Pods running across two Slicer hosts running worker nodes - a Ryzen 9 and a Raspberry Pi 5. The Control Plane is running on a separate N100 fanless mini PC.</p> <p>These Pods run a simple <code>sleep infinity</code> command, so they are not doing anything that taxes the system, however you can view Node usage across the cluster with:</p> <pre><code>watch \"kubectl top nodes\"\n</code></pre> <p>Taints and tolerations</p> <p>You can use a label to prevent new Pods from running on the Control Plane, and simulate autoscaling.</p> <p>In production, OpenFaaS customers tend to run their functions on spot instances rather than on their control plane nodes.</p> <pre><code>kubectl taint nodes slicer-cp-1:NoSchedule\nkubectl taint nodes slicer-cp-2:NoSchedule\nkubectl taint nodes slicer-cp-3:NoSchedule\n</code></pre> <p>Now, newly created or restarted Pods will not be scheduled to run on the Control Plane nodes.</p> <p>Next, you can create a Pod that tolerates the taint, and will be scheduled to run on the Control Plane nodes.</p> <pre><code>kubectl apply -f - &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: sleep-pod\nspec:\n  containers:\n    - name: sleep\n      image: docker.io/library/busybox:latest\n      command: [\"sleep\", \"infinity\"]\n  tolerations:\n    - key: slicer-cp\n      operator: Equal\n      value: \"true\"\n      effect: NoSchedule\nEOF\n</code></pre> <p>You'll see the a new Node being added to the cluster, and the Pod being scheduled to run on it.</p> <p>Finally, remove the Pod and untaint the Control Plane nodes:</p> <pre><code>kubectl delete pod sleep-pod\nkubectl taint nodes slicer-cp-1:NoSchedule-\nkubectl taint nodes slicer-cp-2:NoSchedule-\nkubectl taint nodes slicer-cp-3:NoSchedule-\n</code></pre>"},{"location":"examples/autoscaling-k3s/#troubleshooting","title":"Troubleshooting","text":"<p>Node says as NotReady</p> <p>Check connectivity and routes. Remember these do not survive an interface going down/up or a reboot.</p> <p>Remove the mode manually via <code>kubectl delete node/NAME</code>.</p> <p>Check the Slicer output for the node, perhaps it failed to boot or the network didn't come up in time.</p> <p>Left over nodes</p> <p>Delete all nodes that are not part of the control plane, assuming they are all listed as NotReady:</p> <pre><code>kubectl delete node -l k3sup.dev/node-type=agent\n</code></pre> <p>Only one Slicer instance is getting used</p> <p>The <code>expander</code> setting in values.yaml should be changed to <code>random</code> or <code>most-pods</code> to ensure that all Slicer instances are used.</p> <p>The default is <code>price</code> and meant for the cloud, where the smallest node types are always picked first.</p> <p>The Cluster Autoscaler is not picking up new configuration</p> <p>If you have tainted any nodes, it may mean that the new Pod for the autoscaler cannot be scheduled, so remove the taints.</p> <p>The Cluster Autoscaler says a node group is not ready</p> <p>This error was observed during testing, restarting the deployment resolved the issue. We assume the autoscaler has a bug where it doesn't properly check readiness once it decides it's unhealthy.</p> <p>Message observed: \"Readiness for node group k3s-agents-2 not found\"</p> <p>Why are many Pods stuck as Pending?</p> <p>The logs of the autoscaler are set to a verbose level which will explain most issues.</p> <p>Look out to see if you've exceeded the maximum number of nodes for a node group i.e. \"Skipping node group k3s-agents-1 - max size reached\".</p> <p>Diagnose an issue within a worker node</p> <p>The <code>slicer vm shell</code> command will give you a root shell directly into any worker node.</p> <p>On the host managing <code>k3s-agents-1</code>, you would run:</p> <pre><code># Show the available nodes\nsudo slicer vm list --url http://127.0.0.1:8081/nodes\n\n# Get a root shell into node 1\nsudo slicer vm exec --url http://127.0.0.1:8081/nodes k3s-agents-1-1\n</code></pre> <p>From there you can check the logs of the <code>k3s</code> service via <code>sudo journalctl -u k3s -f</code>.</p>"},{"location":"examples/autoscaling-k3s/#next-steps","title":"Next steps","text":"<p>Now you have a Kubernetes cluster that can scale its capacity up and down with demand.</p> <p>Why not try out something that scales Pods?</p> <p>OpenFaaS which scales Pods</p> <p>OpenFaaS Community Edition (CE) is a version of OpenFaaS that can be used for free for personal use, or a limited commercial trial.</p> <p>Once you've deployed a function you can use something like the hey tool via (<code>arkade get hey</code>) to stress a function, and cause it to scale up.</p> <p>Learn more about OpenFaaS Autoscaling.</p>"},{"location":"examples/buildkit/","title":"Remote Docker builds","text":"<p>BuildKit is a modern backend that powers Docker's build engine. This example shows how to set up a dedicated BuildKit instance in a Slicer VM for isolated, remote container builds.</p> <p>Note</p> <p>Whilst Slicer could be used to design a multi-tenant container builder this example is not intended for that use case.</p> <p>The setup in this example is intended for use by a developer for building images with a faster remote machine, or a different OS/architecture to their machine.</p> <p>Using a remote BuildKit instance in a Slicer VM provides several benefits:</p> <ul> <li>Isolation: Build processes are completely isolated from your host system</li> <li>Resource control: Dedicated CPU and memory resources for builds</li> <li>Security: Builds run in a sandboxed environment</li> <li>Flexibility: Easy to scale up resources or create multiple build environments</li> <li>Clean state: Each VM can be easily reset to a clean state</li> </ul> <p>This example is very minimal and covers the basic setup. You can expand it according to your needs. In the next steps we are going to:</p> <ul> <li>Install and configure BuildKit automatically on first boot with a userdate script.</li> <li>Set up Docker buildx to use the remote BuildKit instance with two connection options:<ol> <li>Forwarding the BuildKit Unix socket to the host using <code>slicer vm forward</code></li> <li>Connect to BuildKit over SSH.</li> </ol> </li> </ul>"},{"location":"examples/buildkit/#buildkit-installation-script","title":"BuildKit installation script","text":"<p>Create the <code>buildkit.sh</code> userdata script that will automatically install and configure BuildKit:</p> <pre><code>#!/usr/bin/env bash\n# BuildKit installation and configuration script\n# This script installs buildkitd, configures the buildkit group, and creates a systemd service\n\n#!/usr/bin/env bash\nset -euxo pipefail\n\n# Install buildkit\narkade system install buildkitd\n\n# Add a buildkit group\nsudo groupadd buildkit\n\n# Add ubuntu user to buildkit group\nsudo usermod -aG buildkit ubuntu\n\n# Systemd service for buidlkit (daemonized under systemd)\ncat &lt;&lt;'EOF' | sudo tee /etc/systemd/system/buildkitd.service &gt; /dev/null\n[Unit]\nDescription=BuildKit Daemon\nAfter=network.target\n\n[Service]\nType=simple\nExecStart=/usr/local/bin/buildkitd --addr unix:///run/buildkit/buildkitd.sock --group buildkit\nRestart=always\nUser=root\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsudo systemctl daemon-reload\nsudo systemctl enable --now buildkitd\n</code></pre>"},{"location":"examples/buildkit/#vm-configuration","title":"VM configuration","text":"<p>Use <code>slicer new</code> to generate a configuration file:</p> <pre><code>slicer new buildkit \\\n  --userdata-file buildkit.sh \\\n  &gt; buildkit.yaml\n</code></pre> <p>If you plan to connect to buildkit over SSH, use the <code>--ssh-key</code> or <code>--github</code> flags to add SSH keys. The Docker buildx remote driver supports connection to a remote BuildKit instance over SSH. On ssh connection is not required when using socket forwarding with <code>slicer vm forward</code>.</p> <p>For better build performance, consider increasing the VM resources:</p> <pre><code>    vcpu: 4\n    ram_gb: 8\n    storage_size: 25G\n</code></pre>"},{"location":"examples/buildkit/#start-the-vm","title":"Start the VM","text":"<p>Start the VM with the following command:</p> <pre><code>sudo slicer up ./buildkit.yaml\n</code></pre>"},{"location":"examples/buildkit/#configure-docker-buildx","title":"Configure Docker buildx","text":"<p>Once your VM is running and BuildKit is installed, you can configure Docker buildx to use it as a remote builder. You have two options:</p> <ul> <li>Use SSH to connect buildx directly to the VM.</li> <li>Forward the BuildKit Unix socket to your host and connect locally.</li> </ul> <p>For more information about Docker builders, see the Docker builders documentation.</p>"},{"location":"examples/buildkit/#option-1-forward-the-buildkit-unix-socket","title":"Option 1: Forward the BuildKit Unix socket","text":"<p>Forward the BuildKit Unix socket from the VM to a local Unix socket:</p> <pre><code>slicer vm forward buildkit-1 \\\n  -L ./buildkitd.sock:/run/buildkit/buildkitd.sock\n</code></pre> <p>Point buildx to the forwarded socket with a <code>unix://</code> URL (the <code>driver-opt</code> value is passed directly to buildx):</p> <pre><code>docker buildx create \\\n  --name slicer \\\n  --driver remote \\\n  unix://$(pwd)/buildkitd.sock\n</code></pre>"},{"location":"examples/buildkit/#option-2-connect-over-ssh","title":"Option 2: Connect over SSH","text":""},{"location":"examples/buildkit/#add-vm-to-known-hosts","title":"Add VM to known hosts","text":"<p>First, add the Slicer VM to your SSH known hosts to avoid connection issues:</p> <pre><code># Replace with your actual VM IP if different\nssh-keyscan 192.168.137.2 &gt;&gt; ~/.ssh/known_hosts\n</code></pre>"},{"location":"examples/buildkit/#create-the-remote-builder","title":"Create the remote builder","text":"<p>Create a new buildx builder instance that connects to your Slicer VM:</p> <pre><code># Create a new builder named 'slicer' using the remote driver\ndocker buildx create \\\n  --name slicer \\\n  --driver remote \\\n  ssh://ubuntu@192.168.137.2\n</code></pre>"},{"location":"examples/buildkit/#verify-the-builder","title":"Verify the builder","text":"<p>After configuring either option above, check that your new builder is available and working:</p> <pre><code># List available builders\ndocker buildx ls\n\n# Inspect the slicer builder for detailed information\ndocker buildx inspect slicer\n</code></pre> <p>When buildx can successfully connect to the builder, the status should show as <code>running</code>:</p> <pre><code>slicer          remote\n \\_ slicer0      \\_ ssh://ubuntu@192.168.137.2    running   v0.24.0    linux/amd64 (+3), linux/386\n</code></pre> <p>The <code>inspect</code> command will show additional details about supported platforms, driver configuration, and connection status.</p>"},{"location":"examples/buildkit/#use-the-remote-builder","title":"Use the remote builder","text":"<p>Once your builder is configured and running, all builds executed with <code>--builder slicer</code> will run on the remote BuildKit instance instead of your local machine.</p> <p>Create a simple Dockerfile:</p> <pre><code>FROM alpine:3.19\n\nCMD [\"echo\", \"Hello, BuildKit!\"]\n</code></pre> <p>Build a container using your remote BuildKit instance:</p> <pre><code># Build and tag an image using the remote builder\ndocker buildx build \\\n  --builder slicer \\\n  -t hello-buildkit \\\n  .\n</code></pre> <p>The remote builder supports all BuildKit features including multi-platform builds, build secrets, cache mounts, and advanced Dockerfile syntax. Build outputs, logs, and any artifacts are handled seamlessly as if building locally, but with the security and resource isolation benefits of the dedicated VM.</p> <p>You can also set the remote builder as your default to avoid specifying <code>--builder</code> on every command:</p> <pre><code>docker buildx use slicer\n</code></pre>"},{"location":"examples/buildkit/#troubleshooting","title":"Troubleshooting","text":"<p>If you're having trouble connecting to the remote builder:</p> <ol> <li> <p>Check VM status: Ensure the VM is running and BuildKit service is active    <pre><code># Open a shell in the VM and check service status\nslicer vm shell buildkit-1 --uid 1000\nsudo systemctl status buildkitd\n</code></pre></p> </li> <li> <p>Check BuildKit socket: Verify the socket is accessible    <pre><code>slicer vm exec buildkit-1 -- \"sudo -u ubuntu -g buildkit buildctl --addr unix:///run/buildkit/buildkitd.sock debug info\"\n</code></pre></p> </li> <li> <p>Verify SSH connectivity (Only required remote buildkit over SSH ): Test SSH connection directly    <pre><code>ssh ubuntu@192.168.137.2 \"echo 'SSH connection successful'\"\n</code></pre></p> </li> </ol>"},{"location":"examples/buildkit/#further-thoughts","title":"Further thoughts","text":"<p>This was a basic example to demonstrate how to run BuildKit isolated in a Slicer VM. Some ideas to explore further:</p> <ul> <li>TCP connection: Run BuildKit over TCP with mTLS in the VM to avoid SSH key management</li> <li>Ephemeral VMs: Use the Slicer REST API to provision temporary VMs for each build, then destroy them.</li> <li>BuildKit pools: Create pools of BuildKit instances for native multi-platform builds (avoiding QEMU emulation) and shared persistent caching.</li> </ul> <p>The isolated nature of microVMs makes this approach particularly well-suited for enterprise build infrastructure where security, resource control, and clean environments are priorities.</p>"},{"location":"examples/cursor-cli-agent/","title":"Run a Headless Coding Agent with Cursor CLI","text":"<p>Cursor's CLI can be used in a non-interactive way within a script or CI job. That makes it ideal for use as a one-shot task with a SlicerVM, or a longer running VM that you connect to with a remote VSCode IDE.</p> <p>Using this flow, you could create a powerful agent that uses your existing Cursor subscription, running with the \"auto\" model so it remains within the free tier.</p> <p>Example use-cases:</p> <ul> <li>Web scraping, data extraction and analysis</li> <li>Code or content generation</li> <li>Analyze user-submitted content such as code, comments, or documents</li> <li>Image or video conversion</li> <li>Chat bots, and webhook receivers / responders - i.e. from GitHub, GitLab, Discord, Slack, etc.</li> </ul> <p>Running within a microVM means you can run the agent in a secure, isolated environment, and you can easily discard the VM when done.</p> <p>Note: Overall, Cursor's CLI has some rough edges, and you may prefer to use opencode, which at time of writing worked better headless and was more polished.</p> <p>The CLI may require an initial interactive session to enable MCP usage and to \"trust\" the working directory.</p> <p>The CLI can also hang indefinitely after responding to a prompt, even in --print mode.</p> <p>The CLI seems to have no practical way to read a prompt from a file or a stdin pipe, so you have to use <code>$(cat prompt.txt)</code> which can end up reading subsequent commands into the prompt.</p>"},{"location":"examples/cursor-cli-agent/#quick-overview","title":"Quick overview","text":"<ul> <li>Create a Cursor API key and save it for use with the CLI.</li> <li>Create a userdata script to pre-install the CLI, Playwright for accessing the web, and your Cursor API key.</li> <li>Boot up the VM, optionally, collect the results from the logs or from the disk via SCP or by mounting the disk image after shutdown.</li> </ul>"},{"location":"examples/cursor-cli-agent/#userdata-script","title":"Userdata script","text":"<p>Populate the prompt section with your own inputs.</p> <p>Save as cursor.sh:</p> <pre><code>#!/usr/bin/env bash\nset -euo pipefail\n\nTARGET_USER=ubuntu\n\n# If we're root, re-exec this script as $TARGET_USER (login shell so HOME/env are correct)\nif [[ $EUID -eq 0 ]]; then\n  if ! id -u \"$TARGET_USER\" &gt;/dev/null 2&gt;&amp;1; then\n    echo \"User '$TARGET_USER' does not exist\" &gt;&amp;2\n    exit 1\n  fi\n\n  # Option A: run the file path directly (keeps $0 correct; requires the user can read the file)\n  # exec sudo -u \"$TARGET_USER\" -i bash \"$0\" \"$@\"\n\n  # Option B: stream the script via stdin (works even if the file isn't readable by $TARGET_USER)\n  exec sudo -u \"$TARGET_USER\" -i bash -s -- \"$@\" &lt; \"$0\"\nfi\n\n# From here on, we are the ubuntu user\necho \"Running as $(whoami), HOME=$HOME\"\n# cd to HOME\ncd\n\ncurl https://cursor.com/install -fsS | sudo -E bash\n\nmkdir -p ~/.cursor/\n\necho 'export PATH=\"$HOME/.local/bin:$PATH\"' &gt;&gt; ./.bashrc\n\n# Add cursor CURSOR_API_KEY env variable\necho \"export CURSOR_API_KEY=$(cat /run/slicer/secrets/cursor-api-key)\" | tee -a ~/.bashrc\n\n# Install playwright mcp tool for browsing web / fetching content\n# These steps could be built into a custom Slicer image to speed up boot time.\nsudo -E arkade system install node\nnpm i -D @playwright/test\nnpm i -D @playwright/mcp@latest\n\nnpx playwright install --with-deps chromium\n#npx playwright install --with-deps webkit\n\ncat &lt;&lt; EOF &gt; ~/.cursor/mcp.json\n{\n  \"mcpServers\": {\n    \"playwright\": {\n      \"command\": \"npx\",\n      \"args\": [\n        \"@playwright/mcp@latest\",\n        \"--browser\", \"chromium\",\n        \"--headless\"\n        ]\n    }\n  }\n}\n\nEOF\n\ncat &lt;&lt; EOF &gt; ~/.cursor/cli-config.json\n{\n  \"permissions\": {\n    \"allow\": [\n      \"*\"\n    ],\n    \"deny\": []\n  }\n}\n\nEOF\n\nmkdir -p ~/.cursor/projects/home-ubuntu/\n\ncat &lt;&lt; EOF &gt; ~/.cursor/projects/home-ubuntu/mcp-approvals.json\n[\n  \"playwright-aced25c5e5b87b1c\"\n]\n\nEOF\n\nmkdir -p ~/.cache/ms-playwright/\n\ncat &lt;&lt; EOF &gt; ~/prompt.txt\nVisit the slicervm.com website and generate a JSON summary of the key elements, value-proposition,\nand target audience. Use playwright-mcp, this has been pre-installed and is ready for your use.\nEOF\n\nsource ~/.bashrc\n\n# Belt and braces..\nsudo chown -R \"$USER:$USER\" \"$HOME\"\n\ncursor-agent mcp list\ncursor-agent mcp list-tools playwright\n\ntime cursor-agent --model auto --force --output-format=text --print \"$(cat ./prompt.txt)\" &gt; ~/cursor.log\n</code></pre> <p>Note: at time of writing, a <code>mcp-approvals.json</code> file had to be created to get the MCP tool calls to work. Hopefully the cursor team will negate this requirement for headless use in a future release.</p> <p>The approval file is not required if you do not use MCP tools.</p>"},{"location":"examples/cursor-cli-agent/#config-file","title":"Config file","text":"<p>Create a slicer VM secret for your Cursor API key (Obtained from: https://cursor.com/dashboard?tab=background-agents)</p> <p>Run the following, then paste in your API key, hit enter once, then Control + D to save the file.</p> <pre><code>sudo mkdir .secrets\n# Ensure only root can read/write to the secrets folder.\nsudo chmod 700 .secrets\n\nsudo cat &gt; .secrets/cursor-api-key\n</code></pre> <p>Use <code>slicer new</code> to generate a configuration file:</p> <p><pre><code>slicer new cursor \\\n  --userdata-file cursor.sh \\\n  &gt; cursor.yaml\n</code></pre> Save the resulting file as <code>cursor.yaml</code>.</p>"},{"location":"examples/cursor-cli-agent/#give-it-a-test-run","title":"Give it a test run","text":"<p>Now run:</p> <pre><code>sudo slicer up ./cursor.yaml\n</code></pre>"},{"location":"examples/cursor-cli-agent/#taking-it-further","title":"Taking it further","text":"<p>You can learn more about the Cursor CLI here and its headless usage here</p> <p>If your agent doesn't need to browse the web, then you can speed up the boot by removing the Playwright instructions.</p> <p>If you have additional MCP servers, you can add them into the userdata and ~/.cursor/mcp.json file.</p> <p>To make the system boot up even quicker, you can derive your own custom image with the CLI and playwright pre-installed. Then the userdata will only be used to set up the API key, and to provide the prompt.</p> <p>Finally, if you are automating access, you could automate the workflow by creating a microVM via the REST API, and using <code>scp</code> to collect the generated code or results from the agent.</p>"},{"location":"examples/docker/","title":"Run a Docker container in Slicer","text":"<p>There are three ways to try out a container or Docker within Slicer:</p> <ol> <li>Start your VM as per the walkthrough, then connect via <code>ssh</code> and install Docker inside the VM.</li> <li>Use Userdata to install Docker and pull your image on first boot.</li> <li>Create a custom base image with Docker pre-installed, and your image pre-pulled, with a systemd service to start it on every boot.</li> </ol> <p>On this page we'll cover options 2 and 3.</p> <p>Option 2 is the easiest to use, and most portable. The boot up time will be delayed whilst your userdata runs. In option 3, that time is spent on the host once, rather than on the first boot of each VM.</p> <p>We'll show you a generic approach here, but you can adapt it to your needs, or use another tool like nerdctl or Podman instead of Docker.</p> <p>The container we're going to use is a Docker Registry, but likewise you could run your own applications, a database, Ollama, Grafana, or any other container.</p>"},{"location":"examples/docker/#install-docker-and-a-container-via-userdata","title":"Install Docker and a container via Userdata","text":"<p>You can use Userdata to install Docker and pull your image on first boot.</p> <p>The below is only a partial snippet to show you the relevant changes:</p> <pre><code>config:\n    host_groups:\n    - name: vm\n      userdata: |\n        #!/bin/bash\n        # Install Docker\n        curl -fsSL https://get.docker.com | sh\n\n        # Add user to docker group\n        usermod -aG docker ubuntu\n\n        docker run -d -p 5000:5000 --restart=always --name registry registry:3\n</code></pre>"},{"location":"examples/docker/#create-a-custom-base-image-for-a-docker-image","title":"Create a custom base image for a Docker image","text":"<p>Create a one-shot systemd service to run your container on boot-up.</p> <p>Call this file <code>docker-registry.service</code>:</p> <pre><code>[Unit]\nDescription=Run a Docker registry\nAfter=docker.service\nRequires=docker.service\nWants=network-online.target\nAfter=network-online.target\nStartLimitIntervalSec=0\nStartLimitBurst=3\n\n[Service]\nType=oneshot\nRemainAfterExit=yes\nExecStart=/usr/bin/docker run -d -p 5000:5000 --restart=always --name registry registry:3\nTimeoutStartSec=0\nRestart=on-failure\nRestartSec=10s\nStandardOutput=journal\nStandardError=journal\nSyslogIdentifier=docker-registry\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <pre><code>FROM ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-latest\n\nRUN curl -fsSL https://get.docker.com | sh &amp;&amp; \\\n    usermod -aG docker ubuntu\n\nCOPY docker-registry.service /etc/systemd/system/docker-registry.service\n\nRUN systemctl enable docker-registry\n</code></pre> <p>Then build the image, and push it to a registry.</p> <p>Next, customise your <code>config.yaml</code> to use your new image:</p> <pre><code>config:\n   image: \"docker.io/alexellis2/slicer-docker-registry:latest\"\n</code></pre> <p>In order to pre-pull the container image during the Docker build, you'd have to use crane to export the image to a local tarball, and then to import that during userdata. This is because you can't run <code>docker pull</code> during a Docker build.</p>"},{"location":"examples/docker/#try-out-your-container","title":"Try out your container","text":"<p>If you deployed a Docker registry, you can push and pull images to it from your host.</p> <pre><code>docker pull alpine\ndocker tag alpine 192.168.137.1:5000/alpine\ndocker push 192.168.137.1:5000/alpine\n</code></pre> <p>You can use this custom registry for your Slicer images too, by adding <code>insecure_registry: true</code> to the <code>config</code> section of your VM's YAML file.</p>"},{"location":"examples/gpu-ollama/","title":"Run a microVM with a GPU mounted for Ollama","text":"<p>Ollama can run in any microVM using its CPU, however a GPU is the best option for a high token throughput and faster response times.</p> <p>You'll need a PC with VFIO support, this allows a PCI device such as a GPU to be passed through to the microVM for exclusive access.</p> <p>What if you have multiple GPUs? Let's imagine you have a ATX tower PC with 2x Nvidia RTX 3090 or 3060 GPUs.</p> <ul> <li>Allocate both GPUs to one machine</li> <li>Allocate each GPU to its own microVM</li> <li>Start up with zero microVMs and launch up to two short-lived tasks at once</li> </ul> <p></p> <p>Ollama running the qwen3 model to generate a story about a microVM's first day at school.</p> <p>GPU / VFIO mounting only works with Slicer running on <code>x86_64</code> at present.</p>"},{"location":"examples/gpu-ollama/#set-up-your-vm-configuration","title":"Set up your VM configuration","text":"<p>There a three differences to the other examples we've seen so far:</p> <ol> <li><code>gpu_count: N</code> is added to the hostgroup, N is a the number of GPUs to allocate to each VM</li> <li><code>hypervisor: cloud-hypervisor</code> - Cloud Hypervisor is used instead of Firecracker, to enable VFIO passthrough of a PCI device</li> <li><code>image</code> - a separate Kernel and root filesystem is required for Cloud Hypervisor</li> </ol> <p>The following to <code>gpu.yaml</code>, make sure you update <code>vcpu</code> and <code>ram_gb</code></p> <pre><code>config:\n  host_groups:\n  - name: gpu\n    storage: image\n    storage_size: 80G\n    count: 1\n    vcpu: 16\n    ram_gb: 64\n    gpu_count: 1\n    network:\n      bridge: brgpu0\n      tap_prefix: gputap\n      gateway: 192.168.137.1/24\n\n  github_user: alexellis\n\n  image: \"ghcr.io/openfaasltd/slicer-systemd-ch:5.10.240-x86_64-latest\"\n\n  hypervisor: cloud-hypervisor\n</code></pre> <p>Boot the VM(s) with:</p> <pre><code>sudo slicer up ./ollama-gpu.yaml\n</code></pre> <p>Note: adding PCI devices will add a boot delay of a few seconds vs. microVMs without any PCI devices. To monitor this, run <code>ping</code> in the background or <code>sudo fstail /var/log/slicer/</code> to see when the dmesg messages start appearing. </p> <p>Then, as usual, add the route on your workstation so you can connect via SSH.</p> <pre><code>ssh ubuntu@192.168.137.2\n</code></pre> <p>View the PCI devices:</p> <pre><code>$ lspci\n00:00.0 Host bridge: Intel Corporation Device 0d57\n00:01.0 Unassigned class [ffff]: Red Hat, Inc. Virtio console (rev 01)\n00:02.0 Mass storage controller: Red Hat, Inc. Virtio block device (rev 01)\n00:03.0 Mass storage controller: Red Hat, Inc. Virtio block device (rev 01)\n00:04.0 Ethernet controller: Red Hat, Inc. Virtio network device (rev 01)\n00:05.0 Unassigned class [ffff]: Red Hat, Inc. Virtio RNG (rev 01)\n00:06.0 VGA compatible controller: NVIDIA Corporation GA102 [GeForce RTX 3090] (rev a1)\n00:07.0 Audio device: NVIDIA Corporation GA102 High Definition Audio Controller (rev a1)\n00:08.0 VGA compatible controller: NVIDIA Corporation GA102 [GeForce RTX 3090] (rev a1)\n00:09.0 Audio device: NVIDIA Corporation GA102 High Definition Audio Controller (rev a1)\n</code></pre> <p>You can see that I mounted 2x Nvidia RTX 3090 GPUs into this microVM.</p> <p>You can install the Nvidia drivers using our utility script:</p> <pre><code>curl -SLsO https://raw.githubusercontent.com/self-actuated/nvidia-run/refs/heads/master/setup-nvidia-run.sh\nchmod +x ./setup-nvidia-run.sh\necho \"Downloading and installing Nvidia drivers... via nvidia.run\"\nsudo bash ./setup-nvidia-run.sh\n</code></pre> <p>Compilation can take a minute or two, but can be sped up by caching all changed files and untaring them over the top of the root filesystem in userdata, or by building a custom VM image with the generated tar expanded.</p> <p>If you run into an error, you can edit the script and uncomment the line <code>--no-unified-memory</code>.</p> <p>Other commands to start a custom agent, or to install frameworks/tools can be added easily via userdata directly within the YAML.</p> <p>Check the status of the driver with <code>nvidia-smi</code>:</p> <pre><code>ubuntu@gpu-1:~$ nvidia-smi\n\nMon Sep  1 11:28:45 2025\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 580.76.05              Driver Version: 580.76.05      CUDA Version: 13.0     |\n+-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 3090        Off |   00000000:00:06.0 Off |                  N/A |\n| 30%   40C    P0            108W /  350W |       0MiB /  24576MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n|   1  NVIDIA GeForce RTX 3090        Off |   00000000:00:08.0 Off |                  N/A |\n| 30%   32C    P0             99W /  350W |       0MiB /  24576MiB |      2%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n</code></pre> <p>Then install Ollama:</p> <pre><code>sudo apt update -qy &amp;&amp; \\\n  sudo apt install -qy zstd\n\ncurl -fsSL https://ollama.com/install.sh | sh\n</code></pre> <p>If you want to access the API from another machine, you can edit the service definition.</p> <p>Edit <code>/etc/systemd/system/ollama.service</code> and add the following line under the <code>[Service]</code> section:</p> <pre><code>Environment=\"OLLAMA_HOST=0.0.0.0:11434\"\n</code></pre> <p>Then restart the service:</p> <pre><code>sudo systemctl daemon-reload &amp;&amp; \\\n  sudo systemctl restart ollama\n</code></pre> <p>Next, pull a model and try out a prompt:</p> <pre><code>ollama run qwen3:latest\n</code></pre> <p>You can also connect to the Ollama API from your host machine by using the VM's IP directly:</p> <pre><code>curl -SLs http://192.168.137.2:11434/api/generate -d '{\n  \"model\": \"qwen3:latest\",\n  \"prompt\":\"Why is the sky blue?\"\n}'\n</code></pre> <p>Since we're using a persistent disk image, any models you download will be available if you restart or shutdown Slicer.</p>"},{"location":"examples/ha-k3s/","title":"Highly Available (HA) Kubernetes with K3s","text":"<p>The following example sets up a 3x Node Kubernetes cluster using K3s. As an optional step, you can set up a Load Balancer running on the Slicer host to distribute traffic across the nodes for the API server and HTTP/HTTPS.</p> <p>If you would like to try out a smaller cluster first, you can simply change the <code>count</code> from <code>3</code> to <code>1</code> when saving the file below.</p> <p>Create <code>k3s-ha.yaml</code>:</p> <pre><code>slicer new k3s \\\n  --cpu 2 \\\n  --ram 4 \\\n  --count=3 \\\n  --cidr 192.168.137.0/24 \\\n  &gt; k3s-ha.yaml\n</code></pre> <p>The IP addresses of the VMs will be as follows:</p> <ul> <li><code>192.168.137.2</code></li> <li><code>192.168.137.3</code></li> <li><code>192.168.137.4</code></li> </ul>"},{"location":"examples/ha-k3s/#setup-kubernetes-with-k3sup-pro","title":"Setup Kubernetes with K3sup Pro","text":"<p>Download K3sup Pro:</p> <pre><code>curl -sSL https://get.k3sup.dev | PRO=true sudo -E sh\n</code></pre> <p>If you want to leave off <code>sudo</code>, then just move the <code>k3sup-pro</code> binary into your <code>$PATH</code> variable manually.</p> <p>Next, on the host where slicer is running, get the devices file from Slicer's API:</p> <pre><code>curl -sLS http://127.0.0.1:8080/nodes &gt; devices.json\n</code></pre> <p>Copy devices.json back to your workstation.</p> <p>On your workstation, add any routes that are specified so you can access the VMs on their own network.</p> <p>Check the options like disabling Traefik, so that you can install Ingress Nginx or Istio instead.</p> <pre><code>k3sup-pro plan --help\n\nk3sup-pro plan --traefik=false --user ubuntu\n</code></pre> <p>This will generate a plan.yaml file, you can review and edit it.</p> <p>Next, run <code>k3sup-pro apply</code>.</p> <p>This will install the first server, then server 2 and 3 in parallel.</p> <p>Finally run:</p> <pre><code>mkdir -p ~/.kube\ncp ~/.kube/config ~/.kube/config.bak || true\n\nk3sup-pro get-config \\\n --local-path ~/.kube/config \\\n --merge \\\n --context slicer-k3s-ha\n</code></pre> <p>Then you can run <code>kubectx slicer-k3s-ha</code>, and start using kubectl.</p> <p>Your cluster is running in HA mode.</p>"},{"location":"examples/ha-k3s/#create-a-ha-loadbalancer-for-the-vms","title":"Create a HA LoadBalancer for the VMs","text":"<p>If you would like to create a load balancer for the microVMs, you can do so using the mixctl add-on.</p> <pre><code>arkade get mixctl\n</code></pre> <p>Create a config named <code>k3s.yaml</code>:</p> <pre><code>version: 0.1\n\nrules:\n- name: k3s-api\n  from: 127.0.0.1:6443\n  to:\n    - 192.168.137.2:6443\n    - 192.168.137.3:6443\n    - 192.168.137.4:6443\n\n- name: k3s-http\n  from: 127.0.0.1:80\n  to:\n    - 192.168.137.2:80\n    - 192.168.137.3:80\n    - 192.168.137.4:80\n\n- name: k3s-tls\n  from: 127.0.0.1:443\n  to:\n    - 192.168.137.2:443\n    - 192.168.137.3:443\n    - 192.168.137.4:443\n</code></pre> <p>Then run <code>mixctl ./k3s.yaml</code></p> <p>Finally, revisit your plan so each server obtains a TLS certificate for the Kubernetes API server for the IP address of the Slicer host.</p> <p>So if the Slicer host were 192.168.1.100:</p> <pre><code>k3sup-pro plan --tls-san 192.168.1.100 \\\n  --update\n\nk3sup-pro apply\n</code></pre> <p>Then edit your <code>~/.kube/config</code> file and replace <code>192.168.137.2:6443</code> with <code>192.168.1.100:6443</code>.</p> <p>Now every time you run <code>kubectl</code>, you'll see mixctl balance traffic across all three servers.</p>"},{"location":"examples/jenkins/","title":"Run Jenkins for CI/CD","text":""},{"location":"examples/jenkins/#jenkins","title":"Jenkins","text":"<p>Jenkins is a popular CI/CD tool with a rich history in the industry. Compared to newer products like GitHub Actions, it has much less complexity, and when used with Slicer, jobs launch almost instantly.</p> <p>There are two use-cases for Jenkins with Slicer:</p> <ol> <li>Run a Jenkins master and add build-slaves separately or via a public cloud plugin (EC2, GCE, etc)</li> <li>Add ephemeral build slaves to a Jenkins master (existing, or hosted in Slicer)</li> </ol>"},{"location":"examples/jenkins/#learn-more","title":"Learn more","text":"<p>Read how Slicer works and compares to the Docker( Docker In Docker / Docker Socket), Kubernetes (privileged Pod), or EC2 (long start-up times), in our announcement on the Actuated blog</p> <p>Watch a video walkthrough on our YouTube channel.</p>"},{"location":"examples/jenkins/#1-run-a-jenkins-master","title":"1. Run a Jenkins master","text":"<p>You can run a Jenkins master within a Slicer microVM, and have everything set up for you automatically via a userdata script.</p> <p>Create <code>setup-master.sh</code>:</p> <pre><code>#!/usr/bin/env bash\nset -euxo pipefail\n\n# --- Tunables (override via env if you like) ---\n: \"${ADMIN_USER:=admin}\"\n: \"${ADMIN_PASS:=$(openssl rand -base64 18)}\"\n: \"${JENKINS_HTTP_PORT:=8080}\"\n\n# Best-guess URL (update later if you put it behind a domain/reverse-proxy)\nDEFAULT_IP=\"$(hostname -I | awk '{print $1}')\"\n: \"${JENKINS_URL:=http://${DEFAULT_IP}:${JENKINS_HTTP_PORT}/}\"\n\nexport DEBIAN_FRONTEND=noninteractive\n\nif command -v apt-get &gt;/dev/null 2&gt;&amp;1; then\n  # ----- Ubuntu/Debian path -----\n  apt-get update &amp;&amp; \\\n    apt-get install -qy --no-install-recommends \\\n      curl gnupg ca-certificates openjdk-17-jdk git\n\n  # Add Jenkins apt repo\n  install -m 0755 -d /etc/apt/keyrings\n  curl -fsSL https://pkg.jenkins.io/debian-stable/jenkins.io-2023.key | tee /etc/apt/keyrings/jenkins-keyring.asc &gt;/dev/null\n  chmod a+r /etc/apt/keyrings/jenkins-keyring.asc\n  echo \"deb [signed-by=/etc/apt/keyrings/jenkins-keyring.asc] https://pkg.jenkins.io/debian-stable binary/\" &gt; /etc/apt/sources.list.d/jenkins.list\n\n  apt-get update &amp;&amp; \\\n    apt-get install -qy --no-install-recommends \\\n      jenkins\nelse\n  echo \"This setup script targets Debian/Ubuntu (apt). If you need RHEL/Alma/Oracle/Rocky Linux, reach out to us via Discord.\"\n  exit 1\nfi\n\n# ----- Plugins configuration -----\n\n# List of plugins to install (name[:version], one per line)\ncat &gt; /var/lib/jenkins/plugins.txt &lt;&lt;'EOF'\nconfiguration-as-code\ngit\nworkflow-aggregator\ncredentials\nssh-credentials\nEOF\n\n# ----- JCasC config -----\nCCFG_DIR=\"/var/lib/jenkins/casc_configs\"\nmkdir -p \"$CCFG_DIR\"\ncat &gt; \"${CCFG_DIR}/jenkins.yaml\" &lt;&lt;'YAML'\njenkins:\n  systemMessage: \"Jenkins bootstrapped by SlicerVM.com\\n\"\n  mode: EXCLUSIVE\n  numExecutors: 0\n  remotingSecurity:\n    enabled: true\n  labelString: \"system\"\n  securityRealm:\n    local:\n      allowsSignup: false\n      users:\n        - id: \"${ADMIN_USER}\"\n          password: \"${ADMIN_PASS}\"\n  authorizationStrategy:\n    loggedInUsersCanDoAnything:\n      allowAnonymousRead: false\nunclassified:\n  location:\n    adminAddress: \"address not configured yet &lt;nobody@nowhere&gt;\"\n    url: \"${JENKINS_URL}\"\nYAML\n\n# Substitute vars into JCasC (simple inline envsubst using bash)\nsed -i \"s|\\${ADMIN_USER}|${ADMIN_USER}|g\" \"${CCFG_DIR}/jenkins.yaml\"\nsed -i \"s|\\${ADMIN_PASS}|${ADMIN_PASS}|g\" \"${CCFG_DIR}/jenkins.yaml\"\nsed -i \"s|\\${JENKINS_URL}|${JENKINS_URL}|g\" \"${CCFG_DIR}/jenkins.yaml\"\n\n# Ensure Jenkins uses our JCasC and skips the setup wizard\ninstall -d -m 0755 /etc/systemd/system/jenkins.service.d\ncat &gt; /etc/systemd/system/jenkins.service.d/override.conf &lt;&lt;EOF\n[Service]\nEnvironment=JENKINS_HOME=/var/lib/jenkins\nEnvironment=CASC_JENKINS_CONFIG=${CCFG_DIR}/jenkins.yaml\nEnvironment=JAVA_OPTS=-Djenkins.install.runSetupWizard=false\nEnvironment=JENKINS_PORT=${JENKINS_HTTP_PORT}\nEOF\n\n# Download and use Jenkins Plugin Manager\nPLUGIN_MANAGER_JAR=\"/tmp/jenkins-plugin-manager.jar\"\ncurl -L -o \"$PLUGIN_MANAGER_JAR\" \"https://github.com/jenkinsci/plugin-installation-manager-tool/releases/download/2.13.2/jenkins-plugin-manager-2.13.2.jar\"\n\n# Install plugins using the plugin manager\njava -jar \"$PLUGIN_MANAGER_JAR\" -d /var/lib/jenkins/plugins --plugin-file /var/lib/jenkins/plugins.txt --verbose\n\n# Clean up\nrm -f \"$PLUGIN_MANAGER_JAR\"\n\n# Permissions &amp; start\nchown -R jenkins:jenkins /var/lib/jenkins\nsystemctl daemon-reload\nsystemctl enable --now jenkins\n\n# Wait for it to be reachable\ntries=60\nuntil curl -fsS \"http://127.0.0.1:${JENKINS_HTTP_PORT}/login\" &gt;/dev/null 2&gt;&amp;1 || [ $tries -le 0 ]; do\n  sleep 2; tries=$((tries-1))\ndone\n\n# Save creds somewhere easy to grab\ncat &gt; /home/ubuntu/jenkins-admin.txt &lt;&lt;CREDS\nJenkins URL: ${JENKINS_URL}\nUsername:    ${ADMIN_USER}\nPassword:    ${ADMIN_PASS}\n\n(saved by user-data at $(date -u +\"%Y-%m-%dT%H:%M:%SZ\"))\nCREDS\nsudo chown ubuntu:ubuntu /home/ubuntu/jenkins-admin.txt\n\necho \"======================================================\"\necho \" Jenkins is up at: ${JENKINS_URL}\"\necho \" Admin credentials are in: /home/ubuntu/jenkins-admin.txt\"\necho \"======================================================\"\n</code></pre> <p>You can customise the script as you wish, and the admin password will be printed out to the console during VM creation.</p> <p>Now create a jenkins-master.yaml file:</p> <pre><code>slicer new jenkins-master \\\n  --storage image \\\n  --storage-size 25G \\\n  --count 1 \\\n  --cpu 2 \\\n  --ram 4 \\\n  --api-bind 0.0.0.0 \\\n  --userdata-file setup-master.sh \\\n  &gt; jenkins-master.yaml\n</code></pre> <p>The disk size, vCPU and RAM can be adjusted as needed.</p> <p>You can provide one or more SSH keys via:</p> <ul> <li><code>--github</code> - Your GitHub username, used to fetch your public SSH keys from your profile</li> <li><code>--ssh-key</code> - One or more SSH public ssh keys (all on one line) to add to the VM</li> </ul> <p>Now create the VM:</p> <pre><code>sudo slicer up ./jenkins-master.yaml\n</code></pre> <p>You can run an interactive shell into the VM and read the password file directly from <code>/home/ubuntu/jenkins-admin.txt</code>:</p> <pre><code>$ sudo slicer vm shell jenkins-master-1\n\nConnecting to VM: jenkins-master-1\nConnected! Press Ctrl+] to exit.\nWelcome to Ubuntu 22.04.5 LTS (GNU/Linux 5.10.240 x86_64)\n\nroot@jenkins-master-1:/root# cat /home/ubuntu/jenkins-admin.txt\n\nJenkins URL: http://192.168.137.2:8080/\nUsername:    admin\nPassword:    ......\n</code></pre> <p>Open the URL in a browser and go to the Settings page.</p> <p>Add any plugins you want, and create jobs as needed.</p> <p>At a minimum, you should install the Pipeline plugin to allow pipeline jobs to run.</p> <p>Go to \"Manage Jenkins\" -&gt; \"Manage Plugins\" and install the \"Pipeline\" plugin.</p> <p>Then, make sure the URL is set for Jenkins itself:</p> <ol> <li>Go to \"Manage Jenkins\" -&gt; \"Configure System\"</li> <li>Scroll down to \"Jenkins Location\"</li> <li>Set the \"Jenkins URL\" to the same URL you used to access Jenkins in your browser</li> <li>Save the configuration.</li> </ol> <p>If you're able, it's also worth disabling the built-in system executors:</p> <ol> <li>Go to \"Manage Jenkins\" -&gt; \"Configure System\"</li> <li>Scroll down to \"Number of executors\"</li> <li>Set it to <code>0</code></li> <li>Add a label of \"system\"</li> <li>Under \"Usage\" select \"Only build jobs with label expressions matching this node\"</li> <li>Save the configuration.</li> </ol> <p>This ensures that no jobs run on the master itself, which is a best practice for Jenkins installations.</p>"},{"location":"examples/jenkins/#2-run-ephemeral-jenkins-build-slaves-in-slicer-via-api","title":"2. Run ephemeral Jenkins build slaves in Slicer via API","text":"<p>In Jenkins, a \"Cloud\" is simply an API that can add and remove build agents (slaves) on demand.</p> <p>We built one that can talk to a slicer API endpoint to create and destroy VMs as needed.</p> <p>Set up another Slicer instance, ideally on another machine on the same network, or with access via an overlay network like a VPN (Wireguard/OpenVPN).</p> <pre><code>slicer new slave \\\n  --storage zfs \\\n  --persistent false \\\n  --count 0 \\\n  --cpu 2 \\\n  --ram 8 \\\n  --cidr 192.168.138.1/24 \\\n  --api-bind 0.0.0.0 \\\n  --userdata-file setup-master.sh \\\n  --graceful-shutdown  false \\\n  &gt; jenkins-slave.yaml\n</code></pre> <ul> <li><code>--count</code> - set to <code>0</code> because slaves are added via the Java Plugin as required</li> <li><code>--storage</code> - use <code>zfs</code> for best performance when starting up new slaves, or <code>image</code> for quick experimentation</li> <li><code>--ssh-keys</code> - add any public keys you want to be able to SSH into the slave VMs - we recommend this over <code>--github</code> since it's much faster than querying GitHub for every launch</li> <li><code>--graceful-shutdown false</code> - slaves are ephemeral so we don't need to wait for a graceful shutdown - it will be destroyed immediately when the job is done</li> </ul> <p>We recommend that you use ZFS-backed storage for the slaves because with ZFS, a snapshot is unpacked when Slicer starts up and it can be cloned instantly.</p> <p>If you don't have time to set up ZFS and are only experimenting, then you can make the following change (but it may add a few seconds to each launch time):</p> <pre><code>-      storage: zfs\n+      storage: image\n+      storage_size: 20G\n</code></pre> <p>If you're running on the same host as the Jenkins master, ensure that the API and SSH ports don't conflict with the master instance.</p> <pre><code>  api:\n    enabled: true\n+   port: 8081\n    bind_address: \"0.0.0.0:\"\n\n  ssh:\n    bind_address: \"127.0.0.1:\"\n+   port: 2223\n</code></pre>"},{"location":"examples/jenkins/#21-create-a-custom-image-for-the-jenkins-slaves","title":"2.1 Create a custom image for the Jenkins slaves","text":"<p>If you want to pre-install any software on the slave images, you can create your own Dockerfile that extends the base Slicer image.</p> <p>By pre-loading the JRE, we can speed up Jenkins slave startup times by skipping an installation of Java on every boot. We may produce an image with the JRE pre-installed in a future release, but most teams tend to want to add something like Docker or Terraform into their slave images.</p> <p>This image is available via CI/CD as: <code>ghcr.io/openfaasltd/slicer-slave:latest</code></p> <pre><code>FROM ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-latest\n\nRUN apt-get update -qy &amp;&amp; \\\n  apt-get install -qy  --no-install-recommends \\\n  curl \\\n  ca-certificates \\\n  openjdk-17-jre-headless\n\nRUN useradd -m -s /bin/bash jenkins &amp;&amp; \\\n  echo 'jenkins ALL=(ALL) NOPASSWD:ALL' &gt; /etc/sudoers.d/jenkins &amp;&amp; \\\n  chmod 440 /etc/sudoers.d/jenkins\n</code></pre> <p>If you also wanted Docker, and basic Kubernetes tooling, you can add the following extra lines, which is also available via CI/CD as: <code>ghcr.io/openfaasltd/slicer-slave-docker:latest</code></p> <pre><code>RUN arkade get \\\n  --path=/usr/local/bin/ \\\n  --quiet \\\n  k3sup \\\n  kubectl \\\n  helm \\\n  kubectx \\\n  kind\n\nRUN curl -sLS https://get.docker.com | sh &amp;&amp; \\\n  usermod -aG docker jenkins &amp;&amp; \\\n  chmod +x /usr/local/bin/*\n\nRUN systemctl disable regen-ssh-host-keys &amp;&amp; \\\n    systemctl disable ssh &amp;&amp; \\\n    systemctl disable sshd &amp;&amp; \\\n    systemctl disable docker\n</code></pre> <p>We disable docker in order to make the slave start faster, however Docker can still be activated via systemd's socket activation when needed.</p> <p>Then build and publish the image to your own container registry.</p> <p>Edit your <code>jenkins-slaves.yaml</code> file to use your custom image:</p> <pre><code>-  image: \"ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-latest\"\n+  image: \"ghcr.io/openfaasltd/slicer-slave-docker:latest\"\n</code></pre> <p>Then you only need to relaunch the Slicer slave instance for the new packages to be available.</p>"},{"location":"examples/jenkins/#22-start-up-the-slicer-slave-instance","title":"2.2 Start up the Slicer slave instance","text":"<p>Create the Slicer instance for the slaves:</p> <pre><code>sudo slicer up ./jenkins-slaves.yaml\n</code></pre> <p>If your master and slave are running on different hosts, make sure you run the routing commands (<code>ip route add</code>) that are printed out on start up.</p> <ul> <li>Master range: 192.168.131.0/24 (255 IP addresses, only one is needed)</li> <li>Slave range: 192.168.138.0/24 (255 IP addresses)</li> </ul> <p>If they are both on the same host, then you must not run those commands.</p>"},{"location":"examples/jenkins/#23-add-the-slicer-cloud-to-jenkins","title":"2.3 Add the Slicer Cloud to Jenkins","text":"<p>Add the \"Slicer VM Cloud\" plugin to your Jenkins master via the plugins page.</p> <p>Upload the <code>slicer-vm-cloud.hpi</code> file you received from our team.</p> <p>Then go to \"Manage Jenkins\" -&gt; \"Configure System\" and scroll down to the \"Cloud\" section.</p> <p>Add a new Cloud, pick \"Slicer VM Cloud\".</p> <p>Enter all the details including the API endpoint for the Slicer instance running the slaves, and its API token (the location is printed upon start-up).</p>"},{"location":"examples/jenkins/#24-example-pipeline-jobs","title":"2.4 Example Pipeline jobs","text":"<p>For every <code>pipeline</code> job you create, you need the following directive:</p> <pre><code>pipeline {\n  agent { label 'slicer'}\n  options { timeout(time: 2, unit: 'MINUTES') }\n</code></pre> <p>Now run a quick test job:</p> <pre><code>pipeline {\n  agent { label 'slicer'}\n  options { timeout(time: 2, unit: 'MINUTES') }\n    stages {\n    stage('Build') { steps { sh '''\ncat /etc/hostname\n    ''' } }\n  }\n}\n</code></pre> <p>If you see the VM launch, and the runner get created in Jenkins, but it's not able to connect, check the log file for the VM itself under <code>/var/log/slicer/</code>. It should be called <code>slave-1.txt</code> if it's the first launch so far.</p> <p>Here's an example that runs a container via Docker:</p> <pre><code>pipeline {\n  agent { label 'slicer'}\n  options { timeout(time: 2, unit: 'MINUTES') }\n\n  stages {\n    stage('Build') { steps { sh '''\nsudo systemctl start docker\ndocker run -i alpine:latest ping -c 4 google.com\n    ''' } }\n  }\n}\n</code></pre> <p>Another example that installs Kubernetes via K3s and installs OpenFaaS Community Edition (CE):</p> <pre><code>pipeline {\n  agent { label 'slicer'}\n  options { timeout(time: 2, unit: 'MINUTES') }\n\n  stages {\n    stage('Build') { steps { sh '''\nexport PATH=$PATH:$HOME/.arkade/bin\n\narkade get k3sup kubectl --progress=false\n\nexport KUBECONFIG=`pwd`/kubeconfig\n\nk3sup install --local --no-extras\nk3sup ready --attempts 5 --pause 100ms\n\nkubectl get nodes -o wide\nkubectl get pods -A -o wide\n\narkade install openfaas\n\nkubectl get deploy -n openfaas -o wide\n''' } }\n  }\n\n}\n</code></pre> <p>The rest is up to you.</p>"},{"location":"examples/jenkins/#3-questions-and-support","title":"3. Questions and support","text":"<p>If you have any questions or notice anything unexpected, reach out to us via our support channels. Use Discord for standard Slicer tiers, email for paid support plans, and Slack/Email for Enterprise.</p> <p>Is the Jenkins URL empty or nil?</p> <p>You can then navigate to the Jenkins URL in your web browser to access the Jenkins master using the IP address from the YAML file i.e.</p> <pre><code>http://192.168.137.2:8080/\n</code></pre> <p>Even if you already see this string, re-enter it and hit Save. Otherwise Jenkins won't be able to generate correct links later on for build slaves.</p> <p>Are you having issues with the build slaves?</p> <p>Find out whether there are networking or other issues with the following</p> <ul> <li>Check the logs of the jenkins server on the master</li> </ul> <pre><code>sudo slicer vm shell jenkins-master-1\nsudo journalctl -u jenkins.service -f\n</code></pre> <ul> <li>Check the logs of the slave VM if it is booting</li> </ul> <pre><code>sudo cat /var/log/slicer/slave-1.txt\n</code></pre> <p>Check networking/routing</p> <p>If your two Slicer instances are running on different machines, make sure you ran the <code>ip route</code> commands as directed.</p> <p>To verify the routes on either machine use <code>ping</code> between the two <code>.1</code> IP addresses i.e. <code>ping 192.168.138.1</code> from the master host, and <code>ping 192.168.137.1</code> from the slave host.</p> <p>Check the proper Slicer URL was given for the Slicer Cloud</p> <p>So long as you haven't changed anything in the tutorial, the Slicer URL should be <code>http://192.168.141.1:8082</code>.</p> <p>The token is read from <code>/var/slicer/auth/token</code>.</p>"},{"location":"examples/k3s-gpu/","title":"Kubernetes with GPUs","text":"<p>In this example, we'll adapt elements of the HA Kubernetes example and the GPU Ollama setup to work together, so you can launch Pods with GPU acceleration.</p>"},{"location":"examples/k3s-gpu/#set-up-a-config","title":"Set up a config","text":"<p>Create the <code>k3s-gpu.yaml</code> file as below:</p> <pre><code>config:\n  host_groups:\n  - name: gpu\n    storage: image\n    storage_size: 30G\n    count: 1\n    vcpu: 4\n    ram_gb: 16\n    gpu_count: 1\n    network:\n      bridge: brgpu0\n      tap_prefix: gputap\n      gateway: 192.168.139.1/24\n\n  github_user: alexellis\n\n  image: \"ghcr.io/openfaasltd/slicer-systemd-ch:5.10.240-x86_64-latest\"\n\n  hypervisor: cloud-hypervisor\n</code></pre> <p>Feel free to customise the vCPU, RAM, and disk sizes.</p> <p>Boot up the VM:</p> <pre><code>sudo slicer up ./k3s-gpu.yaml\n</code></pre> <p>Now, run the route commands so you can SSH into the host from your workstation.</p> <p>Next, log into each VM via SSH:</p> <pre><code>ssh ubuntu@192.168.139.2\n</code></pre> <p>Next, install K3s using K3sup Pro or K3sup CE.</p> <p>For CE:</p> <pre><code>k3sup install --host 192.168.139.2 --user ubuntu\n</code></pre> <p>You'll get a kubeconfig returned, run the commands to export it so kubectl uses it.</p> <p>Install the Nvidia driver:</p> <pre><code>curl -SLsO https://raw.githubusercontent.com/self-actuated/nvidia-run/refs/heads/master/setup-nvidia-run.sh\nchmod +x ./setup-nvidia-run.sh\nsudo bash ./setup-nvidia-run.sh\n</code></pre> <p>Install the Nvidia Container Toolkit using the official instructions. Use the instructions for \"apt: Ubuntu, Debian\".</p> <p>Confirm that the nvidia container runtime has been found by K3s:</p> <pre><code>sudo grep nvidia /var/lib/rancher/k3s/agent/etc/containerd/config.toml\n</code></pre> <p>Apply a new runtime class for Nvidia:</p> <pre><code>cat &gt; nvidia-runtime.yaml &lt;&lt;EOF\napiVersion: node.k8s.io/v1\nkind: RuntimeClass\nmetadata:\n  name: nvidia\nhandler: nvidia\nEOF\n\nkubectl create -f nvidia-runtime.yaml\n</code></pre> <p>Run a test Pod to show the output from nvidia-smi:</p> <pre><code>cat &gt; nvidia-smi-pod.yaml &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nvidia-smi\nspec:\n  runtimeClassName: nvidia\n  restartPolicy: OnFailure\n  containers:\n    - name: nvidia-smi\n      image: nvidia/cuda:12.1.0-base-ubuntu22.04\n      command: ['sh', '-c', \"nvidia-smi\"]\nEOF\n\nkubectl create -f nvidia-smi-pod.yaml\n</code></pre> <p>For the time-being, this Pod uses only the <code>runtimeClassName</code> to request a GPU. Adding the usual <code>limits</code> section as below, does not work at present, and may require additional configuration in K3s or containerd:</p> <pre><code>+     resources:\n+       limits:\n+           nvidia.com/gpu: \"1\"\n</code></pre> <p>Fetch the logs:</p> <pre><code>kubectl logs nvidia-smi\n</code></pre> <pre><code>$ kubectl logs pod/nvidia-smi\nMon Sep  1 15:04:11 2025\n+-----------------------------------------------------------------------------------------+\n| NVIDIA-SMI 580.76.05              Driver Version: 580.76.05      CUDA Version: 13.0     |\n+-----------------------------------------+------------------------+----------------------+\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n|                                         |                        |               MIG M. |\n|=========================================+========================+======================|\n|   0  NVIDIA GeForce RTX 3090        Off |   00000000:00:07.0 Off |                  N/A |\n| 30%   46C    P0            110W /  350W |       0MiB /  24576MiB |      0%      Default |\n|                                         |                        |                  N/A |\n+-----------------------------------------+------------------------+----------------------+\n\n+-----------------------------------------------------------------------------------------+\n| Processes:                                                                              |\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n|        ID   ID                                                               Usage      |\n|=========================================================================================|\n|  No running processes found                                                             |\n+-----------------------------------------------------------------------------------------+\n</code></pre>"},{"location":"examples/k3s-gpu/#enable-device-plugin","title":"Enable Device Plugin","text":"<p>Install Nvidia's Device Plugin for Kubernetes:</p> <pre><code>kubectl create -f https://raw.githubusercontent.com/NVIDIA/k8s-device-plugin/v0.17.1/deployments/static/nvidia-device-plugin.yml\n</code></pre> <p>Patch it so it works with K3s:</p> <pre><code># add runtimeClassName: nvidia to the DS pod spec\nkubectl -n kube-system patch ds nvidia-device-plugin-daemonset \\\n  --type='json' \\\n  -p='[{\"op\":\"add\",\"path\":\"/spec/template/spec/runtimeClassName\",\"value\":\"nvidia\"}]'\n\nkubectl -n kube-system rollout status ds/nvidia-device-plugin-daemonset -n kube-system\n</code></pre> <p>Then run the Pod from earlier, but with the <code>limits</code> in place:</p> <pre><code>cat &gt; nvidia-smi-pod.yaml &lt;&lt;EOF\napiVersion: v1\nkind: Pod\nmetadata:\n  name: nvidia-smi\nspec:\n  runtimeClassName: nvidia\n  restartPolicy: OnFailure\n  containers:\n    - name: nvidia-smi\n      image: nvidia/cuda:12.1.0-base-ubuntu22.04\n      command: ['sh', '-c', \"nvidia-smi\"]\n      resources:\n        limits:\n            nvidia.com/gpu: \"1\"\nEOF\n\nkubectl create -f nvidia-smi-pod.yaml\n</code></pre>"},{"location":"examples/large-scale-k3s/","title":"Large Scale Kubernetes (k3s)","text":"<p>Since the default limit for Kubernetes is around 100 Pods per node, installing Kubernetes directly to a host will not allow for large scale deployments.</p> <p>This blog post is intended for creating large clusters for testing/experimentation, R&amp;D, and customer support purposes.</p> <p>It uses a snapshotting filesystem with CoW, so that only a minimal amount of disk space needs to be allocated, and launch times are kept very fast.</p> <p>You can of course install any Kubernetes distribution in two ways:</p> <ul> <li>Via SSH (K3sup Pro, Ansible, etc)</li> <li>Via a userdata script written in bash</li> </ul> <p>Since K3sup Pro runs in parallel, is designed to work with Slicer's API, and is included for free for Slicer customers, we'll use it here for a speedy installation.</p> <p>Video demo of setting up a large scale K3s cluster:</p>"},{"location":"examples/large-scale-k3s/#setup-the-vm-configuration","title":"Setup the VM configuration","text":"<p>Consider the target hardware, and the specification you require for your nodes.</p> <p>If your system like ours has a 96-core Arm processor, with 196GB of RAM, you could split it up in any number of ways.</p> <p></p> <p>The Adlink Ampere Developer Platform loaded up with 96 cores and 196GB RAM.</p> <ul> <li>12 nodes with 8 cores and 16GB RAM each</li> <li>24 nodes with 4 cores and 8GB RAM each</li> <li>32 nodes with 3 cores and 6GB RAM each</li> <li>48 nodes with 2 cores and 4GB RAM each</li> <li>96 nodes with 1 core and 2GB RAM each</li> </ul> <p>Even if you don't have a large machine like this, you could run Slicer on multiple machines to get to a similar size.</p> <p>Let's go for the mid-ground with 24 nodes.</p> <p>Create <code>k3s-scale.yaml</code>:</p> <pre><code>slicer new k3s \\\n  --storage zfs \\\n  --persistent true \\\n  --count 24 \\\n  --vcpu 4 \\\n  --ram_gb 8 \\\n  &gt; k3s-scale.yaml\n</code></pre> <p>If you need to run the same test with a fresh cluster over and over again, you can change the <code>--persistent true</code> configuration to <code>--persistent false</code>. Then the snapshots will be discarded when Slicer exits. Otherwise, they are retained indefinitely.</p> <p>Now launch Slicer in a tmux window, so you can come back to it later:</p> <pre><code>tmux new-session -s slicer\n\nsudo slicer up ./k3s-scale.yaml\n</code></pre> <p>The Root filesystem will be downloaded and unpacked, then all the VMs will be launched.</p>"},{"location":"examples/large-scale-k3s/#install-kubernetes-with-k3sup-pro","title":"Install Kubernetes with K3sup Pro","text":"<p>Access the API on the server to download the devices file:</p> <pre><code>curl -LsS http://127.0.0.1:8080/devices -o devices.json\n</code></pre> <p>On your workstation, add any routes that are specified so you can access the VMs on their own network.</p> <p>Download the devices.json file to your local computer, and install K3sup Pro:</p> <pre><code>curl -sSL https://get.k3sup.dev | PRO=true sudo -E sh\n</code></pre> <p>Next, create a K3sup Pro plan using the devices file. Simply run the command in the same folder as <code>devices.json</code>.</p> <p>Pick an amount of nodes to dedicate to being servers, the rest will be agents.</p> <p>Always use an odd number, with a minimum of three. In K3s, servers can also run workloads by default.</p> <pre><code>k3sup pro plan \\\n  --servers 3 \\\n  --traefik=false \\\n  --user ubuntu\n</code></pre> <p>The plan command creates or updates a plan.yaml file in the local directory. You can view it or edit it.</p> <p>Apply the plan, the first server is created, then all other nodes are added in parallel based upon the <code>--parallel</code> flag.</p> <pre><code>k3sup pro apply --parallel 8\n</code></pre> <p>After a short period of time, your cluster will be ready.</p>"},{"location":"examples/large-scale-k3s/#get-access-to-the-kubeconfig","title":"Get access to the kubeconfig","text":"<p>Merge it into your KUBECONFIG file:</p> <pre><code>mkdir -p ~/.kube\ncp ~/.kube/config ~/.kube/config.bak || true\n\nk3sup-pro get-config \\\n --local-path ~/.kube/config \\\n --merge \\\n --context slicer-k3s-scale\n</code></pre> <p>Next switch into the context and install something to try out the cluster, like OpenFaaS CE.</p> <pre><code>arkade get kubectx\nkubectx slicer-k3s-scale\n\nkubectl get nodes -o wide\nkubectl top pod\n\narkade install openfaas\n</code></pre>"},{"location":"examples/multiple-machine-k3s/","title":"Multiple machine Kubernetes","text":"<p>In the HA Kubernetes and Large scale Kubernetes examples, we showed a single Kubernetes cluster running on a single Slicer host.</p> <p>In this example, we'll show you spread Kubernetes clusters over multiple machines, each running Slicer.</p> <ol> <li>You want a multi-arch Kubernetes cluster - with Slicer on an x86_64 and Arm64 machine</li> <li>You want a much larger cluster than you can fit on any single machine</li> </ol> <p>The basic idea is that you run Slicer on two or more machines, which manages its own set of VMs. These are put on distinct subnets to avoid IP address conflicts, then a rule is added to the routing table to enable inter-connectivity. This isn't limited to just Kubernetes, you could run any network-based product or service you like across multiple machines with this technique.</p> <p></p> <p>If I need a very large Kubernetes cluster I'll often run the control-plane on an x86_64 Ryzen 9 7900X machine, and the worker nodes on an Arm64 Ampere Altra machine like the Adlink AADP pictured above. The machine above has 96 Arm cores and 196GB RAM.</p>"},{"location":"examples/multiple-machine-k3s/#install-slicer-on-each-machine","title":"Install Slicer on each machine","text":"<p>First, install Slicer on each machine, following the installation instructions.</p> <p>For storage, we'll use the <code>image</code> setting, however if you're going to create many nodes, consider using ZFS for an instant clone of the VM filesystem, and reduced disk space consumption through ZFS's snapshots and Copy On Write (CoW) feature.</p>"},{"location":"examples/multiple-machine-k3s/#create-a-yaml-file-for-each-machine","title":"Create a YAML file for each machine","text":"<p>You need to dedicate one subnet per machine, so that there are no IP address conflicts.</p> <p>Then, you should ideally change the <code>name:</code> prefix of the host group, so that the machines have unique names.</p> <p>The first machine could use:</p> <pre><code>slicer new k3s-a \\\n  --cpu 2 \\\n  --ram 4 \\\n  --count=3 \\\n  --cidr 192.168.137.0/24 \\\n  &gt; k3s-a.yaml\n</code></pre> <p>Then the second would have a different <code>name:</code> prefix, and a different network section.</p> <pre><code>slicer new k3s-b \\\n  --cpu 2 \\\n  --ram 4 \\\n  --count=3 \\\n  --cidr 192.168.138.0/24 \\\n  &gt; k3s-b.yaml\n</code></pre>"},{"location":"examples/multiple-machine-k3s/#start-slicer-on-each-machine","title":"Start Slicer on each machine","text":"<p>On each respective machine run:</p> <pre><code>sudo slicer up ./k3s-a.yaml\n</code></pre> <pre><code>sudo slicer up ./k3s-b.yaml\n</code></pre>"},{"location":"examples/multiple-machine-k3s/#enable-routing-between-the-machines-and-vms","title":"Enable routing between the machines and VMs","text":"<p>Now, you need to be careful here.</p> <p>Copy and paste the routing commands printed upon start-up.</p> <ol> <li>On machine A, run the command from machine B</li> <li>On machine B, run the command from machine A</li> <li>On your workstation, which needs to access all of the VMs, run both commands</li> </ol>"},{"location":"examples/multiple-machine-k3s/#install-kubernetes-with-k3sup-pro","title":"Install Kubernetes with K3sup Pro","text":"<p>On each host run the following:</p> <pre><code>curl -SLsf http://127.0.0.1:8080/nodes &gt; devices-a.json\n\ncurl -SLsf http://127.0.0.1:8080/nodes &gt; devices-b.json\n</code></pre> <p>If you enabled authentication for the API, include the <code>-H \"Authorization Bearer TOKEN\"</code> header as printed out when Slicer started up.</p> <p>Copy the two JSON files to your own workstation.</p> <p>Switch over to a terminal on your own workstation.</p> <p>K3sup Pro can support multiple devices.json files, the first file will be taken for the servers, the rest will be used as agents and installed in parallel.</p> <pre><code>PRO=1 curl -sSL https://get.k3sup.dev | sudo sh\n</code></pre> <pre><code>k3sup-pro plan --user ubuntu \\\n  --devices ./devices-a.json \\\n  --devices ./devices-b.json \\\n  --servers 3 \\\n  --traefik=false\n</code></pre> <p>After the plan.yaml file is generated, you can run <code>k3sup-pro apply</code> to setup the cluster.</p> <pre><code>k3sup-pro apply\n</code></pre> <p>After a few moments, you can get the KUBECONFIG and merge it into your existing kubeconfig file:</p> <pre><code>mkdir -p ~/.kube\ncp ~/.kube/config ~/.kube/config.bak || true\n\nk3sup-pro get-config \\\n    --local-path ~/.kube/config \\\n    --merge \\\n    --context slicer-multi\n</code></pre> <p>Then you can run <code>kubectx slicer-multi</code> to switch to your new cluster and explore it with kubectl.</p> <p>If your two machines were different kinds of architecture, explore the labels applied with:</p> <pre><code>kubectl get nodes --show-labels\n</code></pre> <p>Then you can deploy a Pod with a nodeSelector, or affinity/anti-affinity rule to control where your workloads run.</p> <p>For a number of quick Kubernetes applications to try out, see <code>arkade install --help</code>.</p> <p>The OpenFaaS CE app is quick to install and has a number of samples you can play with.</p>"},{"location":"examples/opencode-agent/","title":"Run a Headless Coding Agent with OpenCode","text":"<p>We'll run a headless coding agent within a microVM using OpenCode, an open-source tool to generate code using large language models (LLMs).</p> <p>OpenCode occasionally supports free trials for popular LLMs. This guide was written whilst Grok's Coder model was available for free - and at no cost.</p> <p>OpenCode supports various model providers including GitHub CoPilot, OpenAI, Ollama (self-hosted models), and Anthropic.</p> <p>This example is very minimal and can be tuned and adapted in many ways to suit your workflow.</p> <p>The general idea is that everything is orchestrated via an initial userdata script:</p> <ul> <li>Install opencode CLI.</li> <li>Configure model authentication using a pre-defined config file <code>~/.local/share/opencode/auth.json</code> from another machine like your workstation.</li> <li>Set the agent working away as a systemd service, with a prompt set in the userdata file.</li> </ul> <p>This example is one-shot, so it's designed to run to completion once, without any further interaction.</p> <p></p> <p>Example of a one-shot execution for a sample prompt to create a Go HTTP server, and to test it via <code>curl</code></p>"},{"location":"examples/opencode-agent/#example-config","title":"Example config","text":"<p>Use <code>slicer new</code> to generate a configuration file:</p> <pre><code>slicer new opencode \\\n  --userdata-file opencode.sh \\\n  &gt;  opencode.yaml\n</code></pre> <p>And the <code>--ssh-keys</code> or <code>--github</code> flag to add additional ssh keys so you can connect via SSH to review the logs, and/or scp to pull out any generated code.</p> <p>On a computer where you've pre-installed and authenticated opencode, copy the opencode auth config file <code>~/.local/share/opencode/auth.json</code>.</p> <p>On host create a slicer secret for the opencode auth config file.</p> <pre><code>sudo mkdir .secrets\n# Ensure only root can read/write to the secrets folder.\nsudo chmod 700 .secrets\n\nsudo -E cp ~/.local/share/opencode/auth.json .secrets/opencode-auth.json\n</code></pre> <p>Then, create opencode.sh:</p> <pre><code>#!/usr/bin/env bash\n# Ubuntu only. Requires: curl + arkade preinstalled.\n# Installs opencode, sets pre-auth, creates a daemonized systemd unit \"opencode.service\".\n\nset -euo pipefail\n\n# Install opencode -&gt; /usr/local/bin\narkade get opencode --path /usr/local/bin &gt;/dev/null\nchown ubuntu /usr/local/bin/opencode\nchmod +x /usr/local/bin/opencode\n\n# Prep dirs &amp; auth for user \"ubuntu\" (no group assumption)\nfor d in /home/ubuntu/workdir /home/ubuntu/.local/share/opencode /home/ubuntu/.local/state /home/ubuntu/.cache; do\n  mkdir -p \"$d\"\n  chown ubuntu \"$d\"\ndone\ncp /run/slicer/secrets/opencode-auth.json /home/ubuntu/.local/share/opencode/auth.json\nchown ubuntu /home/ubuntu/.local/share/opencode/auth.json\nchmod 600 /home/ubuntu/.local/share/opencode/auth.json\n\n# Task payload (edit as needed)\ncat &gt;/home/ubuntu/workdir/task.txt &lt;&lt;'EOF'\nCreate a new Go program with an HTTP server.\nAdd GET /healthz returning 201 \"Created\".\nPrint a git diff at the end.\n\nUse \"arkade system install golang\" to install Go into the environment. Then test the program with \"go run main.go\" and curl localhost:8080/healthz - make sure you kill the program after confirming the correct HTTP code was returned.\n\nSet the PATH variable to include /usr/local/go/bin so that the go command is found.\nEOF\n\nchown ubuntu /home/ubuntu/workdir/task.txt\nchmod 600 /home/ubuntu/workdir/task.txt\n\n# systemd service (daemonized under systemd)\ncat &gt;/etc/systemd/system/opencode.service &lt;&lt;'EOF'\n[Unit]\nDescription=OpenCode one-shot (daemonized under systemd)\nWants=network-online.target\nAfter=network-online.target\n\n[Service]\nType=simple\nUser=ubuntu\nWorkingDirectory=/home/ubuntu/workdir\nEnvironment=HOME=/home/ubuntu\nEnvironment=XDG_STATE_HOME=/home/ubuntu/.local/state\nEnvironment=XDG_CACHE_HOME=/home/ubuntu/.cache\nEnvironment=XDG_DATA_HOME=/home/ubuntu/.local/share\nExecStart=/usr/bin/env bash -lc '/usr/local/bin/opencode run \"$(cat /home/ubuntu/workdir/task.txt)\" -m opencode/grok-code'\nRestart=no\nStandardOutput=journal\nStandardError=journal\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsystemctl daemon-reload\nsystemctl enable --now opencode.service\n</code></pre> <p>Just before you start up the VM, make sure you customise the prompt to have the agent do whatever it is you need.</p> <pre><code>cat &gt;/home/ubuntu/workdir/task.txt &lt;&lt;'EOF'\n\nPrompt goes here\nOver multiple lines\n\nEOF\n</code></pre> <p>Start up the VM i.e. <code>sudo slicer up ./opencode.yaml</code></p> <p>What if you want to copy in a private Git repository?</p> <p>One option is to include an SSH key for the agent/ubuntu user, so that it can clone the repository directly from GitHub or another Git server. To keep permissions tight, you could also simply <code>scp</code> the code into the microVM after it has booted, like below.</p> <p>If you want to work on a private Git repository, simply have the systemd unit wait until it finds a folder within the workdir folder, and then scp the code from your host after the microVM has booted.</p> <p>So if the directory was named i.e. <code>arkade</code>, and we'd cloned it locally, you could amend the <code>opencode</code> systemd unit like so:</p> <pre><code>[Service]\nExecStartPre=/usr/bin/env bash -c 'while [ ! -d /home/ubuntu/workdir/arkade ]; do sleep 5; done'\nExecStart=/usr/bin/env bash -lc '/usr/local/bin/opencode run \"$(cat /home/ubuntu/workdir/task.txt)\" -m opencode/grok-code'\n</code></pre>"},{"location":"examples/opencode-agent/#view-the-results","title":"View the results","text":"<p>Once the VM is running, you can check the status of the <code>opencode</code> service.</p> <p>The code will be written to the <code>$HOME/workdir</code> directory.</p> <pre><code>ssh ubuntu@192.168.137.2\n\nsudo journalctl -u opencode.service -f\n\ncd workdir\nfind .\n\ngit diff\n</code></pre> <p>In the example of the preloaded prompt from above, we saw in <code>~/workdir/main.go</code>:</p> <pre><code>package main\n\nimport (\n        \"fmt\"\n        \"net/http\"\n)\n\nfunc main() {\n        http.HandleFunc(\"/healthz\", func(w http.ResponseWriter, r *http.Request) {\n                w.WriteHeader(http.StatusOK)\n                fmt.Fprint(w, \"ok\")\n        })\n        http.ListenAndServe(\":8080\", nil)\n}\n</code></pre> <p>To copy out the workdir, run something like this on your host:</p> <pre><code>mkdir -p agent-runs\ncd agent-runs\nscp -r ubuntu@192.168.137.2:/home/ubuntu/workdir .\n</code></pre>"},{"location":"examples/opencode-agent/#running-an-opencode-agent-via-the-exec-and-cp-apis","title":"Running an opencode agent via the exec and cp APIs","text":"<p>You can also run an opencode agent from bash after booting up a host, and syncing in a secret for the API keys.</p> <pre><code>slicer new opencode \\\n  --userdata-file userdata.sh \\\n  &gt;  opencode.yaml\n</code></pre> <p>Create your userdata to only setup opencode, its workdir, and to sync the API secret to the correct location from the host.</p> <p>userdata.sh</p> <pre><code>#!/usr/bin/env bash\n\nset -euo pipefail\n\n# Install opencode -&gt; /usr/local/bin\narkade get opencode --path /usr/local/bin &gt;/dev/null\nchown ubuntu /usr/local/bin/opencode\nchmod +x /usr/local/bin/opencode\n\n# Prep dirs &amp; auth for user \"ubuntu\" (no group assumption)\nfor d in /home/ubuntu/workdir /home/ubuntu/.local/share/opencode /home/ubuntu/.local/state /home/ubuntu/.cache; do\n  mkdir -p \"$d\"\n  chown ubuntu \"$d\"\ndone\ncp /run/slicer/secrets/opencode-auth.json /home/ubuntu/.local/share/opencode/auth.json\nchown ubuntu /home/ubuntu/.local/share/opencode/auth.json\nchmod 600 /home/ubuntu/.local/share/opencode/auth.json\n</code></pre> <p>Authenticate to opencode on your host, then copy the auth file to the .secrets folder relative to where you created <code>opencode.yaml</code>.</p> <pre><code>#opencode auth login --provider github --token &lt;your-github-token&gt;\n\nmkdir -p .secrets\nchmod 700 .secrets\ncp ~/.local/share/opencode/auth.json .secrets/opencode-auth.json\nchmod 600 .secrets/opencode-auth.json\n</code></pre> <p>Now boot up the microVM, just before userdata it will synchronise any secrets placed in the .secrets folder into the microVM.</p> <pre><code>sudo slicer up ./opencode.yaml\n</code></pre> <p>The following command will indicate whether the userdata script has completed:</p> <pre><code>sudo slicer vm health opencode-1 \nHOSTNAME                  AGENT UPTIME         SYSTEM UPTIME        AGENT VERSION   USERDATA RAN\n--------                  ------------         -------------        -------------   ------------\nopencode-1                53s                  53s                  0.1.55          1   \n</code></pre> <p>If you're automating the process from bash, you can also run <code>--json</code> for something you can parse with <code>jq</code>.</p> <p>Define a prompt and use the <code>cp</code> command to sync it into the microVM:</p> <pre><code>cat &gt; task.txt &lt;&lt;'EOF'\n\nWrite a simple Go program that prints \"Hello, World!\" to the console.\n\nEOF\n</code></pre> <pre><code>sudo slicer vm cp --uid 1000 ./task.txt opencode-1:/home/ubuntu/workdir/task.txt\n</code></pre> <p>Next, use the <code>exec</code> command to run the opencode agent interactively, streaming the response back to the host.</p> <pre><code>sudo slicer vm exec --uid 1000 --cwd /home/ubuntu/workdir opencode-1 -- opencode run -m opencode/grok-code $(cat task.txt)\n\n# Example output:\n\n|  Write    home/ubuntu/workdir/hello.go\nCreated hello.go\n</code></pre> <p>You can now see the response in the terminal.</p> <p>Finally, copy out the results from the microVM via <code>cp</code> in the alternate direction.</p> <pre><code>sudo slicer vm cp --uid 1000 opencode-1:/home/ubuntu/workdir/main.go ./main.go\n</code></pre> <p>Examine the output:</p> <pre><code>package main\n\nimport \"fmt\"\n\nfunc main() {\n        fmt.Println(\"Hello, World!\")\n}\n</code></pre> <p>It's also possible to copy back the whole workdir folder, which is ideal when opencode has created multiple files and folders.</p> <pre><code>mkdir -p workdir-out\nsudo slicer vm cp --mode=tar --uid 1000 opencode-1:/home/ubuntu/workdir/ ./workdir-out\n</code></pre> <p>Then, examine the contents:</p> <pre><code>sudo find workdir-out\n\nworkdir-out\nworkdir-out/main.go\nworkdir-out/hello.go\nworkdir-out/workdir\nworkdir-out/task.txt\n</code></pre>"},{"location":"examples/opencode-agent/#taking-opencode-automation-further","title":"Taking opencode automation further","text":"<p>Once a microVM has been started, you can run the <code>opencode</code> agent in prompt mode any number of times, or limit it to just a single prompt.</p> <p>One advantage of keeping a microVM alive, is that you can follow-up if the results are not as expected, or give it further direction.</p> <p>A practical application of opencode in a Slicer sandbox, is for code reviews linked to a Source Control Management (SCM) system like GitHub or GitLab.</p> <p>The CLI can be driven manually or via bash, for our own code review bot, we used the Go SDK.</p>"},{"location":"examples/openfaas-edge/","title":"OpenFaaS Edge in slicer","text":"<p>Follow this guide to setup a pre-configured microVM with OpenFaas Edge deployed along with the OpenFaaS Function Builder API.</p> <p>After following the steps in this example you will be able to build and deploy functions to faasd directly from the VM or access the gateway and function builder from other machines by using the VM's IP.</p>"},{"location":"examples/openfaas-edge/#userdata","title":"Userdata","text":"<p>Create <code>openfaas-edge.sh</code>.</p> <p>By default the script installs OpenFaaS Edge a private registry and the OpenFaaS Function Builder API as additional services. Select which services to install by changing the environment variables in the script configuration section.</p> <p>Available configuration options:</p> Env Description Default INSTALL_REGISTRY Install a private registry <code>true</code> INSTALL_BUILDER Install the OpenFaaS Function Builder API <code>true</code> <p>OpenfaaS Edge license</p> <p>Slicer subcriptions or GitHub sponsorship does not include the function builder addon for OpenFaaS Edge. A separate OpenFaaS Edge license is required to use the function builder.</p> <pre><code>#!/usr/bin/env bash\n\n#==============================================================================\n# OpenFaaS Edge Installation Script\n#==============================================================================\n# This script installs OpenFaaS Edge, including optional\n# components like a private registry and function builder.\n#==============================================================================\n\nset -euxo pipefail\n\n#==============================================================================\n# CONFIGURATION\n#==============================================================================\n\n# Install additional services (registry and function builder)\nexport INSTALL_REGISTRY=true\nexport INSTALL_BUILDER=true\n\n#==============================================================================\n# SYSTEM PREPARATION and OpenFaaS Edge Installation\n#==============================================================================\n\nhas_dnf() {\n  [ -n \"$(command -v dnf)\" ]\n}\n\nhas_apt_get() {\n  [ -n \"$(command -v apt-get)\" ]\n}\n\necho \"==&gt; Configuring system packages and dependencies...\"\n\nif $(has_apt_get); then\n  export HOME=/home/ubuntu\n\n  sudo apt update -y\n\n  # Configure iptables-persistent to avoid interactive prompts\n  echo iptables-persistent iptables-persistent/autosave_v4 boolean false | sudo debconf-set-selections\n  echo iptables-persistent iptables-persistent/autosave_v6 boolean false | sudo debconf-set-selections\n\n  arkade oci install --path . ghcr.io/openfaasltd/faasd-pro-debian:latest\n  sudo apt install ./openfaas-edge-*-amd64.deb --fix-broken -y\n\n  if [ \"${INSTALL_REGISTRY}\" = \"true\" ]; then\n    sudo apt install apache2-utils -y\n  fi\nelif $(has_dnf); then\n  export HOME=/home/slicer\n\n  arkade oci install --path . ghcr.io/openfaasltd/faasd-pro-rpm:latest\n  sudo dnf install openfaas-edge-*.rpm -y\n\n\n  if [ \"${INSTALL_REGISTRY}\" = \"true\" ]; then\n    sudo dnf install httpd-tools -y\n  fi\nelse\n    fatal \"Could not find apt-get or dnf. Cannot install dependencies on this OS.\"\n    exit 1\nfi\n\n# Install faas-cli\narkade get faas-cli --progress=false --path=/usr/local/bin/\n\n# Create the secrets directory and touch the license file\nsudo mkdir -p /var/lib/faasd/secrets\ntouch /var/lib/faasd/secrets/openfaas_license\n\n#==============================================================================\n# PRIVATE REGISTRY AND FUNCTION BUILDER SETUP\n#==============================================================================\n\n# Always install registry if builder is installed\nif [ \"${INSTALL_BUILDER}\" = \"true\" ]; then\n INSTALL_REGISTRY=true\nfi\n\nif [ \"${INSTALL_REGISTRY}\" = \"true\" ]; then\n    echo \"==&gt; Setting up private container registry...\"\n\n    # Generate registry authentication\n    export PASSWORD=$(openssl rand -base64 16)\n    echo $PASSWORD &gt; $HOME/registry-password.txt\n\n    # Create htpasswd file for registry authentication\n    htpasswd -Bbc $HOME/htpasswd faasd $PASSWORD\n    sudo mkdir -p /var/lib/faasd/registry/auth\n    sudo mv $HOME/htpasswd /var/lib/faasd/registry/auth/htpasswd\n\n    # Create registry configuration\n    sudo tee /var/lib/faasd/registry/config.yml &gt; /dev/null &lt;&lt;EOF\nversion: 0.1\nlog:\n  accesslog:\n    disabled: true\n  level: warn\n  formatter: text\n\nstorage:\n  filesystem:\n    rootdirectory: /var/lib/registry\n\nauth:\n  htpasswd:\n    realm: basic-realm\n    path: /etc/registry/htpasswd\n\nhttp:\n  addr: 0.0.0.0:5000\n  relativeurls: false\n  draintimeout: 60s\nEOF\n\n    # Configure registry authentication for faas-cli\n    cat $HOME/registry-password.txt | faas-cli registry-login \\\n      --server http://registry:5000 \\\n      --username faasd \\\n      --password-stdin\n\n    # Setup Docker credentials for faasd-provider\n    sudo mkdir -p /var/lib/faasd/.docker\n    sudo cp ./credentials/config.json /var/lib/faasd/.docker/config.json\n\n    # Ensure pro-builder can access Docker credentials\n    sudo mkdir -p /var/lib/faasd/secrets\n    sudo cp ./credentials/config.json /var/lib/faasd/secrets/docker-config\n\n    # Configure local registry hostname resolution\n    echo \"127.0.0.1 registry\" | sudo tee -a /etc/hosts\n\n    echo \"==&gt; Adding registry services to docker-compose...\"\n\n    # Append additional services to docker-compose.yaml\n    sudo tee -a /var/lib/faasd/docker-compose.yaml &gt; /dev/null &lt;&lt;EOF\n\n  registry:\n    image: docker.io/library/registry:3\n    volumes:\n    - type: bind\n      source: ./registry/data\n      target: /var/lib/registry\n    - type: bind\n      source: ./registry/auth\n      target: /etc/registry/\n      read_only: true\n    - type: bind\n      source: ./registry/config.yml\n      target: /etc/docker/registry/config.yml\n      read_only: true\n    deploy:\n      replicas: 1\n    ports:\n      - \"5000:5000\"\nEOF\n\nfi\n\nif [ \"${INSTALL_BUILDER}\" = \"true\" ]; then\n    echo \"==&gt; Configuring function builder...\"\n\n    # Generate payload secret for function builder\n    openssl rand -base64 32 | sudo tee /var/lib/faasd/secrets/payload-secret\n\n    echo \"==&gt; Adding function builder services to docker-compose...\"\n\n    # Append additional services to docker-compose.yaml\n    sudo tee -a /var/lib/faasd/docker-compose.yaml &gt; /dev/null &lt;&lt;EOF\n\n  pro-builder:\n    depends_on: [buildkit]\n    user: \"app\"\n    group_add: [\"1000\"]\n    restart: always\n    image: ghcr.io/openfaasltd/pro-builder:0.5.3\n    environment:\n      buildkit-workspace: /tmp/\n      enable_lchown: false\n      insecure: true\n      buildkit_url: unix:///home/app/.local/run/buildkit/buildkitd.sock\n      disable_hmac: false\n      # max_inflight: 10 # Uncomment to limit concurrent builds\n    command:\n     - \"./pro-builder\"\n     - \"-license-file=/run/secrets/openfaas-license\"\n    volumes:\n      - type: bind\n        source: ./secrets/payload-secret\n        target: /var/openfaas/secrets/payload-secret\n      - type: bind\n        source: ./secrets/openfaas_license\n        target: /run/secrets/openfaas-license\n      - type: bind\n        source: ./secrets/docker-config\n        target: /home/app/.docker/config.json\n      - type: bind\n        source: ./buildkit-rootless-run\n        target: /home/app/.local/run\n      - type: bind\n        source: ./buildkit-sock\n        target: /home/app/.local/run/buildkit\n    deploy:\n      replicas: 1\n    ports:\n     - \"8088:8080\"\n\n  buildkit:\n    restart: always\n    image: docker.io/moby/buildkit:v0.23.2-rootless\n    group_add: [\"2000\"]\n    user: \"1000:1000\"\n    cap_add:\n      - CAP_SETUID\n      - CAP_SETGID\n    command:\n    - rootlesskit\n    - buildkitd\n    - \"--addr\"\n    - unix:///home/user/.local/share/bksock/buildkitd.sock\n    - --oci-worker-no-process-sandbox\n    security_opt:\n    - no-new-privileges=false\n    - seccomp=unconfined        # Required for mount(2) syscall\n    volumes:\n      # Runtime directory for rootlesskit/buildkit socket\n      - ./buildkit-rootless-run:/home/user/.local/run\n      - /sys/fs/cgroup:/sys/fs/cgroup\n      # Persistent state and cache directories\n      - ./buildkit-rootless-state:/home/user/.local/share/buildkit\n      - ./buildkit-sock:/home/user/.local/share/bksock\n    environment:\n      XDG_RUNTIME_DIR: /home/user/.local/run\n      TZ: \"UTC\"\n      BUILDKIT_DEBUG: \"1\"         # Enable for debugging\n      BUILDKIT_EXPERIMENTAL: \"1\"  # Enable experimental features\n    deploy:\n      replicas: 1\nEOF\n\nfi\n\n#==============================================================================\n# INSTALLATION EXECUTION\n#==============================================================================\n\necho \"==&gt; Installing faasd...\"\n\n# Execute the installation\nsudo /usr/local/bin/faasd install\n\n#==============================================================================\n# POST-INSTALLATION CONFIGURATION\n#==============================================================================\n\nif [ \"${INSTALL_BUILDER}\" = \"true\" ]; then\n    echo \"==&gt; Configuring insecure registry access...\"\n\n    # Configure faasd-provider to use insecure registry\n    sudo sed -i '/^ExecStart=/ s|$| --insecure-registry http://registry:5000|' \\\n        /lib/systemd/system/faasd-provider.service\n\n    # Reload systemd and restart faasd-provider\n    sudo systemctl daemon-reload\n    sudo systemctl restart faasd-provider\nfi\n\necho \"==&gt; OpenFaaS Edge installation completed successfully!\"\necho \"\"\necho \"1. Access the OpenFaaS gateway at http://localhost:8080\"\necho \"2. Get your admin password: sudo cat /var/lib/faasd/secrets/basic-auth-password\"\nif [ \"${INSTALL_REGISTRY}\" = \"true\" ]; then\n    echo \"3. Private registry available at http://localhost:5000\"\n    echo \"4. Registry password: cat $HOME/registry-password.txt\"\nfi\nif [ \"${INSTALL_BUILDER}\" = \"true\" ]; then\n    echo \"5. Pro-builder service available at http://localhost:8088\"\nfi\n</code></pre>"},{"location":"examples/openfaas-edge/#vm-config","title":"VM config","text":"<p>Use <code>slicer new</code> to generate a configuration file:</p> <p><pre><code>slicer new of-edge \\\n  --userdata-file ./openfaas-edge.sh \\\n  &gt; openfaas-edge.yaml\n</code></pre> The slicer config will be adapted from the walkthrough. When you create the YAML, name it <code>openfaas-edge.yaml</code>.</p> <p>Optionally use  the <code>--ssh-key</code> or <code>--github</code> flags to add an additional ssh key so you can connect via SSH to deploy functions, review the logs, and access check the status of the faasd services.</p> <p>The userdate script provided in this guide can be used with both Ubuntu and Rocky Linux. You can change user the <code>--image</code> flag to swith the image to your preferred OS. An overview of the available images can be found here.</p>"},{"location":"examples/openfaas-edge/#run-openfaas-edge","title":"Run OpenFaaS Edge","text":"<p>Start the VM:</p> <pre><code>sudo slicer up ./openfaas-edge.yaml\n</code></pre> <p>Login to the VM and activate faasd using a static license key are by running faasd activate.</p> <p>Users with a paid OpenFaaS Edge entitlement can create their license key as follows:</p> <pre><code>sudo nano /var/lib/faasd/secrets/openfaas_license\n</code></pre> <p>For personal use, GitHub Sponsors of @openfaas can run:</p> <pre><code>sudo faasd github login\nsudo faasd activate\n</code></pre> <p>Restart faasd and the faad-provider after adding the license key:</p> <pre><code>sudo systemctl restart faasd faasd-provider\n</code></pre>"},{"location":"examples/openfaas-edge/#build-and-deploy-a-function-within-the-vm","title":"Build and deploy a function within the VM","text":"<p>Login in to the VM over ssh to perform a test build on the VM directly.</p> <p>Once you connected to the VM authenticate the faas-cli for gateway access:</p> <pre><code>sudo cat \"/var/lib/faasd/secrets/basic-auth-password\" \\\n | faas-cli login --password-stdin\n</code></pre> <p>Scaffold a new function for testing:</p> <pre><code>faas-cli new --lang python3-http \\\n  --prefix registry:5000/functions \\\n  pytest\n</code></pre> <p>The <code>--prefix</code> flag is used to set prefix for the function image to our local registry.</p> <p>Build the function using the function builder API and deploy it:</p> <pre><code># Get the payload secret\nsudo cp /var/lib/faasd/secrets/payload-secret ./payload-secret\n\nfaas-cli up \\\n    --remote-builder http://127.0.0.1:8088 \\\n    --payload-secret ./payload-secret \\\n    --tag=digest\n</code></pre>"},{"location":"examples/openfaas-edge/#access-openfaas-edge-from-your-own-workstation","title":"Access OpenFaaS Edge from your own workstation","text":"<p>Get the basic auth password from the VM and login with the faas-cli:</p> <pre><code>export OPENFAAS_URL=http://192.168.137.2:8080\nssh ubuntu@192.168.137.2 \"sudo cat /var/lib/faasd/secrets/basic-auth-password\" \\\n | faas-cli login --password-stdin\n</code></pre> <p>Note that the <code>OPENFAAS_URL</code> is configured to use the VM's IP address.</p> <p>Deploy a function form the function store:</p> <pre><code>faas-cli store deploy nodeinfo\n</code></pre> <p>Invoke the nodeinfo function:</p> <pre><code>curl -s http://192.168.137.2:8080/function/nodeinfo\n</code></pre> <p>If you have the function builder installed you can build and deploy a function using the function builder API:</p> <p>Scaffold a new function for testing:</p> <pre><code>faas-cli new --lang python3-http \\\n  --prefix registry:5000/functions \\\n  pytest\n</code></pre> <p>The <code>--prefix</code> flag is used to set the prefix for the function image to our local registry deployed on the VM.</p> <pre><code># Get the payload secret from the VM\nssh ubuntu@192.168.137.2 \"sudo cat /var/lib/faasd/secrets/payload-secret\" &gt; ./payload-secret\n\nfaas-cli up \\\n  --remote-builder http://192.168.137.2:8088 \\\n  --payload-secret ./payload-secret \\\n  --tag=digest\n</code></pre> <p>The <code>--remote-builder</code> flag points to the Function Builder API exposed on the VM.</p> <p>Test the new function:</p> <pre><code>curl -s http://192.168.137.2:8080/function/pytest\n</code></pre>"},{"location":"examples/openfaas-edge/#access-the-registry","title":"Access the registry","text":"<p>The be able to push and pull function from the image registry directly you need to add the VM IP address to your hosts file and login with docker.</p> <p>Add a new entry to your hosts file that points to the registry:</p> <pre><code>echo \"192.168.137.2 registry\" | sudo tee -a /etc/hosts\n</code></pre> <p>The registry has to be added as an insecure registry in the docker daemon configuration file, <code>/etc/docker/daemon.json</code>:</p> <pre><code>{\n  \"insecure-registries\": [ \"registry:5000\" ]\n}\n</code></pre> <p>Get get the registry password from the VM and login with docker:</p> <pre><code>ssh ubuntu@192.168.137.2 \"sudo cat ./registry-password.txt\" &gt; ./registry-password.txt\ncat registry-password.txt | docker login \\\n  --username faasd \\\n  --password-stdin \\\n  http://registry:5000\n</code></pre> <p>You should now be able to push and pull images from the registry.</p> <p>You can try this by building a function on your workstation and pushing it to the registry. M</p> <pre><code>faas-cli new --lang node22 \\\n  --prefix registry:5000/functions \\\n  --append stack.yaml \\\n  nodetest\n\nfaas-cli up\n</code></pre> <p>Test the new function:</p> <pre><code>curl -s http://192.168.137.2:8080/function/nodetest -d \"Hello World\"\n</code></pre>"},{"location":"examples/openfaas/","title":"OpenFaaS Pro and CE in Slicer","text":"<p>Boot up a pre-configured microVM with OpenFaaS Pro or CE setup along with <code>faas-cli</code>, <code>helm</code>, and <code>stern</code> for log tailing.</p> <p>For either edition, the URL for the gateway is set up for you for in the default user's <code>.bashrc</code> file:</p> <pre><code>export OPENFAAS_URL=http://127.0.0.1:31112\n</code></pre> <p>You can use that address directly on the VM during an SSH session, but you'll also be able to access the gateway from other machines by using the VM's IP i.e. <code>http://192.168.137.2:31112</code>.</p> <p>Get instructions to fetch the password via <code>arkade info openfaas</code>.</p> <p>A multi-node setup is possible, but in that case, it's better to set up K3s using K3sup Pro from outside the VMs, and to follow the HA K3s example followed by the OpenFaaS for Kubernetes instructions.</p>"},{"location":"examples/openfaas/#openfaas-pro","title":"OpenFaaS Pro","text":"<p>Create a slicer secret for your OpenFaaS Pro license key. This let's you securely pass the license to the slicer VM.</p> <p>Any secrets the VM or user data script need access to should be added to the <code>.secrets</code> folder.</p> <pre><code>sudo mkdir .secrets\n# Ensure only root can read/write to the secrets folder.\nsudo chmod 700 .secrets\n\nsudo -E cp ~/.openfaas/LICENSE .secrets/openfaas-license\n</code></pre> <p>Create a user data file, <code>openfaas-pro.sh</code>, to setup an OpenFaaS Pro cluster.</p> <pre><code>#!/bin/bash\n\nexport HOME=/home/ubuntu\nexport USER=ubuntu\n\ncd /home/ubuntu/\n\n(\narkade get kubectl kubectx helm faas-cli k3sup stern --path /usr/local/bin\nchown $USER /usr/local/bin/*\n\nmkdir -p .kube\n)\n\n(\nk3sup install --local\nmv ./kubeconfig ./.kube/config\nchown $USER .kube/config\n)\n\n(\nkubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml\n\nkubectl create secret generic \\\n  -n openfaas \\\n  openfaas-license \\\n  --from-file license=/run/slicer/secrets/openfaas-license\n\nhelm repo add openfaas https://openfaas.github.io/faas-netes/\nhelm repo update &amp;&amp; \\\n  helm upgrade --install openfaas \\\n  --install openfaas/openfaas \\\n  --namespace openfaas \\\n  -f https://raw.githubusercontent.com/openfaas/faas-netes/refs/heads/master/chart/openfaas/values-pro.yaml\n\nchown -R $USER $HOME\n\necho \"export OPENFAAS_URL=http://127.0.0.1:31112\" &gt;&gt; $HOME/.bashrc\n\n)\n</code></pre> <p>Use <code>slicer new</code> to generate a configuration file:</p> <pre><code>slicer new openfaas-pro \\\n  --userdata-file openfaas-pro.sh \\\n  &gt;  openfaas-pro.yaml\n</code></pre> <p>For SSH access, use the <code>--github</code> flag to specify a GitHub username to import SSH keys from or set the <code>--ssh-key</code> flag to add publis SSH keys to the VM . Learn more about SSH in Slicer.</p> <p>Then start up slicer with the generated config:</p> <pre><code>sudo slicer up -f openfaas-pro.yaml\n</code></pre> <p>Connect to the slicer VM over SSH:</p> <pre><code>ssh ubuntu@192.168.137.2\n</code></pre> <p>Run the follwing commands to deploy and invoke a function.</p> <p>Login to OpenFaaS:</p> <pre><code>PASSWORD=$(kubectl get secret -n openfaas basic-auth -o jsonpath=\"{.data.basic-auth-password}\" | base64 --decode; echo)\necho -n $PASSWORD | faas-cli login --username admin --password-stdin\n</code></pre> <p>Deploy a function:</p> <pre><code>faas-cli store deploy figlet\n</code></pre> <p>Describe, then invoke the function:</p> <pre><code>faas-cli describe figlet\nfaas-cli invoke &lt;&lt;&lt; \"SlicerVM.com\"\n</code></pre>"},{"location":"examples/openfaas/#openfaas-ce","title":"OpenFaaS CE","text":"<p>OpenFaaS CE is licensed for personal, non-commercial use or a single 60 day commercial trial per company.</p> <p>The above userdata script can be re-used, with a few options removed. Create the <code>openfaas-ce.sh</code> userdata script:</p> <pre><code>export HOME=/home/ubuntu\nexport USER=ubuntu\n\ncd /home/ubuntu/\n\n(\narkade get kubectl kubectx helm faas-cli k3sup stern --path /usr/local/bin\nchown $USER /usr/local/bin/*\n\nmkdir -p .kube\nmkdir -p .openfaas\n)\n\n(\nk3sup install --local\nmv ./kubeconfig ./.kube/config\nchown $USER .kube/config\n)\n\n(\nkubectl apply -f https://raw.githubusercontent.com/openfaas/faas-netes/master/namespaces.yml\n\nhelm repo add openfaas https://openfaas.github.io/faas-netes/\nhelm repo update &amp;&amp; \\\n  helm upgrade --install openfaas \\\n  --install openfaas/openfaas \\\n  --namespace openfaas \\\n\nchown -R $USER $HOME\n\necho \"export OPENFAAS_URL=http://127.0.0.1:31112\" &gt;&gt; $HOME/.bashrc\n\n)\n</code></pre> <p>Create <code>openfaas-ce.yaml</code> clicer configuration file:</p> <pre><code>slicer new openfaas-ce \\\n  --userdata-file openfaas-ce.sh \\\n  &gt;  openfaas-ce.yaml\n</code></pre>"},{"location":"examples/pihole-adblock/","title":"PiHole Adblocker Example with Slicer","text":"<p>PiHole is a common adblocker for home networks. This example shows how to run PiHole in a Slicer VM.</p> <p>You can use a regular PC like an N100 or a Raspberry Pi.</p>"},{"location":"examples/pihole-adblock/#userdata","title":"Userdata","text":"<p>The automated installation of PiHole uses a custom userdata script.</p> <p>Save this as <code>setup-pihole.sh</code>:</p> <pre><code>#!/usr/bin/env bash\nset -euxo pipefail\n\n# ---------- Tunables ----------\nUPSTREAMS=(\"1.1.1.1\" \"9.9.9.9\")   # change if you like\nENABLE_DNSSEC=\"true\"              # \"true\" or \"false\"\n# -------------------------------\n\nretry() { i=0; while true; do if \"$@\"; then return 0; fi; i=$((i+1)); [ \"$i\" -ge 30 ] &amp;&amp; return 1; sleep 2; done; }\nwait_default_route() { for _ in $(seq 1 60); do /usr/sbin/ip -4 route show default | /usr/bin/grep -q . &amp;&amp; return 0; sleep 1; done; return 1; }\n\n# 1) Network ready before any fetches\nwait_default_route\n\n# 2) Bring up systemd-resolved (no stub on :53 to avoid conflict)\n#    and point resolv.conf at resolved's generated file\n/usr/bin/mkdir -p /etc/systemd || :\n/usr/bin/tee /etc/systemd/resolved.conf &gt;/dev/null &lt;&lt;EOF\n[Resolve]\nDNS=${UPSTREAMS[*]}\nFallbackDNS=1.0.0.1 149.112.112.112\nDNSStubListener=no\nEOF\n/bin/systemctl unmask systemd-resolved || :\n/bin/systemctl enable --now systemd-resolved || :\nif [ -f /run/systemd/resolve/resolv.conf ]; then\n  chattr -i /etc/resolv.conf || :\n  chattr -i /etc/systemd/resolved.conf || :\n\n  /bin/ln -sf /run/systemd/resolve/resolv.conf /etc/resolv.conf\nelse\n  # fallback if resolved isn't providing it for some reason\n  /usr/bin/tee /etc/resolv.conf &gt;/dev/null &lt;&lt;EOF\n$(for s in \"${UPSTREAMS[@]}\"; do echo \"nameserver $s\"; done)\noptions timeout:2 attempts:2\nEOF\nfi\n\n# 3) Speed up apt (prefer IPv4)\n#    NOTE: do this BEFORE update to avoid AAAA timeouts\n/usr/bin/tee /etc/apt/apt.conf.d/99force-ipv4 &gt;/dev/null &lt;&lt;'EOF'\nAcquire::ForceIPv4 \"true\";\nEOF\n\n# 4) Base tools\nretry /usr/bin/apt-get update\nDEBIAN_FRONTEND=noninteractive retry /usr/bin/apt-get install -y \\\n  curl ca-certificates iproute2 procps e2fsprogs openssl dnsutils || :\n\n# 5) Detect primary interface + IP/CIDR\nIFACE=\"$(/usr/sbin/ip -o -4 route show to default | /usr/bin/awk '{print $5}' | /usr/bin/head -n1)\"\nIP_CIDR=\"$(/usr/sbin/ip -o -4 addr show dev \"${IFACE}\" | /usr/bin/awk '{print $4}' | /usr/bin/head -n1)\"\nVM_IP=\"$(echo \"$IP_CIDR\" | /usr/bin/cut -d/ -f1)\"\n\n# 6) Preseed Pi-hole v6 config (TOML + setupVars)\n/usr/bin/mkdir -p /etc/pihole\n# pihole.toml\n/usr/bin/tee /etc/pihole/pihole.toml &gt;/dev/null &lt;&lt;EOF\n[dns]\ninterface = \"0.0.0.0\"\nupstreams = [$(printf '\"%s\",' \"${UPSTREAMS[@]}\" | sed 's/,$//')]\ndnssec = ${ENABLE_DNSSEC}\nEOF\n\n# setupVars.conf (some installers still consult it)\n/usr/bin/tee /etc/pihole/setupVars.conf &gt;/dev/null &lt;&lt;EOF\nPIHOLE_INTERFACE=${IFACE}\nIPV4_ADDRESS=${IP_CIDR}\nIPV6_ADDRESS=\nDNSMASQ_LISTENING=single\nPIHOLE_DNS_1=${UPSTREAMS[0]}\nPIHOLE_DNS_2=${UPSTREAMS[1]:-${UPSTREAMS[0]}}\nDNSSEC=${ENABLE_DNSSEC}\nQUERY_LOGGING=true\nINSTALL_WEB_SERVER=true\nINSTALL_WEB_INTERFACE=true\nLIGHTTPD_ENABLED=true\nBLOCKING_ENABLED=true\nWEBPASSWORD=\nEOF\n\n# 7) Unattended install\nretry /usr/bin/curl -fsSL https://install.pi-hole.net -o /tmp/pihole-install.sh\nPIHOLE_SKIP_OS_CHECK=true /bin/bash /tmp/pihole-install.sh --unattended || :\n\n# 8) CLI path + admin password\nPIHOLE_BIN=\"/usr/local/bin/pihole\"; [ -x \"$PIHOLE_BIN\" ] || PIHOLE_BIN=\"/usr/bin/pihole\"\n\"${PIHOLE_BIN}\" setpassword  \"$(cat \"/run/slicer/secrets/pihole-admin-pass\")\" || :\n\"${PIHOLE_BIN}\" -g || :\n\n# 9) Ensure service enabled &amp; started\n/bin/systemctl enable pihole-FTL || :\n/bin/systemctl restart pihole-FTL || :\n\n# 10) Output &amp; quick hints\n/usr/bin/printf \"Pi-hole admin: http://%s/admin\\n\" \"${VM_IP}\"\necho \"Test locally: dig +short openfaas.com @127.0.0.1 || true\"\n</code></pre> <p>Generate an admin password for PiHole and save it in the <code>.secrets</code>. Slicer will copy the secret into the VM, see: slicer secrets for more information.</p> <pre><code>sudo mkdir .secrets\n# Ensure only root can read/write to the secrets folder.\nsudo chmod 700 .secrets\n\nsudo /usr/bin/openssl rand -base64 18 &gt; .secrets/pihole-admin-pass\n</code></pre> <p>The secret will be available on the VM at <code>/run/slicer/secrets/pihole-admin-pass</code></p>"},{"location":"examples/pihole-adblock/#vm-configuration-file","title":"VM configuration file","text":"<p>Next, set up a modest VM configuration file with <code>slicer new</code>:</p> <pre><code>slicer new pihole \\\n  --userdata-file setup-pihole.sh \\\n  &gt; pihole.yaml\n</code></pre> <p>Start the VM with the following command:</p> <pre><code>sudo slicer up ./pihole.yaml\n</code></pre>"},{"location":"examples/pihole-adblock/#accessing-pihole","title":"Accessing PiHole","text":"<p>Watch the logs from the serial console to see when the install is complete.</p> <pre><code>arkade get fstail\nsudo fstail /var/log/slicer/\n</code></pre> <p>Next, the PiHole admin interface will be available on port 80 of the VM's IP address.</p> <p>This is typically <code>http://192.168.137.2/admin</code> and the admin password we generated can be found at <code>.secrets/pihole-admin-pass</code>.</p> <p>Example DNS query:</p> <p>Add nslookup and dig to your Slicer system if not already present</p> <pre><code>sudo apt update &amp;&amp; \\\n    sudo apt install -qy dnsutils\n</code></pre> <p>Perform a lookup using PiHole as the DNS server:</p> <pre><code># Should work fine\nnslookup openfaas.com 192.168.137.2\n\n# Should be blocked\nnslookup doubleclick.net 192.168.137.2\n</code></pre> <p>To use PiHole from another machine, you have a couple of options:</p> <ol> <li>Log into your router and add a static route for the VM's IP address via the slicer host. Then set the DNS server under the router's DHCP settings to the PiHole VM's IP address.</li> <li>Enable DNAT via iptables from the Slicer host to the PiHole VM, then go into your home router and set the DNS server under the router's DHCP settings to the Slicer host's IP address.</li> </ol> <p>Example DNAT rule for iptables:</p> <pre><code># First, clear any existing DNS DNAT rules to avoid conflicts\n# Optional step..\n# sudo iptables -t nat -F PREROUTING\n\n# Set your PiHole VM IP and detect the network interface\nexport PIHOLE_IP=192.168.136.2  # Update this to match your actual PiHole VM IP\nexport SLICER_IFACE=$(ip -o -4 route show to default | awk '{print $5}' | head -n1)\n\necho \"Slicer host interface: $SLICER_IFACE\"\necho \"PiHole VM IP: $PIHOLE_IP\"\n\n# Add DNAT rules for DNS traffic coming from the external interface\nsudo iptables -t nat -A PREROUTING -i $SLICER_IFACE -p udp --dport 53 -j DNAT --to-destination $PIHOLE_IP:53\nsudo iptables -t nat -A PREROUTING -i $SLICER_IFACE -p tcp --dport 53 -j DNAT --to-destination $PIHOLE_IP:53\n\n# Add MASQUERADE rule so return traffic works properly\nsudo iptables -t nat -A POSTROUTING -j MASQUERADE\n</code></pre> <p>If you'd also like a rule for the admin dashboard, so it's also accessible from the host IP:</p> <pre><code># Set your PiHole VM IP and detect the network interface\nexport PIHOLE_IP=192.168.136.2  # Update this to match your actual PiHole VM IP\nexport SLICER_IFACE=$(ip -o -4 route show to default | awk '{print $5}' | head -n1)\n\nsudo iptables -t nat -A PREROUTING -i $SLICER_IFACE -p tcp --dport 80 -j DNAT --to-destination $PIHOLE_IP:80\n</code></pre>"},{"location":"examples/remote-vscode/","title":"Remote VSCode","text":"<p>Let's say that you work away from your computer often, or maybe just want to have a remote IDE that you can access from any kind of device like an iPad, or a Chromebook.</p> <p>You can run Microsoft's VSCode server in a VM using Slicer and connect to it remotely. Microsoft also offer free tunnels to connect to your server with authentication enabled via GitHub or Microsoft accounts.</p> <p>It's incredibly convenient.</p> <p>Alex recently migrated his blog from Ghost 1.x which had a built-in markdown editor, over to a static Next.js site. This example lets him edit the content of his blog without having a laptop with him. This whole setup could be run on a Raspberry Pi 5, or a cheap N100 mini computer.</p> <p></p> <p>VSCode tunneled to the web with built-in authentication and Copilot helping out</p>"},{"location":"examples/remote-vscode/#set-up-your-initial-vm","title":"Set up your initial VM","text":"<p>Use <code>slicer new</code> to get a basic VM config with the specs you want, with your SSH keys pre-installed, and use a disk image as backing to make it persistent.</p> <p>When you create your config, you can add userdata to install the VSCode server, so it's already there by the time you get in.</p> <p>The below userdata script is based upon the Official VSCode Documentation, save it as <code>vscode.sh</code>.</p> <pre><code>#!/bin/bash\n\n(\nsudo apt-get install -qy curl gpg apt-transport-https\n\ncurl -sLS -o - https://packages.microsoft.com/keys/microsoft.asc | gpg --dearmor &gt; microsoft.gpg\nsudo install -D -o root -g root -m 644 microsoft.gpg /usr/share/keyrings/microsoft.gpg\n\ncat &gt; /etc/apt/sources.list.d/vscode.sources &lt;&lt; EOF\nTypes: deb\nURIs: https://packages.microsoft.com/repos/code\nSuites: stable\nComponents: main\nArchitectures: amd64,arm64,armhf\nSigned-By: /usr/share/keyrings/microsoft.gpg\nEOF\n\nsudo apt install -qy apt-transport-https\nsudo apt update\nsudo apt install -qy code --no-install-recommends\n)\n</code></pre> <p>Generate a slicer configuration file:</p> <pre><code>slicer new vscode \\\n  --userdata-file vscode.sh \\\n  &gt; vscode.yaml\n</code></pre> <p>Use the <code>--ssh-key</code> or <code>--github</code> flags to add your ssh keys so you can connect to the VSCode remote instance over SSH. </p> <p>Start the VM with the following command:</p> <pre><code>sudo slicer up ./vscode.yaml\n</code></pre> <p>You can watch the installation by adding the fstail tool via <code>arkade get fstail</code>.</p> <p>Then run the following to attach to any VM log files that are detected:</p> <pre><code>sudo fstail /var/log/slicer\n</code></pre> <p>It seemed to take about 16s to get all of the above installed and configured.</p>"},{"location":"examples/remote-vscode/#start-the-vscode-server","title":"Start the VSCode server","text":"<p>Log in via ssh i.e. <code>ssh ubuntu@192.168.137.2</code></p> <p>To use Microsoft's built-in tunneling service, you need to accept their license terms:</p> <pre><code>code tunnel --accept-server-license-terms \\\n    --name $(hostname)\n</code></pre> <p>You'll be presented with a URL to authenticate with GitHub using a device code.</p> <p>Then you'll get a URL for your remote VSCode server.</p>"},{"location":"examples/remote-vscode/#alternative-without-microsofts-tunneling-service","title":"Alternative without Microsoft's tunneling service","text":"<p>Alternatively, you can run the server without tunneling, and connect directly to it over HTTPS.</p> <pre><code>code serve-web\n</code></pre> <p>A link will be printed out pointing to <code>http://localhost:8080</code> including a token used to authenticate.</p> <p>Then port-forward the port 8000 to your local machine:</p> <pre><code>ssh -L 8000:127.0.0.1:8000 ubuntu@192.168.137.2\n</code></pre> <p>Or launch an inlets HTTPS tunnel via inletsctl to get a public URL with TLS enabled. You can enable auth using basic auth or OAuth2 with the inlets command.</p>"},{"location":"examples/remote-vscode/#customise-the-environment","title":"Customise the environment","text":"<p>Install any tooling you want such as Node, or Go, Python, Docker etc.</p> <p>If you use Copilot, log into your Microsoft account, and then you can enable the Copilot extension in your remote VSCode server.</p>"},{"location":"examples/remote-vscode/#another-approach-vscode-with-ssh","title":"Another approach: VScode with SSH","text":"<p>Instead of launching a browser-based VSCode server, you can use your local VSCode installation to connect to the microVM over SSH.</p> <ul> <li>Any agents you run can work in YOLO mode, without risking your content, confidential data, or any risk of breaking your main host.</li> <li>You can create, delete, and iterate on microVMs easily, and if you break something, just delete it and start again.</li> <li>You can get access to a full Linux host, whilst working on MacOS or Windows.</li> <li>You can test programs out on an Arm host, even if your main host is x86_64.</li> </ul> <p>You can also use VSCode's built-in SSH support to connect to your VM. This allows you to use your local VSCode installation to edit files on the remote machine, with all your familiar settings already available.</p> <ol> <li>Install the Remote - SSH extension in your local VSCode.</li> <li>Configure your SSH settings to connect to your VM.</li> <li>Open a remote window in VSCode and start editing files on your VM.</li> </ol> <p>This approach gives you the full power of your local VSCode environment whilst making sure any packages you install, or changes you make, do not affect your main host.</p>"},{"location":"examples/router-firewall/","title":"Create a Linux Router/Firewall with Slicer","text":"<p>In this example, we'll turn a regular PC into a router/firewall with standard Linux networking daemons such as dnsmasq and iptables.</p> <p>Why use a regular Linux VM over a product like pfSense, or OPNsense?</p> <p>Whether it's based upon FreeBSD or Linux, these products are often extremely bloated, often closed source, and require a lot of resources to run. It's hard to know where to start when you need to customise these products to your own needs, and they often bundle far more than you need for a router/firewall.</p> <p>Instead, we'll use a microVM that's easy to create from scratch, and can be customised as much as you need. You can then add in additional daemons or services as required like a VPN uplink, Inlets tunnels, or something like PiHole for ad blocking.</p> <p>Most importantly, you'll be in control, you'll know exactly what is and what is not running in your appliance, and how to troubleshoot it or customise it - an LLM agent can help you with that if you're not used to Linux networking.</p> <p>And why Slicer?</p> <p>Well instead of having to flash an ISO directly to your main drive, you can run as many microVMs as you want, each performing a different task or role. One common complaint with off the shelf router/firewall products is their poor support for Linux containers. With Slicer, you can simply run an extra microVM, you're not locked into one OS or product for the whole machine.</p>"},{"location":"examples/router-firewall/#network-topology","title":"Network Topology","text":"<pre><code>                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502           Slicer Host           \u2502\n                    \u2502                                 \u2502\n                    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n                    \u2502  \u2502  microVM Router/Firewall \u2502   \u2502\n                    \u2502  \u2502                          \u2502   \u2502\n                    \u2502  \u2502  eth0: 192.168.130.2/24  \u2502   \u2502\n                    \u2502  \u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500  \u2502   \u2502\n                    \u2502  \u2502                          \u2502   \u2502\n                    \u2502  \u2502  ens7: 10.88.0.1/24      \u2502   \u2502\n                    \u2502  \u2502  (PCI Passthrough/VFIO)  \u2502   \u2502\n                    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n                    \u2502           \u2502                     \u2502\n                    \u2502           \u2502 PCI Passthrough     \u2502\n                    \u2502           \u2502 (VFIO)              \u2502\n                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                \u2502\n                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                    \u2502                       \u2502\n                    \u2502                       \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502       LAN1           \u2502 \u2502       LAN2       \u2502\n         \u2502  (Main Network)      \u2502 \u2502  (Isolated)      \u2502\n         \u2502  192.168.130.0/24    \u2502 \u2502  10.88.0.0/24    \u2502\n         \u2502                      \u2502 \u2502                  \u2502\n         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502 \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n         \u2502  \u2502  Other Devices \u2502  \u2502 \u2502  \u2502 Raspberry Pi\u2502 \u2502\n         \u2502  \u2502  (LAN1)        \u2502  \u2502 \u2502  \u2502 (LAN2)      \u2502 \u2502\n         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502 \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n         \u2502                      \u2502 \u2502                  \u2502\n         \u2502  Internet Gateway    \u2502 \u2502  DHCP/DNS        \u2502\n         \u2502  Router              \u2502 \u2502  via dnsmasq     \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The microVM router has two network interfaces:</p> <ul> <li>eth0: Connected to LAN1 (192.168.130.0/24) via bridge networking</li> <li>ens7: Connected to LAN2 (10.88.0.0/24) via PCI passthrough (VFIO)</li> </ul> <p>All traffic from LAN2 must pass through the microVM router to reach LAN1 or the Internet, providing physical Layer 1 separation between the networks.</p> <p></p> <p>N100 mini PC routing/firewalling a separate Internal network</p> <p>You will have physical L1 (OSI) separation between the main LAN1 and a separate LAN2.</p> <p>The LAN1 in this case will be your main network, which is plugged directly into the Slicer host.</p> <p>The LAN2 will be a separate network plugged into its own switch, WiFi access point, or an Ethernet port on another device.</p> <p>For this example, we'll use the microVM to create an isolated physical network for a Raspberry Pi, with a separate IP range than the main network LAN1.</p>"},{"location":"examples/router-firewall/#prerequisites","title":"Prerequisites","text":"<p>Whilst I'm using a Raspberry Pi for the LAN2 network, you can use any device that supports standard Linux networking daemons.</p>"},{"location":"examples/router-firewall/#bind-a-pci-network-adapter-to-vfio","title":"Bind a PCI network adapter to VFIO","text":"<p>You'll need to follow the instructions on the PCI Passthrough page to bind a PCI network adapter to VFIO.</p> <p>You'll be able to identify which network adapters are available, and are part of their own IOMMU group.</p> <p>In the case of the N100 mini PC being used here, the PCI address is <code>0000:04:00.0</code>, which can also be written in short form as <code>04:00.0</code>.</p> <p>Take note of this address and use it in the config file in the next step. </p>"},{"location":"examples/router-firewall/#set-up-the-microvm","title":"Set up the microVM","text":"<p>Create a new Slicer config file, and add in the PCI address of the network adapter you bound to VFIO:</p> <pre><code>config:\n  pci:\n    router-1: [\"0000:04:00.0\"]\n\n  host_groups:\n    - name: router\n      userdata_file: ./userdata.sh\n      storage: image\n      storage_size: 25G\n      count: 1\n      vcpu: 2\n      ram_gb: 4\n      network:\n        bridge: brrouter0\n        tap_prefix: router\n        gateway: 192.168.130.1/24\n  image: \"ghcr.io/openfaasltd/slicer-systemd-ch:x86_64-latest\"\n  hypervisor: cloud-hypervisor\n\n  api:\n    port: 8080\n    bind_address: \"127.0.0.1\"\n    auth:\n      enabled: true\n  ssh:\n    port: 0\n</code></pre> <p>This configures Slicer to create a single microVM with 2 vCPUs and 4GB of RAM.</p> <p>We now need to find out the stable name of the network adapter when it's passed through via VFIO into the microVM.</p> <p>Create a userdata file for the initial boot which will log the network adapters to the serial console log file:</p> <pre><code>cat &gt;&gt; userdata.sh &lt;&lt; EOF\n#!/bin/bash\n\nip link show\n\nlspci\n\nEOF\n</code></pre> <p>Then, boot up the microVM, then check the logs for the stable name of the network adapter:</p> <pre><code>sudo slicer up ./router.yaml\n</code></pre> <p>Once booted, review the log file for the microVM, the userdata script will output to the console:</p> <pre><code>cat /var/log/slicer/router-1.txt\n</code></pre> <p>The following output came from the N100 being used in this example:</p> <pre><code>1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n       valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host \n       valid_lft forever preferred_lft forever\n2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000\n    link/ether 2e:31:2a:ff:7d:45 brd ff:ff:ff:ff:ff:ff\n    altname enp0s4\n    altname ens4\n    inet 192.168.130.2/24 brd 192.168.130.255 scope global eth0\n       valid_lft forever preferred_lft forever\n    inet6 fe80::2c31:2aff:feff:7d45/64 scope link \n       valid_lft forever preferred_lft forever\n3: ens7: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc mq state UP group default qlen 1000\n    link/ether 60:be:b4:1e:19:63 brd ff:ff:ff:ff:ff:ff\n    altname enp0s7\n    inet 10.88.0.1/24 scope global ens7\n       valid_lft forever preferred_lft forever\n    inet6 fe80::62be:b4ff:fe1e:1963/64 scope link \n       valid_lft forever preferred_lft forever\n</code></pre> <p><code>lo</code> and <code>eth0</code> are standard and available for all microVMs, so look a different one i.e. <code>ens7</code> in this case.</p> <p>Hit Control-C to stop the microVM, then run <code>sudo rm -rf *.img</code> to remove the storage image.</p> <p>Create a userdata script to configure the router daemons.</p> <p>Edit <code>ens7</code> and <code>enp0s7</code> to match the values show in the output of the <code>ip addr show</code> command run above.</p> <p>userdata.sh</p> <pre><code>#!/usr/bin/env bash\nset -euo pipefail\nlog(){ echo \"[router-setup] $*\"; }\n\nWAN_IF=\"eth0\"\nLAN_IF=\"ens7\"\n\nLAN_IP=\"10.88.0.1\"\nLAN_CIDR=\"${LAN_IP}/24\"\nLAN_NET=\"10.88.0.0/24\"\n\nDHCP_START=\"10.88.0.50\"\nDHCP_END=\"10.88.0.200\"\n\nUP_DNS1=\"1.1.1.1\"\nUP_DNS2=\"8.8.8.8\"\n\nexport DEBIAN_FRONTEND=noninteractive\n\nif ! ip link show \"${LAN_IF}\" &gt;/dev/null 2&gt;&amp;1; then\n  log \"ERROR: ${LAN_IF} not found. Run: ip -br link\"\n  ip -br link || true\n  exit 1\nfi\n\nlog \"Installing packages\"\napt-get update -y\necho iptables-persistent iptables-persistent/autosave_v4 boolean true | debconf-set-selections || true\necho iptables-persistent iptables-persistent/autosave_v6 boolean true | debconf-set-selections || true\napt-get install -y dnsmasq iptables iptables-persistent nmap\n\nlog \"Configuring systemd-networkd for ${LAN_IF}=${LAN_CIDR} (eth0 untouched)\"\nmkdir -p /etc/systemd/network\ncat &gt;/etc/systemd/network/10-lan-${LAN_IF}.network &lt;&lt;EOF\n[Match]\nName=${LAN_IF}\n\n[Link]\nRequiredForOnline=no\n\n[Network]\nAddress=${LAN_CIDR}\nConfigureWithoutCarrier=yes\nIPv6AcceptRA=no\nEOF\n\nsystemctl enable systemd-networkd\nsystemctl restart systemd-networkd\nnetworkctl reconfigure \"${LAN_IF}\" || true\nip link set \"${LAN_IF}\" up || true\n\nlog \"Enable IPv4 forwarding (set only what we need; avoid sysctl --system noise)\"\ncat &gt;/etc/sysctl.d/99-router.conf &lt;&lt;EOF\nnet.ipv4.ip_forward=1\nEOF\nsysctl -w net.ipv4.ip_forward=1 &gt;/dev/null || true\n\nlog \"Configuring dnsmasq (LAN only, resilient bind)\"\nmkdir -p /etc/dnsmasq.d\ncat &gt;/etc/dnsmasq.d/pi-lan.conf &lt;&lt;EOF\n# Only serve on LAN\ninterface=${LAN_IF}\nexcept-interface=${WAN_IF}\nexcept-interface=lo\n\n# Resilient bind: don't fail if IP isn't present yet; rebind when it appears\nbind-dynamic\n\n# DHCP scope\ndhcp-range=${DHCP_START},${DHCP_END},255.255.255.0,12h\ndhcp-option=option:router,${LAN_IP}\ndhcp-option=option:dns-server,${LAN_IP}\n\n# DNS forwarding upstream\nno-resolv\nserver=${UP_DNS1}\nserver=${UP_DNS2}\n\n# Debug\nlog-dhcp\nlog-queries\nEOF\n\n# Don't rely on apt auto-start (policy-rc.d may block it)\nsystemctl enable dnsmasq || true\n\nlog \"Writing router defaults\"\ncat &gt;/etc/default/router-apply &lt;&lt;EOF\nWAN_IF=${WAN_IF}\nLAN_IF=${LAN_IF}\nLAN_NET=${LAN_NET}\nEOF\nchmod 0644 /etc/default/router-apply\n\nlog \"Writing router runtime apply script (runs every boot)\"\ncat &gt;/usr/local/sbin/router-apply.sh &lt;&lt;'EOF'\n#!/usr/bin/env bash\nset -euo pipefail\nlog(){ echo \"[router-apply] $*\"; }\n\n# shellcheck disable=SC1091\nsource /etc/default/router-apply\n\n: \"${WAN_IF:?missing WAN_IF}\"\n: \"${LAN_IF:?missing LAN_IF}\"\n: \"${LAN_NET:?missing LAN_NET}\"\n\nip link set \"${LAN_IF}\" up || true\n\niptables -C INPUT -i \"${LAN_IF}\" -p udp --dport 67 -j ACCEPT 2&gt;/dev/null || \\\n  iptables -A INPUT -i \"${LAN_IF}\" -p udp --dport 67 -j ACCEPT\niptables -C INPUT -i \"${LAN_IF}\" -p udp --dport 53 -j ACCEPT 2&gt;/dev/null || \\\n  iptables -A INPUT -i \"${LAN_IF}\" -p udp --dport 53 -j ACCEPT\niptables -C INPUT -i \"${LAN_IF}\" -p tcp --dport 53 -j ACCEPT 2&gt;/dev/null || \\\n  iptables -A INPUT -i \"${LAN_IF}\" -p tcp --dport 53 -j ACCEPT\n\niptables -C FORWARD -i \"${LAN_IF}\" -o \"${WAN_IF}\" -s \"${LAN_NET}\" -j ACCEPT 2&gt;/dev/null || \\\n  iptables -A FORWARD -i \"${LAN_IF}\" -o \"${WAN_IF}\" -s \"${LAN_NET}\" -j ACCEPT\niptables -C FORWARD -i \"${WAN_IF}\" -o \"${LAN_IF}\" -d \"${LAN_NET}\" -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT 2&gt;/dev/null || \\\n  iptables -A FORWARD -i \"${WAN_IF}\" -o \"${LAN_IF}\" -d \"${LAN_NET}\" -m conntrack --ctstate ESTABLISHED,RELATED -j ACCEPT\n\niptables -t nat -C POSTROUTING -o \"${WAN_IF}\" -s \"${LAN_NET}\" -j MASQUERADE 2&gt;/dev/null || \\\n  iptables -t nat -A POSTROUTING -o \"${WAN_IF}\" -s \"${LAN_NET}\" -j MASQUERADE\n\niptables-save &gt; /etc/iptables/rules.v4\nip6tables-save &gt; /etc/iptables/rules.v6\n\n# Start/restart dnsmasq now that interface/address is ready\nsystemctl restart dnsmasq || true\n\nlog \"Applied.\"\nEOF\nchmod +x /usr/local/sbin/router-apply.sh\n\nlog \"Creating systemd oneshot service\"\ncat &gt;/etc/systemd/system/router-apply.service &lt;&lt;'EOF'\n[Unit]\nDescription=Apply router LAN + firewall config\nAfter=systemd-networkd.service\nWants=systemd-networkd.service\n\n[Service]\nType=oneshot\nExecStart=/usr/local/sbin/router-apply.sh\nRemainAfterExit=yes\n\n[Install]\nWantedBy=multi-user.target\nEOF\n\nsystemctl daemon-reload\nsystemctl enable router-apply.service\nsystemctl enable netfilter-persistent || true\n\n# Run once now\n/usr/local/sbin/router-apply.sh\n\nlog \"Done.\"\nip -br addr || true\niptables -L -v -n || true\nsystemctl --no-pager --full status dnsmasq || true\n</code></pre> <p>Start up the microVM:</p> <pre><code>sudo slicer up ./router.yaml\n</code></pre> <p>You can then connect and start viewing the logs from the DHCP/DNS server:</p> <pre><code>sudo slicer vm shell --uid 1000 router-1\n\n# Then within the microVM:\nsudo journalctl -u dnsmasq --since today -f\n</code></pre>"},{"location":"examples/router-firewall/#configure-a-raspberry-pi-to-run-on-lan2","title":"Configure a Raspberry Pi to run on LAN2","text":"<p>Raspberry Pi plugged directly into the N100</p> <p>Now, we need to configure a Raspberry Pi to run on LAN2.</p> <p>First, flash the standard 64-bit Raspberry Pi OS Lite (Trixie) image to a microSD card.</p> <p>i.e. replace <code>sdX</code> with the device name of the microSD card found via <code>lsblk</code>:</p> <pre><code>sudo dd if=/path/to/raspberrypi-os-lite-64-trixie.img of=/dev/sdX bs=4M status=progress &amp;&amp; sync\n</code></pre> <p>Next, mount the microSD card so that you can supply config via the boot partition.</p> <p>We'll be using userdata, and a custom username and password.</p> <p>Save <code>provision.sh</code>:</p> <pre><code>#!/usr/bin/env bash\nset -euo pipefail\n\nBOOT=/mnt/boot\nHOSTNAME=slicer-client-1\nUSERNAME=alex\n\n# 1) Generate a strong random password (URL-safe)\nPLAINTEXT_PASS=\"$(openssl rand -base64 18)\"\n\n# 2) Hash it for cloud-init (SHA-512)\nHASHED_PASS=\"$(openssl passwd -6 \"$PLAINTEXT_PASS\")\"\n\n# 3) Write user-data\ncat &lt;&lt;EOF | sudo tee \"$BOOT/user-data\" &gt;/dev/null\n#cloud-config\n\nhostname: $HOSTNAME\nmanage_etc_hosts: true\n\nusers:\n  - name: $USERNAME\n    gecos: $USERNAME\n    groups: [sudo]\n    sudo: ALL=(ALL) NOPASSWD:ALL\n    shell: /bin/bash\n    lock_passwd: false\n    passwd: \"$HASHED_PASS\"\n\nssh:\n  install-server: true\n  allow-pw: true\n\ndisable_root: true\n\npackage_update: false\npackage_upgrade: false\n\nruncmd:\n  - systemctl enable ssh\n  - systemctl start ssh\nEOF\n\n# 4) Write network-config\ncat &lt;&lt;EOF | sudo tee \"$BOOT/network-config\" &gt;/dev/null\nversion: 2\nethernets:\n  eth0:\n    dhcp4: true\n    dhcp6: false\nEOF\n\n# 5) Write meta-data\ncat &lt;&lt;EOF | sudo tee \"$BOOT/meta-data\" &gt;/dev/null\ninstance-id: $HOSTNAME\nlocal-hostname: $HOSTNAME\nEOF\n\n# 6) Output credentials (your copy)\necho\necho \"======================================\"\necho \"  Raspberry Pi credentials generated\"\necho \"======================================\"\necho \"Host:      $HOSTNAME\"\necho \"User:      $USERNAME\"\necho \"Password:  $PLAINTEXT_PASS\"\necho \"======================================\"\necho\n</code></pre> <p>Mount the SD card's first partition to <code>/mnt/boot</code>, then run the script:</p> <pre><code>sudo mount /dev/sdX1 /mnt/boot\nsudo ./provision.sh\nsudo umount /mnt/boot\n</code></pre> <p>Next, make sure the Raspberry Pi is plugged into the network interface on the Slicer host.</p>"},{"location":"examples/router-firewall/#access-the-raspberry-pi","title":"Access the Raspberry Pi","text":"<p>From the Slicer host, open a shell into the microVM:</p> <pre><code>sudo slicer vm shell --uid 1000 router-1\n</code></pre> <p>You can either find the IP given out from the <code>journalctl</code> command we ran earlier, or use nmap to discover the Raspberry Pi on the network:</p> <pre><code>nmap -sP 10.88.0.0/24\n</code></pre> <p>You should see the Raspberry Pi's IP address in the output.</p> <pre><code>Starting Nmap 7.80 ( https://nmap.org ) at 2026-01-06 10:34 UTC\nNmap scan report for 10.88.0.1\nHost is up (0.00041s latency).\nNmap scan report for 10.88.0.67\nHost is up (0.00068s latency).\nNmap done: 256 IP addresses (2 hosts up) scanned in 2.32 seconds\n</code></pre> <p>You can then connect to the Raspberry Pi over SSH using the credentials from earlier:</p> <pre><code>ssh $USERNAME@10.88.0.1\n</code></pre> <p>On the Raspberry Pi, you'll be able to ping the microVM's IP address, and access the Internet via the router/firewall microVM.</p> <pre><code>ubuntu@router-1:~$ ping -c1 10.88.0.1\nPING 10.88.0.1 (10.88.0.1) 56(84) bytes of data.\n64 bytes from 10.88.0.1: icmp_seq=1 ttl=64 time=0.073 ms\n\n--- 10.88.0.1 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 0.073/0.073/0.073/0.000 ms\nubuntu@router-1:~$ ping -c1 8.8.8.8\nPING 8.8.8.8 (8.8.8.8) 56(84) bytes of data.\n64 bytes from 8.8.8.8: icmp_seq=1 ttl=118 time=7.51 ms\n\n--- 8.8.8.8 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 7.507/7.507/7.507/0.000 ms\nubuntu@router-1:~$ curl google.com\n&lt;HTML&gt;&lt;HEAD&gt;&lt;meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\"&gt;\n&lt;TITLE&gt;301 Moved&lt;/TITLE&gt;&lt;/HEAD&gt;&lt;BODY&gt;\n&lt;H1&gt;301 Moved&lt;/H1&gt;\nThe document has moved\n&lt;A HREF=\"http://www.google.com/\"&gt;here&lt;/A&gt;.\n&lt;/BODY&gt;&lt;/HTML&gt;\nubuntu@router-1:~$ \n</code></pre>"},{"location":"examples/router-firewall/#block-access-to-your-main-network","title":"Block access to your main network","text":"<p>By using iptables, you can block access to your main network (LAN1) from LAN2. This creates a Demilitarized Zone (DMZ) where devices on LAN2 can access the Internet and the router itself, but not the rest of your main network.</p> <p>For example, if your main network is <code>192.168.1.0/24</code> (different from the router's LAN1 interface subnet <code>192.168.130.0/24</code>), you can firewall it off so that LAN2 only has access to the Internet and the microVM router.</p> <p>Add this iptables rule to your userdata script before the existing FORWARD ACCEPT rules. Since iptables evaluates rules in order, the DROP rule must come before the ACCEPT rule:</p> <pre><code># Block access to LAN1 network (192.168.1.0/24) from LAN2\n# IMPORTANT: This must be inserted BEFORE the existing ACCEPT rules\n# The -I flag inserts at position 1 (beginning of chain), so it's evaluated first\niptables -I FORWARD -i \"${LAN_IF}\" -d 192.168.1.0/24 -j DROP\n</code></pre> <p>Here's what the full table ends up looking like on the microVM end:</p> <pre><code>root@router-1:~# iptables -L -n -v\nChain INPUT (policy ACCEPT 0 packets, 0 bytes)\n pkts bytes target     prot opt in     out     source               destination         \n    2   654 ACCEPT     udp  --  ens7   *       0.0.0.0/0            0.0.0.0/0            udp dpt:67\n    2   134 ACCEPT     udp  --  ens7   *       0.0.0.0/0            0.0.0.0/0            udp dpt:53\n    0     0 ACCEPT     tcp  --  ens7   *       0.0.0.0/0            0.0.0.0/0            tcp dpt:53\n\nChain FORWARD (policy ACCEPT 0 packets, 0 bytes)\n pkts bytes target     prot opt in     out     source               destination         \n    0     0 DROP       all  --  ens7   *       0.0.0.0/0            192.168.1.0/24      \n    8   608 ACCEPT     all  --  ens7   eth0    10.88.0.0/24         0.0.0.0/0           \n    8   608 ACCEPT     all  --  eth0   ens7    0.0.0.0/0            10.88.0.0/24         ctstate RELATED,ESTABLISHED\n\nChain OUTPUT (policy ACCEPT 0 packets, 0 bytes)\n pkts bytes target     prot opt in     out     source               destination         \nroot@router-1:~#\n</code></pre> <p>Place this rule in your userdata script before the existing FORWARD rules (around line 269-274). The <code>-I</code> flag inserts the rule at the beginning of the FORWARD chain, ensuring it's evaluated before the general ACCEPT rule that allows all LAN2 -&gt; LAN1 traffic.</p> <p>The existing rules already handle: - Access to the router itself (INPUT rules allow DNS/DHCP on <code>${LAN_IF}</code>) - Established connections back from Internet (the existing FORWARD rule with <code>ESTABLISHED,RELATED</code>)</p> <p>With this rule in place, devices on LAN2 can: - Access the Internet (via NAT masquerade through <code>${WAN_IF}</code> which connects to LAN1) - Access the router/firewall microVM itself (10.88.0.1) for DNS and DHCP - Not access any devices on LAN1 (<code>192.168.1.0/24</code>)</p> <p>This setup is ideal for running servers, CI runners, and other services exposed to the Internet through tunnels such as Inlets.</p>"},{"location":"examples/router-firewall/#wrapping-up","title":"Wrapping up","text":"<p>You've now created a lightweight Linux router/firewall which physically isolates LAN2 from LAN1.</p> <p></p> <p>Raspberry Pi accessing the Internet via the router/firewall microVM.</p> <p>In the screenshot, the top pane shows the PCI device being passed through to the microVM. The left pane shows the output of the Linux daemons providing IP addresses via DHCP and DNS. The right pane shows the Raspberry Pi being able to access the Internet via the new LAN2 IP range.</p> <p>LAN2 has its own IP range: <code>10.88.0.0/24</code> and all devices must pass through the microVM to access the Internet or LAN1.</p> <p>See the Block access to your main network section above for instructions on how to firewall off your main network from LAN2.</p>"},{"location":"examples/run-a-task/","title":"Run a task in a VM","text":"<p>This example shows how to run a one-shot task in a VM via API. The CLI can also act as a client to the API during testing.</p> <p>Use-cases could include:</p> <ul> <li>Running an AI coding agent in a contained environment without risking your whole workstation</li> <li>Starting on-demand IDEs for pull request development or review</li> <li>Autoscaling Kubernetes nodes - added and removed on demand</li> <li>Running a CI build or compiling untrusted customer code</li> <li>Starting a temporary service such as a database for end to end testing</li> <li>Cron jobs, batch jobs, and serverless functions</li> </ul> <p>One-shot tasks are VMs that are launched on demand for a specific purposes. But there's no limit on the lifetime of these VMs, they can run for any period of time - be that 250ms to process a webhook, 48 hours to run some fine-tuning, or several weeks. Just bear in mind that if you shut down or close Slicer, they will also be shut down and destroyed.</p> <p>Watch a demo of the tutorial to see how fast it is to launch microVMs for one-shot tasks.</p>"},{"location":"examples/run-a-task/#tutorial","title":"Tutorial","text":"<p>Create an empty hostgroup configuration.</p> <p>For the fastest possible boot times, use ZFS for storage.</p> <p>If you don't have ZFS set up yet, you can simply replace the storage flags with something like:</p> <pre><code>--storage: image\n</code></pre> <p>Create <code>tasks.yaml</code> slicer config:</p> <pre><code>slicer new buildkit \\\n  --cpu 1 \\\n  --ram 2 \\\n  --count 0 \\\n  --storage zfs \\\n  --persistent false \\\n  &gt; tasks.yaml\n</code></pre> <p>Now start up slicer:</p> <pre><code>sudo slicer up ./tasks.yaml\n</code></pre> <p>Now set up a HTTP endpoint using a free service like ReqBin.com or webhook.site.</p> <p>Write a userdata script to send a POST request to your HTTP endpoint on boot-up, then have it exit.</p> <p>Save task.sh:</p> <pre><code>cat &gt; task.sh &lt;&lt;'EOF'\n#!/bin/bash\ncurl -i -X POST -d \"$(cat /etc/hostname) booted\\nUptime: $(uptime)\" \\\n    https://webhook.site/f38eddbf-6285-4ff8-ae3e-f2e782c73d8f\n\nsleep 1\nsudo reboot\nexit 0\nEOF\n</code></pre> <p>Then run your task by booting a VM with the script as its userdata:</p> <pre><code>curl -isLSf http://127.0.0.1:8081/hostgroup/task/nodes \\\n    -H \"Content-Type: application/json\" \\\n    --data-binary \"{\n  \\\"userdata\\\": $(cat ./task.sh | jq -Rs .)\n}\"\n</code></pre> <p>Check your HTTP bin for the results.</p> <p>You can also run this in a for loop:</p> <pre><code>for i in {1..5}\ndo\n  curl -sLSf http://127.0.0.1:8081/hostgroup/task/nodes \\\n      -H \"Content-Type: application/json\" \\\n      --data-binary \"{\\\"userdata\\\": $(cat ./task.sh | jq -Rs .)}\"\ndone\n</code></pre> <p></p> <p>Each of the 5 tasks that executed and exited, posted to the endpoint</p>"},{"location":"examples/run-a-task/#launch-a-task-from-the-cli","title":"Launch a task from the CLI","text":"<p>The slicer CLI can act as a HTTP client to the REST API, which makes it a bit easier for initial exploration:</p> <pre><code>for i in {1..5};\ndo\n  slicer vm add \\\n    task \\\n    --api http://127.0.0.1:8081 \\\n    --userdata-file ./task.sh\ndone\n</code></pre> <p>The output will be as follows:</p> <pre><code>VM created\n  Hostname: task-1\n  Group:    task\n  IP:       192.168.138.2/24\n  Specs:    1 vCPU, 2GB RAM, GPUs: 0\n  Persistent: false\n  Created:  2025-09-09T09:45:27+01:00\n</code></pre> <p>When using Cloud Hypervisor for GPU support, the <code>--gpus</code> flag can be passed to allocate a number of GPUs from the host into the guest VM.</p> <p>This would be useful for batch inference, or AI jobs that benefit from direct access to a local LLM.</p>"},{"location":"examples/run-a-task/#optimise-the-image-for-start-up-speed","title":"Optimise the image for start-up speed","text":"<p>After various Kernel modules are loaded, and the system has performed its self-checking, your code should be running at about the 2.5s mark, or a bit earlier depending on your machine.</p> <p>To optimise the boot time further for one-shot use-cases, the SSH host key regenerate step that is present on start-up. It can add a few seconds to the boot time, especially if entropy is low on your system.</p> <p>You can derive your own image to use, with this disabled:</p> <pre><code>FROM ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-latest\n\nRUN systemctl disable regen-ssh-host-keys &amp;&amp;\n    systemctl disable ssh &amp;&amp; \\\n    systemctl disable sshd &amp;&amp; \\\n    systemctl disable slicer-vmmeter\n</code></pre> <p>After SSH is disabled, the only way to debug a machine is via the Slicer agent using <code>slicer vm shell</code> to get a shell.</p> <p>You can also disable <code>slicer-agent</code> (not actually a full SSH daemon), however the <code>slicer vm</code> commands will no longer work.</p> <p>If you publish an image to the Docker Hub, make sure you include its prefix i.e. <code>docker.io/owner/repo:tag</code>.</p>"},{"location":"examples/run-a-task/#cache-the-kernel-to-a-local-file","title":"Cache the Kernel to a local file","text":"<p>Rather than downloading an extracting the Kernel on each run of Slicer, you can extract a given vmlinux file and tell the YAML file to use that.</p> <p>This is preferred for a long-running host, where we need to keep the root-filesystem image and Kernel in sync.</p> <p>The Kernel must agree with the root filesystem image, which means using a proper tag and not a <code>latest</code> tag.</p> <p>Why? The Kernel is built as a vmlinux, however its modules are packaged into the root filesystem image.</p> <p>Run the following:</p> <pre><code>$ arkade get crane\n$ crane ls ghcr.io/openfaasltd/actuated-kernel:5.10.240-x86_64-latest\n\n5.10.240-x86_64-3d7a67d1683b524b4128ad338f90b1da710f2fd9\n5.10.240-kvm-x86_64-3d7a67d1683b524b4128ad338f90b1da710f2fd9\n5.10.240-x86_64-ea04b63b9117c966a57e17e1bc1bfcf713cd6276\n5.10.240-x86_64-bb71bdd1cd06bad2cc11f8ab3f323c3f19d41c8b\n5.10.240-x86_64-2f2ebc0bbefe128aa3061e6ea6806cbcdc975208\n6.1.90-aarch64-2f2ebc0bbefe128aa3061e6ea6806cbcdc975208\n6.1.90-aarch64-5c59e9b9b08eea49499be8449099291c93469b80\n5.10.240-x86_64-5c59e9b9b08eea49499be8449099291c93469b80\n</code></pre> <p>Pick a stable tag for your architecture i.e. <code>x86_64-SHA</code> or <code>aarch64-SHA</code>, then run:</p> <pre><code>$ arkade oci install ghcr.io/openfaasltd/actuated-kernel:5.10.240-x86_64-2f2ebc0bbefe128aa3061e6ea6806cbcdc975208 --output ./\n$ ls\nvmlinux\n</code></pre> <p>Next, find the matching root filesystem image:</p> <pre><code>$ crane ls ghcr.io/openfaasltd/slicer-systemd\n</code></pre> <p>Use it in your YAML file, replacing <code>kernel_image</code> with <code>kernel_file</code>:</p> <pre><code>  kernel_file: \"./vmlinux\"\n  image: \"ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-2f2ebc0bbefe128aa3061e6ea6806cbcdc975208\"\n</code></pre>"},{"location":"getting-started/daemon/","title":"How to run Slicer as a Daemon","text":"<p>The term \"daemon\" is a well established phrase from UNIX meaning a process that runs in the background. Daemons are usually managed by an init system which can monitor and restart them.</p> <p>Do you need to run Slicer as a systemd service right off the bat? It depends.</p> <p>You may not need a systemd service if you're still in the development and testing phase. It's easier and more convenient to start running Slicer in a <code>tmux</code>. You can attach and detach as you like, and it'll stay running in the background. A permanent service comes in useful when you need to run Slicer on boot-up and to ensure it gets restarted if it crashes for any reason. Learn how Alex uses tmux to run processes in the background.</p> <p>Not only can we monitor Slicer's logs via <code>journalctl</code>, but we can manage it with standard <code>systemctl</code> commands.</p> <p>You can run one or many Slicer daemons in this way, just make sure the host group CIDRs do not overlap, and that the API binds to a different UNIX socket or a different TCP port.</p> <p>Let's say you wanted to create a service for a hostgroup named \"vm\":</p> <pre><code>mkdir -p ~/vm/\ncd ~/vm/\nslicer new vm &gt; ./slicer.yaml\n</code></pre> <p>To listen on a UNIX port instead, use <code>slicer new vm --api-auth=false --api-bind ./slicer.sock &gt; ./slicer.yaml</code>.</p> <p>Create a service named i.e. <code>vm.service</code>:</p> <pre><code>[Unit]\nDescription=Slicer for vm\n\n[Service]\nUser=alex\nType=simple\nWorkingDirectory=/home/alex/vm\nExecStart=sudo /usr/local/bin/slicer up ./slicer.yaml\nEnvironment=SUDO_USER=alex\nRestart=always\nRestartSec=30s\nKillMode=mixed\nTimeoutStopSec=30\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Install the service, and set it to start up on reboots:</p> <pre><code>sudo cp ./vm.service /etc/systemd/system/\nsudo systemctl daemon-reload\nsudo systemctl enable --now vm.service\n</code></pre> <p>Assuming you went for the UNIX socket option, you could run i.e.</p> <pre><code>sudo slicer --url ~/vm/slicer.sock vm list\nHOSTNAME                  IP              RAM          CPUS     STATUS     CREATED              TAGS                \n--------                  --              ---          ----     ------     -------              -----               \nvm-1                      192.168.137.2   4GiB         2        Running    2026-02-27 16:30:53      \n</code></pre> <p>For the TCP option (with the default port of 8080), you'd run:</p> <pre><code>sudo slicer vm list\nHOSTNAME                  IP              RAM          CPUS     STATUS     CREATED              TAGS                \n--------                  --              ---          ----     ------     -------              -----               \nvm-1                      192.168.137.2   4GiB         2        Running    2026-02-27 16:30:53      \n</code></pre> <p>To view the logs for the service run:</p> <pre><code># Page through all logs\nsudo journalctl -f --output=cat -u vm\n\n# Page through all logs\nsudo journalctl --output=cat -u vm\n\n# View all logs since today/yesterday\nsudo journalctl --output=cat --since today -u vm\nsudo journalctl --output=cat --since yesterday -u vm\n</code></pre> <p>To stop the service run <code>sudo systemctl stop vm</code>, and to prevent it loading on start-up run: <code>sudo systemctl disable vm</code>.</p> <p>You can create multiple Slicer services to run different sets of VMs or configurations on the same host.</p>"},{"location":"getting-started/install/","title":"Installation for Slicer for Linux","text":"<p>Don't wait for the perfect system. Slicer can run practically anywhere.</p> <p>To activate Slicer, pick the tier that matches your use-case on the Slicer pricing page. A free trial is available.</p> <p>If you need Slicer for Mac instead, use the Slicer for Mac installation guide first.</p> <p>After the installation, when you run <code>slicer activate</code> you'll get an invite link to the Discord server. We highly recommend joining.</p>"},{"location":"getting-started/install/#system-requirements","title":"System requirements","text":"<p>Any reasonably modern computer can run Slicer, the requirements are very low - x86_64 or Arm64 (including the Raspberry Pi).</p> <p>Ideal for labs and the home office:</p> <ul> <li>Low powered mini PC i.e. Intel N100, Beelink,  Minisforum,  Acemagic, etc</li> <li>Adlink Ampere Developer Platform / system76 Thelio Astra / Raspberry Pi 4 or 5 (an NVMe is better than SD card)</li> <li>Mac Mini M1 or M2 (with Asahi Linux installed)</li> <li>PC, laptop, or used server from eBay - under your desk or in your basement.</li> </ul> <p>Cloud-based bare-metal:</p> <ul> <li>Hetzner bare-metal aka \"robot\" (cheapest, best value)</li> <li>Phoenix NAP</li> <li>Latitude.sh</li> </ul> <p>Additional cloud-based options for KVM are included on this page on our sister site (Actuated)</p> <p>Enterprise:</p> <ul> <li>On-premises datacenter with your own bare-metal servers</li> <li>OpenStack / VMware (with nested virtualisation)</li> <li>Azure, DigitalOcean, GCP VMs (with nested virtualisation)</li> </ul> <p>A Linux system with KVM is required (bare-metal or nested virtualisation), so if you see <code>/dev/kvm</code>, Slicer will work there.</p> <p>Ubuntu LTS is formally supported, whilst Debian, Fedora, RHEL-like Operating Systems (Rocky, Alma, CentOS), and Arch Linux should work - we won't be able to debug your system.</p> <p>Ideally, nothing else should be installed on a host that runs Slicer. It should be thought of as a basic appliance - a bare OS, with minimal packages.</p>"},{"location":"getting-started/install/#quick-installation","title":"Quick installation","text":"<p>The default slicer installation only enables support for <code>image</code> storage. Additional storage backends for zfs or devmapper can be enabled using the <code>--zfs</code> and <code>--devmapper</code> flags. See Snapshot-based storage.</p> <pre><code>curl -sLS https://get.slicervm.com | sudo bash\n</code></pre> <p>The installer sets up Firecracker, Cloud Hypervisor, containerd for storage, and a few networking options.</p> <p>Feel free to read/verify the installation script before running it.</p> <p>Additional storage backends can always be enabled later by running the installer again with the appropriate flags.</p> <p>Activate Slicer to obtain your license key:</p> <pre><code>slicer activate --help\n\nslicer activate\n</code></pre> <p>For the Individual tier the a license key will be written to <code>~/.slicer/LICENSE</code> along with a GitHub token. The key will last for 30 days, but can be refreshed via <code>slicer activate</code>. When refreshing, it'll use the stored GitHub token, instead of running the device flow again.</p> <p>For higher tiers, the license key is an API key delivered to you via email. You should save it to <code>~/.slicer/LICENSE</code>.</p> <p>Next, start your first VM with the walk through.</p>"},{"location":"getting-started/install/#snapshot-based-storage","title":"Snapshot-based storage","text":"<p>Snapshot-based storage is not required for development and testing, instead, it's recommended that most users use the <code>image</code> storage approach instead, until they want to trade a little extra setup, for much improved VM disk clone times.</p> <p>Snapshot-based storage enables much faster VM creation times. ZFS is the recommended option for Slicer, devmapper is also supported. See storage for slicer for more info on the different storage backends.</p> <p>For best performance using a dedicated drive, volume or partition for the storage backend is recommended. If no disk is provided a loopback device will be created automatically.</p> <p>ZFS on non-Ubuntu distributions</p> <p>Automatic ZFS installation is only supported on Ubuntu. On other distributions, install ZFS manually before running the install script with the <code>--zfs</code> flag.</p> <p>ZFS (loopback)</p> <pre><code>curl -sLS https://get.slicervm.com | sudo bash -s -- \\\n  --zfs\n</code></pre> <p>ZFS (dedicated drive)</p> <pre><code>curl -sLS https://get.slicervm.com | sudo bash -s -- \\\n  --zfs /dev/sdb\n</code></pre> <p>Devmapper (loopback)</p> <pre><code>curl -sLS https://get.slicervm.com | sudo bash -s -- \\\n  --devmapper\n</code></pre> <p>Devmapper (dedicated drive)</p> <pre><code>curl -sLS https://get.slicervm.com | sudo bash -s -- \\\n  --devmapper /dev/sdb\n</code></pre> <p>The <code>--devmapper</code> and <code>--zfs</code> flags can be used together to enable both storage backends.</p>"},{"location":"getting-started/install/#updating-slicer","title":"Updating slicer","text":"<p>To update Slicer, use the <code>slicer update</code> command:</p> <pre><code>sudo slicer update\n</code></pre> <p>Alternatively, if you're on an earlier version, repeat this command from the installation step:</p> <pre><code>sudo arkade oci install ghcr.io/openfaasltd/slicer:latest \\\n  --path /usr/local/bin\n</code></pre>"},{"location":"getting-started/walkthrough/","title":"Create a Linux VM","text":"<p>In this example we'll walk through how to create a Linux VM using Slicer on an x86_64 host, or an Arm64 host.</p> <p>The <code>/dev/kvm</code> device must exist and be available to continue.</p>"},{"location":"getting-started/walkthrough/#create-the-vm-configuration","title":"Create the VM configuration","text":"<p>Slicer is a long lived process that can be run in the foreground or as a daemon with systemd.</p> <p>Config files can be copy and pasted from the docs, or you can generate one from a template by running the below, where <code>vm</code> is the name of the hostgroup.</p> <pre><code>slicer new vm &gt; vm-image.yaml\n</code></pre> <p>The default configuration uses a Linux bridge for networking, a disk image for storage, and the Firecracker hypervisor.</p> <pre><code>config:\n  host_groups:\n  - name: vm\n    storage: image\n    storage_size: 25G\n    count: 1\n    vcpu: 2\n    ram_gb: 4\n    network:\n      bridge: brvm0\n      tap_prefix: vmtap\n      gateway: 192.168.137.1/24\n\n  github_user: alexellis\n\n  image: \"ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-latest\"\n\n  hypervisor: firecracker\n\n  api:\n    port: 8080\n    bind_address: \"127.0.0.1\"\n  ssh:\n    port: 2222\n    bind_address: \"0.0.0.0\"\n</code></pre> <ul> <li><code>count</code> - the number of VMs to create - the default is <code>1</code> - you can set this to <code>0</code> to create VMs via API instead</li> <li><code>vcpu</code> - the number of virtual CPUs to allocate to each VM</li> <li><code>ram_gb</code> - the amount of RAM in GB to allocate to each VM</li> <li><code>storage</code> - image is the simplest option to get started</li> <li><code>storage_size</code> - for storage backends which support it, you can specify the size of the disk</li> <li><code>github_user</code> - your GitHub username, used to fetch your public SSH keys from your profile - additional SSH keys can be added via the ssh_keys API.</li> </ul> <p>The HTTP API is enabled by default and can be disabled with <code>enabled: false</code>.</p> <p>The Serial Over SSH (SOS) server is disabled by default and can be enabled by providing a port. Most users will  not need the SOS.</p> <p>The default <code>bind_address</code> for both the API and SSH is <code>127.0.0.1</code> - loopback on the host system.</p> <p>There's no need to provide the HTTP API section unless you plan to run Slicer more than once on the same host, in that case, it's useful to include it so you can change it to a different port on another slicer instance.</p> <p>Configuration for an Arm64 Slicer installation</p> <p><code>slicer new</code> will generate a configuration file for the correct image for Arm or x86_64, but if you create one manually, you'll need to edit the <code>image</code> field.</p> <pre><code>-  image: \"ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-latest\"\n+  image: \"ghcr.io/openfaasltd/slicer-systemd-arm64:6.1.90-aarch64-latest\"\n</code></pre> <p>The <code>storage: image</code> setting means a disk image will be cloned from the root filesystem into a local file. It's not the fastest option for the initial setup, but it's the simplest, persistent and great for long-living VMs.</p> <p>Now, open a new terminal window, or ideally launch <code>tmux</code> so you can leave the binary running in the background.</p> <pre><code>sudo slicer up ./vm-image.yaml\n</code></pre> <p>Having customised the <code>github_user</code> to your own username, your SSH keys will have been fetched from your profile, and preinstalled into the VM.</p> <p>On your workstation, add any routes that are specified so you can access the VMs on their own network.</p> <p>If you need to get the route output back again, you can use the <code>slicer vm route</code> command on the host itself, specifying the config file as the argument.</p> <p>Never run any route commands outputted by Slicer on the host itself. It's not required and will break the networking.</p> <p>Then, you can connect with SSH:</p> <pre><code>ssh ubuntu@192.168.137.2\n</code></pre>"},{"location":"getting-started/walkthrough/#ignore-changing-ssh-host-keys","title":"Ignore changing SSH host keys","text":"<p>If, like the developers of Slicer, you'll be re-creating many hosts with the same IP addresses, you have two options:</p> <ul> <li>Memorise and get familiar with the <code>ssh-keygen -R &lt;ip-address&gt;</code> command</li> <li>Or add the following to your <code>~/.ssh/config</code> file to stop it complaining</li> </ul> <pre><code>Host 192.168.137.*\n    StrictHostKeyChecking no\n    UserKnownHostsFile /dev/null\n    GlobalKnownHostsFile /dev/null\n    CheckHostIP no\n    LogLevel QUIET\n    User ubuntu\n</code></pre> <p>Repeat it once for each IP range you use with Slicer.</p> <p>And bear in mind that you should not do this for production or long-running hosts.</p>"},{"location":"getting-started/walkthrough/#view-the-serial-console","title":"View the serial console","text":"<p>The logs from the serial console including the output from the boot process are available on disk:</p> <pre><code>$ sudo tail -f /var/log/slicer/vm-1.txt\n\n         Starting OpenBSD Secure Shell server...\n[  OK  ] Started OpenBSD Secure Shell server.\n[  OK  ] Reached target Multi-User System.\n[  OK  ] Reached target Graphical Interface.\n         Starting Record Runlevel Change in UTMP...\n[  OK  ] Finished Record Runlevel Change in UTMP.\n\nUbuntu 22.04.5 LTS vm-1 ttyS0\n\nvm-1 login:\n</code></pre> <p>If you want to tail the logs from all available VMs at once, use <code>fstail</code> via <code>arkade get fstail</code>:</p> <pre><code>sudo -E fstail /var/log/slicer/\n</code></pre>"},{"location":"getting-started/walkthrough/#launch-a-second-vm","title":"Launch a second VM","text":"<p>Edit the <code>count</code> field, and set it to <code>2</code>.</p> <p>Then hit Control + C and launch slicer again.</p> <p>You'll see the second VM come online and can connect to it over SSH.</p>"},{"location":"getting-started/walkthrough/#enable-the-http-api","title":"Enable the HTTP API","text":"<p>The API is used by the <code>slicer vm</code> commands, and can also be used directly via <code>curl</code>.</p> <pre><code>  api:\n    port: 8080\n    bind_address: \"127.0.0.1:\"\n    auth:\n      enabled: true\n</code></pre> <p>The auth token will be created at <code>/var/lib/slicer/auth/token</code> and can be used via a <code>Authorization: Bearer</code> header.</p> <p>i.e.</p> <pre><code>curl -s http://127.0.0.1:8080/nodes -H \"Authorization: Bearer $(sudo cat /var/lib/slicer/auth/token)\" | jq\n</code></pre>"},{"location":"getting-started/walkthrough/#enable-the-serial-over-ssh-console","title":"Enable the Serial Over SSH Console","text":"<p>The Serial Over SSH (SOS) console can be used to log into a VM without a password, and without any form of networking enabled.</p> <pre><code>  ssh:\n    port: 2222\n    bind_address: \"0.0.0.0:\"\n</code></pre> <p>Example:</p> <pre><code>ssh -p 2222 ubuntu@localhost\n</code></pre>"},{"location":"mac/coding-agents/","title":"Coding agents","text":"<p>You can run coding agents like Claude Code, OpenCode, and Amp inside your Slicer for Mac VM. VirtioFS sharing means the agent edits code on a shared folder that's visible on both your Mac and the VM, so you get the isolation of a VM without copying files around.</p> <p>Running an agent inside a VM means you don't have to pass <code>--dangerously-skip-permissions</code> on your Mac. The agent gets its own kernel and filesystem, and can't touch your SSH keys, cloud credentials, or browser sessions.</p>"},{"location":"mac/coding-agents/#automated-agent-and-sandbox-launches","title":"Automated agent and sandbox launches","text":"<p>Slicer has a number of experimental commands that do the following:</p> <ul> <li>Boot a new VM</li> <li>Copy the current directory to the VM in the same relative path</li> <li>Installs the specified AI agent, and copies in its config/auth files</li> <li>Launches the agent in a shell (<code>slicer vm shell</code>)</li> </ul> <p>Available agent commands:</p> <ul> <li><code>slicer amp</code></li> <li><code>slicer copilot</code></li> <li><code>slicer codex</code></li> <li><code>slicer claude</code></li> <li><code>slicer opencode</code></li> </ul> <p>Finally, there's another option for custom AI agents <code>slicer workspace</code></p> <p>To launch i.e. a new slicer VM for Claude:</p> <pre><code>mkdir -p ~/dev/\ncd ~/dev/\ngit clone --depth=1 https://github.com/alexellis/arkade\n\ncd arkade\n\nslicer claude .\n</code></pre> <p>When launching a VM for a coding agent, you can specify:</p> <ul> <li><code>--tmux=none</code> - launch the agent directly in the shell</li> <li><code>--tmux=local</code> - launch a shell using tmux on your mac (requires <code>brew install tmux</code>)</li> <li><code>--tmux=remote</code> - launch a shell using tmux on the VM</li> </ul> <p>Tmux is a time-tested terminal tool and ideal for running processes in the background, and reconnecting to them later.</p>"},{"location":"mac/coding-agents/#install-agents-manually","title":"Install agents manually","text":"<p>Shell into your VM and install agents with arkade:</p> <pre><code>slicer vm shell slicer-1\n\n# Install Claude Code\narkade get claude --path /usr/local/bin\n\n# Install OpenCode\narkade get opencode --path /usr/local/bin\n</code></pre> <p>Both binaries are installed into <code>/usr/local/bin</code> inside the VM. They persist across reboots since <code>slicer-1</code> is a persistent VM.</p>"},{"location":"mac/coding-agents/#authenticate","title":"Authenticate","text":""},{"location":"mac/coding-agents/#claude-code-with-an-api-key","title":"Claude Code with an API key","text":"<p>Set the <code>ANTHROPIC_API_KEY</code> environment variable inside the VM:</p> <pre><code>export ANTHROPIC_API_KEY=sk-ant-...\n</code></pre> <p>Add it to <code>~/.bashrc</code> to persist across sessions:</p> <pre><code>echo 'export ANTHROPIC_API_KEY=sk-ant-...' &gt;&gt; ~/.bashrc\n</code></pre> <p>Then run Claude Code:</p> <pre><code>cd /home/ubuntu/host/my-project\nclaude --dangerously-skip-permissions\n</code></pre> <p>Inside the VM, <code>--dangerously-skip-permissions</code> is acceptable because the VM is the sandbox.</p> <p>Claude Max plan</p> <p>The Claude Max subscription uses OAuth-based authentication. Run <code>claude</code> inside the VM and follow the interactive login flow. The auth token is stored inside the VM and does not affect your Mac.</p>"},{"location":"mac/coding-agents/#opencode","title":"OpenCode","text":"<p>Authenticate with your provider of choice:</p> <pre><code>opencode auth login --provider github --token &lt;your-github-token&gt;\n</code></pre> <p>The auth config is stored at <code>~/.local/share/opencode/auth.json</code> inside the VM.</p>"},{"location":"mac/coding-agents/#work-on-shared-folders","title":"Work on shared folders","text":"<p>With folder sharing, your Mac project paths can be visible at <code>/home/ubuntu/host/</code>.</p> <pre><code>cd /home/ubuntu/host/code/my-project\n\n# Run Claude Code\nclaude --dangerously-skip-permissions\n\n# Or run OpenCode\nopencode\n</code></pre> <p>Changes the agent makes are immediately visible on your Mac because VirtioFS is a shared mount, not a copy.</p>"},{"location":"mac/coding-agents/#share-a-sub-path-for-tighter-isolation","title":"Share a sub-path for tighter isolation","text":"<p>If you don't want the agent to see your entire home directory, set <code>share_home</code> to a sub-path in <code>slicer-mac.yaml</code>:</p> <pre><code>share_home: \"~/code/\"\n</code></pre> <p>Restart the daemon after changing this setting. The agent can only see projects under <code>~/code/</code>, not your SSH keys, cloud credentials, or dotfiles.</p>"},{"location":"mac/coding-agents/#run-agents-in-sandboxes","title":"Run agents in sandboxes","text":"<p>For one-off tasks, launch a sandbox instead of using your persistent VM:</p> <pre><code># Launch a sandbox\nslicer vm launch sbox\n\n# Copy a repo in\nslicer vm cp ./my-project sbox-1:/home/ubuntu/my-project\n\n# Run the agent\nslicer vm exec --uid 1000 --cwd /home/ubuntu/my-project sbox-1 -- \\\n  claude --dangerously-skip-permissions -p \"Write tests for main.go\"\n\n# Copy results out\nslicer vm cp sbox-1:/home/ubuntu/my-project ./my-project-result\n\n# Delete when done\nslicer vm delete sbox-1\n</code></pre> <p>The sandbox has its own kernel and filesystem. If the agent does something unexpected, delete it and start fresh.</p>"},{"location":"mac/coding-agents/#next-steps","title":"Next steps","text":"<ul> <li>Sandboxes - more on launching and managing ephemeral VMs</li> <li>Headless OpenCode on Slicer for Linux - one-shot agent execution via userdata and the exec API</li> <li>Headless Cursor CLI on Slicer for Linux - running Cursor's CLI agent headlessly</li> <li>Trying Claude Code with Ollama - using Claude Code with a local LLM backend</li> </ul>"},{"location":"mac/docker/","title":"Use Docker from macOS with the Slicer VM socket","text":"<p>Slicer for Mac can replace the need to run Docker For Mac, but you can also run both if you wish.</p>"},{"location":"mac/docker/#want-to-run-docker-for-mac-and-slicervm-together","title":"Want to run Docker for Mac and SlicerVM together?","text":"<p>To run both together, configure Docker for Mac to use the Docker VMM option. Docker VMM is a wrapper for libkrun, which will use a different IP range for VMs that does not conflict with the range used by Slicer.</p> <p></p>"},{"location":"mac/docker/#install-and-configure-docker-in-the-vm","title":"Install and configure Docker in the VM","text":"<p>You can keep Docker running inside <code>slicer-1</code> and access it from your Mac with the local Docker CLI by forwarding the VM's Unix socket.</p> <pre><code>slicer vm shell slicer-1\n\n# Install Docker inside Ubuntu\ncurl -sLS https://get.docker.com | sudo sh\nsudo systemctl enable docker --now\n\n# Add the current Ubuntu user to the docker group\nsudo usermod -aG docker \"$USER\"\n</code></pre> <p>Log out and back into the VM so the group change is applied:</p> <pre><code>exit\nslicer vm shell slicer-1\n</code></pre> <p>Verify Docker works in the VM:</p> <pre><code>docker ps\n</code></pre>"},{"location":"mac/docker/#forward-the-docker-socket-to-your-mac","title":"Forward the Docker socket to your Mac","text":"<p>Create a Unix socket on the Mac (<code>~/.slicer/docker.sock</code>) that points to the Docker socket inside the VM:</p> <pre><code>slicer vm forward slicer-1 -L ~/.slicer/docker.sock:/var/run/docker.sock\n</code></pre> <p>Keep this command running for as long as you want to use the local socket.</p>"},{"location":"mac/docker/#use-the-local-docker-client","title":"Use the local Docker client","text":"<p>Point your Mac Docker CLI at the forwarded socket:</p> <pre><code>export DOCKER_HOST=unix://$HOME/.slicer/docker.sock\n\ndocker ps\ndocker run --rm hello-world\n</code></pre> <p>You can also add the <code>DOCKER_HOST</code> export to <code>~/.zshrc</code> or <code>~/.bashrc</code> for persistent access.</p>"},{"location":"mac/docker/#next-steps","title":"Next steps","text":"<ul> <li>Linux VM setup - broader VM setup notes, including K3s forwarding</li> </ul>"},{"location":"mac/folder-sharing/","title":"Folder sharing","text":"<p>Slicer for Mac uses VirtioFS to mount your Mac directory into the VM.</p> <p>The default setup uses <code>share_home</code> with <code>~/</code>.</p>"},{"location":"mac/folder-sharing/#mount-the-shared-folder","title":"Mount the shared folder","text":"<pre><code>slicer vm shell slicer-1\n</code></pre> <pre><code>sudo mkdir -p /home/ubuntu/home\nsudo mount -t virtiofs home /home/ubuntu/home\n</code></pre>"},{"location":"mac/folder-sharing/#make-sharing-persistent","title":"Make sharing persistent","text":"<pre><code>echo \"home /home/ubuntu/home virtiofs rw,nofail 0 0\" | sudo tee -a /etc/fstab\n</code></pre>"},{"location":"mac/folder-sharing/#limit-the-mount-path","title":"Limit the mount path","text":"<pre><code>share_home: \"~/code/\"\n</code></pre> <p>Use a sub-path in <code>slicer-mac.yaml</code> to avoid exposing your entire home directory. Restart the daemon after changing <code>share_home</code>.</p>"},{"location":"mac/installation/","title":"Installation","text":"<p>Slicer for Mac is available on all Slicer license tiers. We've tested on macOS Sequoia and Tahoe. <code>slicer-mac</code> does not need <code>sudo</code>.</p>"},{"location":"mac/installation/#install-the-binaries","title":"Install the binaries","text":"<p>You need three binaries: <code>slicer-mac</code> (the daemon), <code>slicer</code> (the CLI client), and <code>slicer-tray</code> (the menu bar app).</p> <p>If you have the Invididual tier, then first, install the <code>slicer</code> CLI and activate your license:</p> <pre><code>curl -sLS https://get.slicervm.com | sudo bash\nslicer activate\n</code></pre> <p>The activate command will start an OAuth Device Flow via GitHub. Follow the instructions to complete the flow, and you'll get a license key valid for 30 days. The key is written to <code>~/.slicer/LICENSE</code> along with a GitHub token. The token can be used to refresh the license key without going through Device Flow again.</p> <p>If you have a higher tier than Individual, you'll have received an email with the license key, copy and paste it into a file at <code>~/.slicer/LICENSE</code>.</p> <p>Then use the CLI to install the Mac-specific binaries:</p> <pre><code>slicer install slicer-mac ~/slicer-mac\n</code></pre> <p>During preview, Slicer for Mac does not come with a background service/definition (known as a plist on macOS). So you need to launch it as and when you want it. Either directly in a terminal, or in a <code>tmux</code> window.</p> <p><code>tmux</code> is available via <code>brew install tmux</code>, and you can get brew here.</p> <p>The <code>slicer</code> command acts as an API client to Slicer for Mac.</p> <p>By default, <code>slicer</code> auto-discovers the local mac socket at <code>~/slicer-mac/slicer.sock</code>, so you usually don't need any socket flags or environment variables for local use.</p>"},{"location":"mac/installation/#initial-configuration","title":"Initial configuration.","text":"<p>As a new user, we recommend you do not change the default configuration in any way.</p> <p>First of all, get used to it, leave it in the path we recommend (<code>~/slicer-mac</code>) and explore the use-cases you have in mind.</p> <p>The <code>slicer-mac</code> OCI bundle includes a default <code>slicer-mac.yaml</code> in the folder after install, so you can use that file directly.</p> <p>Only run this if you want to recreate the file because you edited or edited it.</p> <pre><code>cd ~/slicer-mac\n./slicer-mac new &gt; slicer-mac.yaml\n</code></pre> <p>The generated <code>slicer-mac.yaml</code> has two host groups:</p> <ul> <li><code>slicer</code> (<code>count: 1</code>) - your persistent Linux VM, starts with the daemon</li> <li><code>sbox</code> (<code>count: 0</code>) - ephemeral sandboxes, launched on demand for things like coding agents.</li> </ul> <pre><code>config:\n  host_groups:\n    - name: slicer\n      count: 1\n      vcpu: 4\n      ram_gb: 8\n      storage_size: 15G\n      share_home: \"~/\" # Set to \"\" to disable sharing\n      rosetta: true\n      network:\n        mode: nat\n        gateway: 192.168.64.1/24\n    - name: sbox\n      count: 0\n      vcpu: 2\n      ram_gb: 4\n      storage_size: 15G\n      rosetta: true\n      network:\n        mode: nat\n        gateway: 192.168.64.1/24\n\n  image: \"ghcr.io/openfaasltd/slicer-systemd-arm64-avz:latest\"\n  hypervisor: apple\n\n  api:\n    socket: \"./slicer.sock\"\n</code></pre> <p>The <code>share_home</code> field maps your Mac home directory into the VM via VirtioFS. Set it to <code>\"\"</code> to disable sharing entirely, or to a sub-path like <code>\"~/code/\"</code> to limit what the VM can see.</p> <p>Need Rosetta for x86_64 binaries? Follow Enable Rosetta.</p> <p>Run the daemon. The first run pulls and prepares the VM image automatically.</p>"},{"location":"mac/installation/#start-the-daemon","title":"Start the daemon","text":"<p>Start <code>slicer-mac</code>:</p> <pre><code>cd ~/slicer-mac\n./slicer-mac up\n</code></pre>"},{"location":"mac/installation/#start-the-menu-bar-app-optional","title":"Start the menu bar app (optional)","text":"<p>The menu bar is an optional extension that gives you quick access to VM status, shells, and controls:</p> <pre><code>cd ~/slicer-mac\n./slicer-tray\n</code></pre> <p></p> <p>The Slicer menu bar showing a running VM with options to open a shell, view logs, or shut down.</p> <p>By default, shells open in Terminal.app. For Ghostty or another terminal:</p> <pre><code>slicer-tray --terminal \"ghostty\"\n</code></pre>"},{"location":"mac/installation/#set-up-the-slicer-cli","title":"Set up the Slicer CLI","text":"<p>Verify it's working:</p> <pre><code>slicer vm list\nHOSTNAME          IP              STATUS     CREATED\n--------          --              ------     -------\nslicer-1          192.168.64.2    Running    2026-02-10 12:46:14\n</code></pre> <p>The <code>slicer-1</code> VM is your main development environment, and is persistent.</p> <p>You can shell into it with:</p> <pre><code>slicer vm shell slicer-1\n</code></pre> <p>To launch temporary VMs, run the following to launch a new VM into the <code>sbox</code> host group:</p> <pre><code>slicer vm launch sbox\n</code></pre> <p>Then:</p> <pre><code># The following will show the two VMs:\nslicer vm list\n\n# Access a shell\nslicer vm shell sbox-1\n\n# Copy a file in\nslicer vm cp ~/file.txt sbox-1:~/\n\n# Use that file, remotely via exec:\nslicer vm exec sbox-1 -- stat ~/file.txt\n\n# Delete that VM\nslicer vm delete sbox-1\n</code></pre>"},{"location":"mac/installation/#run-slicer-mac-as-a-background-service","title":"Run slicer-mac as a background service","text":"<pre><code>cd ~/slicer-mac\n\n# Simplest, without tray icon / menu:\n./slicer-mac install --no-tray\n\n# With tray icon / menu:\n./slicer-mac install\n</code></pre> <p>To uninstall:</p> <pre><code>cd ~/slicer-mac\n./slicer-mac uninstall\n</code></pre>"},{"location":"mac/installation/#next-steps","title":"Next steps","text":"<ul> <li>Linux VM - mount shared folders, forward Docker and K3s</li> <li>Sandboxes - spin up ephemeral VMs</li> </ul>"},{"location":"mac/linux-vm/","title":"Linux VM","text":"<p>Your main VM is <code>slicer-1</code> in the <code>slicer</code> host group. It's a persistent arm64 Linux environment that boots with the daemon and has your Mac home folder shared in via VirtioFS. This is where you do your day-to-day Linux work - running Docker, K3s, coding agents, Go/Rust builds, and anything else you'd do on a Linux workstation.</p> <p>SSH is usually not required for normal workflows. Most tasks are faster with the Slicer CLI:</p> <ul> <li><code>slicer vm shell slicer-1</code></li> <li><code>slicer vm cp ...</code></li> <li><code>slicer vm port-forward ...</code></li> </ul> <p>Use SSH only when you need direct shell access outside this interface. You can add keys directly in the guest by writing to <code>~/.ssh/authorized_keys</code>:</p> <pre><code>curl -sLS https://github.com/alexellis.keys &gt; ~/.ssh/authorized_keys\n</code></pre>"},{"location":"mac/linux-vm/#folder-sharing","title":"Folder sharing","text":"<p>See Folder sharing for setup details.</p>"},{"location":"mac/linux-vm/#forward-docker","title":"Forward Docker","text":"<p>Install Docker in the VM if it's not already present:</p> <pre><code>curl -sLS https://get.docker.com | sudo sh\nsudo usermod -aG docker ubuntu\nsudo systemctl enable docker --now\n</code></pre> <p>Forward Docker's socket to your Mac so <code>docker</code> commands work natively on the host:</p> <pre><code>slicer vm port-forward slicer-1 \\\n  ~/.slicer/docker.sock:/var/run/docker.sock\n</code></pre> <p>Then on your Mac:</p> <pre><code>export DOCKER_HOST=unix://$HOME/.slicer/docker.sock\n\ndocker ps\n</code></pre> <p>Add the <code>DOCKER_HOST</code> export to your <code>~/.zshrc</code> or <code>~/.bashrc</code> to make it permanent.</p>"},{"location":"mac/linux-vm/#forward-k3s","title":"Forward K3s","text":"<p>Install K3s tooling in the guest first if needed:</p> <pre><code>arkade get k3sup kubectl\nk3sup install --local\n</code></pre> <p>Forward K3s port 6443 so <code>kubectl</code> on your Mac can reach the cluster:</p> <pre><code>slicer vm port-forward slicer-1 \\\n  6443:127.0.0.1:6443\n</code></pre> <p>Then point kubectl at it:</p> <pre><code>slicer vm cp slicer-1:/etc/rancher/k3s/k3s.yaml ~/.kube/slicer-k3s-config\nexport KUBECONFIG=$HOME/.kube/slicer-k3s-config\n\nkubectl get nodes\n</code></pre> <p>With K3s running inside Slicer, you can test controllers locally, validate Helm charts with a real install, or try RBAC changes without touching a shared cluster.</p>"},{"location":"mac/linux-vm/#architecture-diagram","title":"Architecture diagram","text":"<pre><code>                         +----------------------------+\n                         |          slicer CLI        |\n                         |   (vm shell / vm cp / API) |\n                         +-------------+--------------+\n                                       |\n                                       v\n      +--------------------------------+-----------------------------------+\n      |              slicer-mac daemon on macOS                            |\n      |  Reads `slicer-mac.yaml` and controls local microVMs               |\n      +-----------------------+----------------------+---------------------+\n                              |                      |\n                              |                      |\n                              v                      v\n             +-----------------------------+    +----------------------------+\n             | host_group: slicer          |    | host_group: sbox           |\n             | Long-lived primary workload |    | Disposable / on-demand VMs |\n             +--------------+--------------+    +-------------+--------------+\n                            |                                |\n                            v                                v\n                     +-------------+                  +----------------+\n                     |   slicer-1  |                  |    sbox-1      |\n                     | main VM     |                  | sample sbox VM |\n                     +-------------+                  +----------------+\n</code></pre> <p>Docker's socket is port-forwarded to your Mac as a Unix socket, so <code>docker</code> commands on the Mac talk directly to the VM. K3s exposes port 6443, so <code>kubectl</code> on your Mac can target the cluster running inside <code>slicer-1</code>.</p>"},{"location":"mac/linux-vm/#next-steps","title":"Next steps","text":"<ul> <li>Sandboxes - spin up ephemeral VMs for AI agents and automation</li> <li>Copy files to/from a VM - use <code>slicer vm cp</code> to move files in and out</li> <li>Execute commands in a VM - run commands remotely with <code>slicer vm exec</code></li> </ul>"},{"location":"mac/overview/","title":"Slicer for Mac","text":"<p>Slicer for Mac runs arm64 Linux VMs on Apple Silicon using Apple's native Virtualization framework. It provides a persistent Linux VM with native folder sharing, Docker, K3s, and disposable sandboxes - all driven by the same CLI and REST API as Slicer for Linux.</p> <p>Preview</p> <p>Slicer for Mac is available on all Slicer license tiers. We've tested on macOS Sequoia and Tahoe. <code>slicer-mac</code> does not need <code>sudo</code>.</p>"},{"location":"mac/overview/#how-it-works","title":"How it works","text":"<p>Two binaries:</p> <ul> <li><code>slicer</code> - the CLI client (same binary as Slicer for Linux) and used to download <code>slicer-mac</code></li> <li><code>slicer-mac</code> - the daemon that manages VMs using Apple's Virtualization framework</li> </ul> <p>An optional menu bar app (<code>slicer-tray</code>) provides quick access to VM status, shells, and controls. It is shipped as part of the <code>slicer-mac</code> OCI asset set. If you need tray details, see Tray integration. If you need x86_64 support, see Enable Rosetta.</p> <p>The daemon reads a <code>slicer-mac.yaml</code> config file that defines two host groups:</p> <ul> <li>Services (<code>slicer</code> group) - a persistent Linux VM that boots with the daemon and stays running. This is your day-to-day Linux environment, similar to WSL on Windows.</li> <li>Sandboxes (<code>sbox</code> group) - ephemeral VMs launched on demand via the CLI or API. They're destroyed when you restart the daemon, close the lid, or delete them.</li> </ul>"},{"location":"mac/overview/#architecture-conceptual","title":"Architecture (conceptual)","text":"<pre><code>                         +----------------------------+\n                         |          slicer CLI        |\n                         |   (vm shell / vm cp / API) |\n                         +-------------+--------------+\n                                       |\n                                       v\n      +--------------------------------+-----------------------------------+\n      |              slicer-mac daemon on macOS                            |\n      |  Reads `slicer-mac.yaml` and controls local microVMs               |\n      +-----------------------+----------------------+---------------------+\n                              |                      |\n                              |                      |\n                              v                      v\n             +-----------------------------+    +----------------------------+\n             | host_group: slicer          |    | host_group: sbox           |\n             | Long-lived primary workload |    | Disposable / on-demand VMs |\n             +--------------+--------------+    +-------------+--------------+\n                            |                                |\n                            v                                v\n                     +-------------+                  +----------------+\n                     |   slicer-1  |                  |    sbox-1      |\n                     | main VM     |                  | sample sbox VM |\n                     +-------------+                  +----------------+\n</code></pre> <p>Docker's socket is port-forwarded to your Mac as a Unix socket, so <code>docker</code> commands on the Mac talk directly to the VM. K3s exposes port 6443, so <code>kubectl</code> on your Mac can target the cluster running inside <code>slicer-1</code>. It feels like you're on Linux.</p>"},{"location":"mac/overview/#storage","title":"Storage","text":"<p>Unlike Slicer for Linux (which supports devmapper and ZFS CoW backends), the Mac version uses image-backed storage. The OCI image is unpacked once, then cloned instantly using APFS' native Copy-on-Write. Launching a new sandbox doesn't copy the entire disk.</p>"},{"location":"mac/overview/#macbooks-and-sleep","title":"MacBooks and sleep","text":"<p>Host sleep behavior for MacBooks is controlled by <code>sleep_action</code> in <code>slicer-mac.yaml</code> and affects VM lifecycle. See Sleep behavior for full guidance and per-mode behavior.</p>"},{"location":"mac/overview/#next-steps","title":"Next steps","text":"<ul> <li>Installation - install binaries and start Slicer for Mac</li> <li>Linux VM - configure your persistent VM, shared folders, Docker, and K3s</li> <li>Sandboxes - spin up and tear down ephemeral VMs</li> <li>Coding agents - run Claude Code, OpenCode, and other agents inside the VM</li> </ul>"},{"location":"mac/rosetta/","title":"Enable Rosetta for x86_64 binaries","text":"<p>Enable Rosetta if you need to run x86_64 Intel/AMD Linux binaries in your Slicer guest. For more background on Rosetta, see Apple\u2019s documentation: https://support.apple.com/en-us/102527</p>"},{"location":"mac/rosetta/#enable-in-slicer-macyaml","title":"Enable in <code>slicer-mac.yaml</code>","text":"<p>Edit the relevant host group in your <code>slicer-mac.yaml</code> and flip <code>rosetta</code> to <code>true</code>.</p> <ul> <li><code>slicer</code> host group affects your persistent main VM (<code>slicer-1</code>).</li> <li><code>sbox</code> host group affects sandbox VMs (<code>sbox-*</code>).</li> </ul> <pre><code>-  rosetta: false\n+  rosetta: true\n@@\n-  rosetta: false\n+  rosetta: true\n</code></pre>"},{"location":"mac/rosetta/#install-rosetta-in-the-guest-vm-if-wanted","title":"Install Rosetta in the guest VM (if wanted)","text":"<p>If your distribution needs Rosetta and <code>rosetta: true</code>, copy the helper into the guest and run it there.</p> <pre><code>slicer vm cp ./enable-rosetta.sh slicer-1:~/\n</code></pre> <pre><code>slicer vm exec slicer-1 -- bash ~/enable-rosetta.sh\n</code></pre> <p>Keep this off unless you need x86_64 compatibility.</p>"},{"location":"mac/sandboxes/","title":"Sandboxes","text":"<p>The <code>sbox</code> host group is for ephemeral arm64 Linux VMs that you launch on demand. Each sandbox gets its own kernel and filesystem.</p> <p>This is the second half of the Slicer model for Mac:</p> <ul> <li>Services (<code>slicer</code>) are your persistent day-to-day VMs.</li> <li>Sandboxes (<code>sbox</code>) are short-lived, disposable environments for tests and agents.</li> </ul> <p>Use sandboxes when you want fast isolation without changing your persistent environment.</p> <pre><code>+------------------------------------------------------------------+\n|                        macOS Host                                |\n|                                                                  |\n|  +----------------------------+   +---------------------------+  |\n|  | sbox-1 (ephemeral)         |   | sbox-2 (ephemeral)        |  |\n|  | 2 vCPU, 4GB RAM            |   | 2 vCPU, 4GB RAM           |  |\n|  |                            |   |                           |  |\n|  | opencode + git repo        |   | Docker build + test       |  |\n|  | -------------------------- |   | ------------------------- |  |\n|  | Copy repo in, run agent,   |   | Copy Dockerfile in,       |  |\n|  | copy results out           |   | build, run tests          |  |\n|  +----------------------------+   +---------------------------+  |\n|                                                                  |\n|  +-----------------------------------------------------+         |\n|  | slicer-mac daemon (Apple Virtualization)            |         |\n|  | slicer.sock (Unix socket API)                       |         |\n|  +-----------------------------------------------------+         |\n+------------------------------------------------------------------+\n</code></pre>"},{"location":"mac/sandboxes/#launch-use-and-delete","title":"Launch, use, and delete","text":"<p>Launch a sandbox:</p> <pre><code>slicer vm launch sbox\n</code></pre> <p>Check what is running:</p> <pre><code>slicer vm list\n</code></pre> <p>Run a command inside the sandbox:</p> <pre><code>slicer vm exec sbox-1 -- hostname\n</code></pre> <p>Open an interactive shell:</p> <pre><code>slicer vm shell sbox-1\n</code></pre> <p>Stop and remove the sandbox when you are done:</p> <pre><code>slicer vm delete sbox-1\n</code></pre>"},{"location":"mac/sandboxes/#use-cases","title":"Use cases","text":"<p>Sandboxes are designed for short-lived workloads and experimentation:</p> <ul> <li>AI coding agents - copy a repo in, let the agent work, copy results out, then delete if needed.</li> <li>CI and testing - run builds and repeatable test suites in a clean environment.</li> <li>Untrusted code - isolate risky runs from your main services.</li> <li>Comparison builds - run multiple versions of tooling side-by-side without rebuilding your persistent VM.</li> </ul> <p>You can launch sandboxes from the CLI, Go SDK, or REST API.</p>"},{"location":"mac/sandboxes/#customise-sandbox-resources","title":"Customise sandbox resources","text":"<p>Edit the <code>sbox</code> host group in <code>slicer-mac.yaml</code> to change the default vCPU, RAM, or disk size for new sandboxes:</p> <pre><code>- name: sbox\n  count: 0\n  vcpu: 2\n  ram_gb: 4\n  storage_size: 15G\n  rosetta: true\n  network:\n    mode: nat\n    gateway: 192.168.64.1/24\n</code></pre> <p>Restart the daemon after changing the config.</p>"},{"location":"mac/sandboxes/#next-steps","title":"Next steps","text":"<ul> <li>Copy files to/from a VM - use <code>slicer vm cp</code> to move files in and out of sandboxes</li> <li>Execute commands in a VM - run commands remotely with <code>slicer vm exec</code></li> <li>Slicer REST API - automate sandbox lifecycle via the API</li> </ul>"},{"location":"mac/sleep/","title":"Mac sleep behavior","text":"<p>Apple's sleep behaviour can get in the way of running local VMs, whatever solution you happen to use.</p> <p>A fool-proof solution is to run <code>slicer vm suspend slicer-1</code> before closing the laptop lid or putting it into sleep. Then, later you can run <code>slicer vm restore slicer-1</code>. This relies on the suspend/resume functionality of Apple's Virtualization framework, and dumps all memory to disks. Slicer will sync the clock via its guest agent after restoring any suspended VM.</p> <p>Running Slicer for Mac on a laptop means host sleep can affect VM lifecycle.</p> <p>You can set a <code>sleep_action</code> inside each host group in <code>slicer-mac.yaml</code> (<code>slicer</code> and <code>sbox</code> blocks):</p> <pre><code>  - name: slicer\n    count: 1\n    share_home: \"~/\"\n    sleep_action: prevent\n  - name: sbox\n    count: 0\n    sleep_action: prevent\n</code></pre> <p>On a Mac, sleep can start when:</p> <ul> <li>the screen goes dark after idle time,</li> <li>you close the laptop lid,</li> <li>you choose Sleep from the Apple menu,</li> <li>scheduled wake/sleep policies or power settings trigger it.</li> </ul>"},{"location":"mac/sleep/#sleep_action-options","title":"<code>sleep_action</code> options","text":"<ul> <li><code>none</code>: no explicit action.</li> <li><code>shutdown</code>: request VM shutdown on host sleep.</li> <li><code>prevent</code>: keep macOS awake while VMs are running.</li> <li><code>suspend</code>: snapshot VM state and restore on wake.</li> </ul>"},{"location":"mac/sleep/#what-happens","title":"What happens","text":""},{"location":"mac/sleep/#sleep-event","title":"Sleep event","text":"<ul> <li><code>shutdown</code>: request clean VM shutdown.</li> <li><code>suspend</code>: write snapshot state and stop the VM (fallback to shutdown if snapshot is unavailable).</li> <li><code>none</code> / <code>prevent</code>: no shutdown or snapshot action.</li> </ul>"},{"location":"mac/sleep/#wake-event","title":"Wake event","text":"<ul> <li><code>suspend</code>: on wake, wait 5 seconds for the system to settle, then run state-aware restore.</li> <li><code>none</code>, <code>shutdown</code>, <code>prevent</code>: no restore action.</li> </ul>"},{"location":"mac/storage/","title":"Storage on Slicer for Mac","text":"<p>Slicer for Mac stores VM state using a base image flow with copy-on-write clones.</p> <p>You can set storage size per host group with <code>storage_size</code> in <code>slicer-mac.yaml</code>.</p> <p>Use <code>nG</code> style values (for example <code>15G</code>), and set at least <code>5G</code> to give enough initial headroom.</p> <p>Sparse files are used, so disk space is not allocated up front. A declared size of <code>15G</code> represents logical size and grows as data is written.</p>"},{"location":"mac/storage/#imagebootstrap-workflow","title":"Image/bootstrap workflow","text":"<p>Each host group (<code>slicer</code>, <code>sbox</code>) follows the same process:</p>"},{"location":"mac/storage/#first-start","title":"First start","text":"<ol> <li>Pull the image from <code>config.image</code> and unpack it into the local OCI cache.</li> <li>Build the host-group base image (for example <code>./slicer-base.img</code>).</li> <li>Extract the kernel to <code>./kernel/&lt;host_group&gt;/Image</code>.</li> <li>Create the first VM disk from that base image.</li> </ol>"},{"location":"mac/storage/#new-vm-start-warm-path","title":"New VM start (warm path)","text":"<ol> <li>Reuse the OCI cache.</li> <li>Reuse the host-group base image.</li> <li>Clone the VM disk with APFS copy-on-write:</li> </ol> <pre><code>cp -c ./slicer-base.img ./slicer-1.img\n</code></pre> <ol> <li>Start the VM from the clone.</li> </ol> <p>If <code>cp -c</code> is unavailable, the tool falls back to a normal file copy.</p>"},{"location":"mac/storage/#why-storage-sizes-look-smaller-than-expected","title":"Why storage sizes look smaller than expected","text":"<p><code>slicer-base.img</code>, <code>slicer-base</code>, and VM <code>.img</code> files are sparse. A size like <code>15G</code> is a logical size and grows as data is written. Use the <code>nG</code> format and keep the value at least <code>5G</code>.</p>"},{"location":"mac/storage/#rebuild-or-reset-local-state","title":"Rebuild or reset local state","text":"<p>To force a full rebuild from OCI:</p> <pre><code>rm -f ./slicer-base.img\nrm -rf ./kernel/slicer ./kernel/sbox\nrm -rf ./oci-cache\n</code></pre> <p>To remove a single VM image and recreate it from the host-group base image:</p> <pre><code>rm -f ./slicer-1.img\n</code></pre>"},{"location":"mac/storage/#troubleshooting","title":"Troubleshooting","text":"<p>If your VM crashes for some reason, or the Mac's sleep mode causes an issue with the VM, you may find the <code>.img</code> file needs to be checked or repaired with the <code>e2fsck</code> utility.</p> <pre><code>e2fsck -f ./slicer-1.img\n</code></pre> <p>You can obtain <code>e2fsck</code> by installing the <code>e2fsprogs</code> package via <code>brew install e2fsprogs</code>. Brew is available separately at: https://brew.sh/.</p>"},{"location":"mac/tray-integration/","title":"Tray integration","text":"<p>The tray app gives you quick access to VM status, logs, and shell access.</p> <p>Start the tray and connect it to your local socket:</p> <pre><code>./slicer-tray\n</code></pre> <p>By default, the tray opens shells in Terminal.app. Use another terminal if you prefer:</p> <pre><code>./slicer-tray --terminal \"ghostty\"\n</code></pre> <p></p>"},{"location":"reference/api/","title":"Slicer REST API","text":"<p>This page documents the HTTP API exposed by Slicer for managing micro-VMs, images, disks, and operations. Some endpoints will fail if a trailing slash is given, i.e. <code>/nodes</code> is documented, however <code>/nodes/</code> may return an error.</p> <p>A note on Slicer Services vs Slicer Sandboxes:</p> <ul> <li>A Slicer Service is a VM launched via YAML - protected from VM deletion - disk persistent is controlled via the <code>persistent</code> flag in the YAML config. <code>storage: image</code> is persistent by default.</li> <li>A Slicer Sandbox is a VM launched via API - and is ephemeral/disposable. It can be deleted, and disk/snapshot is cleaned up on shutdown of the VM or deletion. Shutting down Slicer will terminate and delete all sandboxes.</li> </ul>"},{"location":"reference/api/#authentication","title":"Authentication","text":"<p>No authentication, ideal for local/dev work:</p> <pre><code>config:\n  api:\n    bind_address: 127.0.0.1\n    port: 8080\n</code></pre> <p>For production:</p> <pre><code>config:\n  api:\n    bind_address: 127.0.0.1\n    port: 8080\n    auth:\n      enabled: true\n</code></pre> <p>The token will be saved to: <code>/var/lib/slicer/auth/token</code>.</p> <p>Send an <code>Authorization: Bearer TOKEN</code> header for authenticated Slicer daemons.</p> <p>If you intend to expose the Slicer API over the Internet using something like a self-hosted inlets tunnel, or Inlets Cloud, then make sure you use the \"we terminate TLS for you\" option.</p> <p>i.e.</p> <pre><code>DOMAIN=example.com\ninletsctl create \\\n    slicer-api \\\n    --letsencrypt-domain $DOMAIN \\\n    --letsencrypt-email webmaster@$DOMAIN\n</code></pre> <p>Alternatively, if Slicer is on a machine that's public facing, i.e. on Hetzner or another bare-metal cloud, you can use Caddy to terminate TLS. Make sure the API is bound to 127.0.0.1:</p> <p>Caddyfile:</p> <pre><code>{\n    email \"webmaster@example.com\"\n    # Uncomment to try the staging certificate issuer first\n    #acme_ca https://acme-staging-v02.api.letsencrypt.org/directory\n    # The production issuer:\n    acme_ca https://acme-v02.api.letsencrypt.org/directory\n }\n\nslicer-1.example.com {\n reverse_proxy 127.0.0.1:8080\n}\n</code></pre> <p>You can install Caddy via <code>arkade system install caddy</code></p>"},{"location":"reference/api/#conventions","title":"Conventions","text":"<ul> <li>Content-Type: application/json unless noted.</li> <li>Timestamps: RFC3339.</li> <li>IDs: opaque strings returned by the API.</li> <li>Errors: JSON: { \"error\": \"message\", \"code\": \"optional_code\" } with appropriate HTTP status.</li> </ul>"},{"location":"reference/api/#get-info","title":"GET /info","text":"<p>Get server version information.</p> <p>Response 200:</p> <pre><code>{ \"version\": \"1.0.0\", \"git_commit\": \"abcdef123\" }\n</code></pre>"},{"location":"reference/api/#get-healthz","title":"GET /healthz","text":"<p>Check service liveness.</p> <p>Response 200: <pre><code>{ \"status\": \"ok\" }\n</code></pre></p>"},{"location":"reference/api/#slicer-agent-health","title":"Slicer Agent Health","text":"<p>The VM Agent provides:</p> <ul> <li>Serial console/Shell sessions</li> <li>File copying</li> <li>Remote command execution</li> <li>Health check/readiness of the agent itself</li> </ul> <p>So before accessing any of these endpoints, it makes sense to first check whether the agent is healthy and accessible, especially if the VM is being booted up via code, then automated immediately afterwards.</p> <p>For automation, you can use the <code>HEAD</code> method to simply check if the agent is started.</p> <p>To get additional data like System Uptime, Agent Uptime and Agent Version, use a HTTP <code>GET</code> method.</p> <p><code>/vm/{hostname}/health</code></p> <p>Response 200:</p> <pre><code>{\n  \"hostname\": \"vm-1\",\n  \"agent_uptime\": 3600,\n  \"agent_version\": \"0.1.0\",\n  \"system_uptime\": 7200,\n  \"userdata_ran\": true\n}\n</code></pre>"},{"location":"reference/api/#list-nodes","title":"List nodes","text":"<p>HTTP GET</p> <p><code>/nodes</code></p> <pre><code>[{\"hostname\":\"vm-1\",\"ip\":\"192.168.137.2\",\"created_at\":\"2025-09-02T08:32:37.667253315+01:00\"}]\n</code></pre>"},{"location":"reference/api/#list-host-groups","title":"List Host Groups","text":"<p>HTTP GET</p> <p><code>/hostgroup</code></p> <p>Response 200:</p> <pre><code>[{\"name\":\"vm\",\"count\":1,\"ram_bytes\":4294967296,\"cpus\":2,\"gpu_count\":0,\"arch\":\"x86_64\"}]\n</code></pre>"},{"location":"reference/api/#create-a-new-host-within-a-host-group","title":"Create a new host within a Host Group","text":"<p>HTTP POST</p> <p><code>/hostgroup/NAME/nodes</code></p> <p>Add a host with the defaults:</p> <pre><code>{\n}\n</code></pre> <p>Or add a host with userdata:</p> <p>Make sure the string for userdata is JSON encoded.</p> <pre><code>{\n  \"userdata\": \"sudo apt update &amp;&amp; sudo apt upgrade\"\n}\n</code></pre> <p>Add a host with a custom GitHub user to override the SSH keys:</p> <pre><code>{\n    \"github_user\": \"your_github_username\"\n}\n</code></pre>"},{"location":"reference/api/#list-nodes-in-a-host-group","title":"List nodes in a Host Group","text":"<p>HTTP GET</p> <p><code>/hostgroup/NAME/nodes</code></p> <p>Response 200:</p> <pre><code>[{\"hostname\":\"vm-1\",\"ip\":\"192.168.137.2\",\"ram_bytes\":2147483648,\"cpus\":2,\"created_at\":\"2025-09-02T08:32:37.667253315+01:00\",\"arch\":\"x86_64\",\"status\":\"Running\"}]\n</code></pre>"},{"location":"reference/api/#get-serial-console-logs-from-a-vm","title":"Get serial console logs from a VM","text":"<p>HTTP GET</p> <p><code>GET /vm/{hostname}/logs?lines=&lt;n&gt;</code></p> <p>When <code>n</code> is empty, a default of 20 lines will be read from the end of the log.</p> <p>When <code>n</code> is <code>0</code>, the whole contents will be returned.</p> <p>Response 200:</p> <pre><code>{\"hostname\":\"vm-1\",\"lines\":20,\"content\":\"[  OK  ] Started slicer-ssh-agent.\\n[  OK  ] Started slicer-vmmeter.\\n\"}\n</code></pre>"},{"location":"reference/api/#delete-a-host-within-a-host-group","title":"Delete a host within a Host Group","text":"<p>HTTP DELETE</p> <p><code>/hostgroup/NAME/nodes/HOSTNAME</code></p> <p>Response 200:</p> <pre><code>{\"message\":\"Node deleted\",\"disk_removed\":\"true\"}\n</code></pre>"},{"location":"reference/api/#get-node-consumption-details","title":"Get node consumption details","text":"<p>Providing that the <code>slicer-vmmeter</code> service is running in your VM, detailed usage and consumption metrics can be obtained.</p> <p>HTTP GET</p> <p><code>/nodes/stats</code></p> <pre><code>[\n  {\n    \"hostname\": \"vm-1\",\n    \"ip\": \"192.168.137.2\",\n    \"created_at\": \"2025-09-02T08:32:37.667253315+01:00\",\n    \"snapshot\": {\n      \"hostname\": \"vm-1\",\n      \"arch\": \"x86_64\",\n      \"timestamp\": \"2025-09-02T07:39:32.388239809Z\",\n      \"uptime\": \"6m53s\",\n      \"totalCpus\": 2,\n      \"totalMemory\": 4024136000,\n      \"memoryUsed\": 220184000,\n      \"memoryAvailable\": 3803952000,\n      \"memoryUsedPercent\": 5.471584459372148,\n      \"loadAvg1\": 0,\n      \"loadAvg5\": 0,\n      \"loadAvg15\": 0,\n      \"diskReadTotal\": 62245888,\n      \"diskWriteTotal\": 7815168,\n      \"networkReadTotal\": 44041,\n      \"networkWriteTotal\": 19872,\n      \"diskIOInflight\": 0,\n      \"openConnections\": 0,\n      \"openFiles\": 480,\n      \"entropy\": 256,\n      \"diskSpaceTotal\": 26241896448,\n      \"diskSpaceUsed\": 820826112,\n      \"diskSpaceFree\": 24062115840,\n      \"diskSpaceUsedPercent\": 3.12792222782572\n    }\n  }\n]\n</code></pre>"},{"location":"reference/api/#get-node-consumption-details-for-a-single-vm","title":"Get node consumption details for a single VM","text":"<p>HTTP GET</p> <p><code>/node/{hostname}/stats</code></p> <p>Response 200:</p> <pre><code>{\n  \"hostname\": \"vm-1\",\n  \"ip\": \"192.168.137.2\",\n  \"created_at\": \"2025-09-02T08:32:37.667253315+01:00\",\n  \"snapshot\": {\n    \"hostname\": \"vm-1\",\n    \"arch\": \"x86_64\",\n    \"timestamp\": \"2025-09-02T07:39:32.388239809Z\",\n    \"uptime\": \"6m53s\",\n    \"totalCpus\": 2,\n    \"totalMemory\": 4024136000,\n    \"memoryUsed\": 220184000,\n    \"memoryAvailable\": 3803952000,\n    \"memoryUsedPercent\": 5.471584459372148\n  }\n}\n</code></pre>"},{"location":"reference/api/#copy-files-to-and-from-the-microvm","title":"Copy files to and from the microVM","text":"<p>HTTP POST/GET</p> <p><code>/vm/{hostname}/cp</code></p> <p>Copy files to/from a VM. This endpoint requires the <code>slicer-agent</code> service to be running in the target VM.</p> <p>Files or directories are always streamed, and not buffered in memory or on disk.</p> <ul> <li>Files are copied using <code>binary</code> mode, where the request or response body is the binary data of the file being copied.</li> <li>Directories can be copied using <code>tar</code> without compression. When copying folders in either direction, very little metadata is preserved, so you may also want to combine additional options such as <code>uid</code> or <code>gid</code>, or run your own <code>chown</code> via the exec endpoint after the copy.</li> </ul> <p>For the highest level of fidelity, you can create your own tar and copy it as a file. </p>"},{"location":"reference/api/#copy-a-file-or-folder-to-the-vm","title":"Copy a file or folder to the VM","text":"<p>Copy a file to the VM</p> <ul> <li>Content-Type: <code>application/octet-stream</code></li> <li>Body: binary data of file to copy</li> <li>Query parameters:</li> <li><code>path</code> (required): destination path in VM</li> <li><code>uid</code> (optional): user ID for file ownership</li> <li><code>gid</code> (optional): group ID for file ownership</li> <li><code>mode</code> (optional): <code>binary</code> or <code>tar</code> (defaults to <code>binary</code>)</li> <li><code>permissions</code> (optional): permissions for copied files (e.g. <code>0644</code>)</li> </ul> <p>Copy a directory to the VM</p> <ul> <li>Content-Type: <code>application/x-tar</code></li> <li>Body: tar stream of files to copy</li> <li>Query parameters:</li> <li><code>path</code> (required): destination path in VM</li> <li><code>uid</code> (optional): user ID for file ownership</li> <li><code>gid</code> (optional): group ID for file ownership</li> <li><code>mode</code> (optional): <code>tar</code> (recommended when sending tar content)</li> <li><code>permissions</code> (optional): permissions for copied files (e.g. <code>0644</code>)</li> </ul>"},{"location":"reference/api/#copy-a-file-or-folder-from-the-microvm","title":"Copy a file or folder from the microVM","text":"<p>The same applies as copying a file to the VM, however it works in the reverse. Instead of sending a <code>Content-Type</code> header, an <code>Accept</code> header is used to indicate whether to use <code>tar</code> mode or <code>binary</code> mode.</p> <p>Copy a file to the client</p> <ul> <li>Accept: <code>application/octet-stream</code></li> <li>Query parameters:</li> <li><code>path</code> (required): destination path in VM</li> <li><code>mode</code> (optional): <code>binary</code> or <code>tar</code></li> <li><code>permissions</code> (optional): permissions for copied files (e.g. <code>0644</code>)</li> <li>Response: binary data of file</li> </ul> <p>Copy a directory to the client</p> <ul> <li>Accept: <code>application/octet-stream</code></li> <li>Query parameters:</li> <li><code>path</code> (required): destination path in VM</li> <li><code>mode</code> (optional): <code>tar</code> (recommended when receiving tar content)</li> <li>Response: binary data of directory</li> </ul>"},{"location":"reference/api/#execute-commands","title":"Execute commands","text":"<p>HTTP POST</p> <p><code>/vm/{hostname}/exec</code></p> <p>Execute commands on a VM. Requires the <code>slicer-agent</code> service running in the guest VM.</p> <p>Query parameters: - <code>cmd</code> (required): command to execute - <code>args</code> (optional, multiple): command arguments - <code>uid</code> (optional): user ID to run command as (default: 0) - <code>gid</code> (optional): group ID to run command as (default: 0) - <code>cwd</code> (optional): working directory - <code>shell</code> (optional): shell interpreter (default: /bin/bash) - <code>stdin</code> (optional): enable stdin pipe (true/false) - <code>stdout</code> (optional): enable stdout capture (true/false) - <code>stderr</code> (optional): enable stderr capture (true/false) - <code>permissions</code> (optional): permissions for the command</p> <p>Body: stdin data (if <code>stdin=true</code>)</p> <p>Response: streaming newline-delimited JSON with stdout/stderr/exit_code</p> <p>Example response stream:</p> <pre><code>{\"timestamp\":\"2025-09-02T07:39:32.388239809Z\",\"stdout\":\"Hello from VM\\n\",\"stderr\":\"\",\"exit_code\":0}\n{\"timestamp\":\"2025-09-02T07:39:32.488239809Z\",\"stdout\":\"\",\"stderr\":\"some error\\n\",\"exit_code\":0}\n{\"timestamp\":\"2025-09-02T07:39:32.588239809Z\",\"stdout\":\"\",\"stderr\":\"\",\"exit_code\":0}\n</code></pre> <p>Each JSON object is separated by a newline character.</p> <p>Note that the <code>exit_code</code> variable is an integer so will always be populated with <code>0</code>. This is a limitation of JSON serialisation, so to determine the actual exit code, wait until the TCP connection has been disconnected, and keep track of the final exit code value that is received.</p> <p>The <code>error</code> variable may also be populated if there was a problem running, starting, or finding the requested binary or shell to execute. For example:</p> <pre><code>{\"error\": \"Some error message\", \"exit_code\": 1}\n</code></pre>"},{"location":"reference/api/#manage-secrets","title":"Manage secrets","text":"<p>HTTP GET</p> <p><code>/secrets</code></p> <pre><code>[\n  {\n    \"name\": \"my-secret\",\n    \"size\": 1024,\n    \"permissions\": \"0600\",\n    \"uid\": 1000,\n    \"gid\": 1000,\n    \"mod_time\": \"2025-09-02T08:32:37.667253315+01:00\"\n  }\n]\n</code></pre> <p>HTTP POST</p> <p><code>/secrets</code></p> <p>Create a secret with base64-encoded data:</p> <pre><code>{\n  \"name\": \"my-secret\",\n  \"data\": \"base64-encoded-content\",\n  \"permissions\": \"0600\",\n  \"uid\": 1000,\n  \"gid\": 1000\n}\n</code></pre> <p>HTTP PATCH</p> <p><code>/secrets/{name}</code></p> <p>Update an existing secret:</p> <pre><code>{\n  \"data\": \"base64-encoded-content\",\n  \"permissions\": \"0644\",\n  \"uid\": 1001,\n  \"gid\": 1001\n}\n</code></pre> <p>HTTP DELETE</p> <p><code>/secrets/{name}</code></p> <p>Response 200: No content</p>"},{"location":"reference/api/#obtain-an-interactive-shell-serial-console","title":"Obtain an interactive shell / serial console","text":"<p>HTTP GET</p> <p>This is endpoint is used by <code>slicer vm shell</code> to obtain a shell without needing SSH to be listening in the VM, or to be exposed on any network adapters. It replaces the concept of a serial console from a traditional headless server. No routable access to the VM or its subnet is required, only the REST API of Slicer.</p> <p>This endpoint is only available in a VM if the <code>slicer-agent</code> service is running. The CLI is the only official client compatible with this functionality.</p> <p><code>/vm/{hostname}/shell</code></p> <p>Query parameters: - <code>uid</code> (optional): user ID to run shell as - <code>gid</code> (optional): group ID to run shell as - <code>shell</code> (optional): shell interpreter - <code>cwd</code> (optional): working directory</p>"},{"location":"reference/api/#agent-health-no-body","title":"Agent health (no body)","text":"<p>HTTP HEAD</p> <p><code>/vm/{hostname}/health</code></p> <p>Response 200: No body</p>"},{"location":"reference/api/#shutdown-or-reboot-a-vm","title":"Shutdown or reboot a VM","text":"<p>HTTP POST</p> <p><code>/vm/{hostname}/shutdown</code></p> <p>Query parameters: - <code>action</code> (optional): <code>shutdown</code> or <code>reboot</code></p> <p>Response 200: text/plain</p>"},{"location":"reference/api/#pause-a-vm","title":"Pause a VM","text":"<p>HTTP POST</p> <p><code>/vm/{hostname}/pause</code></p> <p>Response 200: text/plain</p>"},{"location":"reference/api/#resume-a-vm","title":"Resume a VM","text":"<p>HTTP POST</p> <p><code>/vm/{hostname}/resume</code></p> <p>Response 200: text/plain</p>"},{"location":"reference/api/#forward-tcp-connections","title":"Forward TCP connections","text":"<p>HTTP GET</p> <p><code>/vm/{hostname}/forward</code></p> <p>This endpoint is used by the inlets client to establish TCP tunnels via the VM agent.</p>"},{"location":"reference/api/#prometheus-metrics","title":"Prometheus metrics","text":"<p>HTTP GET</p> <p><code>/metrics</code></p>"},{"location":"reference/images/","title":"Images for Slicer MicroVMs","text":"<p>Images for Slicer are built with a Dockerfile, and can be extended with additional layers.</p> <p>It is also possible to build completely different images from scratch, however this is not recommended or documented at this time. So if you need a different OS, reach out to the team via Discord and request it there.</p> <p>Cloud Hypervisor is required to mount a PCI device such as an Nvidia GPU into a microVM. In all other cases, Firecracker is preferred.</p> <p>Generally, Slicer makes use of systemd to run, tune, and manage services and background agents. It is technically possible to use other init systems such as openrc, reach out if this is a core requirement for your usage.</p> <p>Image availability:</p> Operating System Firecracker (x86_64) Firecracker (arm64) Cloud Hypervisor (x86_64) Cloud Hypervisor (arm64) Ubuntu 22.04 Ubuntu 24.04 Rocky Linux 9 Arch Linux <p>Ubuntu 22.04 is supported by Cannoncial is supported until April 2027.</p> <p>Table of image tags:</p> Operating System Firecracker (x86_64) Firecracker (arm64) Cloud Hypervisor (x86_64) Cloud Hypervisor (arm64) Default user Ubuntu 22.04 <code>ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-latest</code>, <code>ghcr.io/openfaasltd/slicer-systemd:6.1.90-x86_64-latest</code> <code>ghcr.io/openfaasltd/slicer-systemd-arm64:6.1.90-aarch64-latest</code> <code>ghcr.io/openfaasltd/slicer-systemd-ch:5.10.240-x86_64-latest</code>,  <code>ghcr.io/openfaasltd/slicer-systemd-ch:6.1.90-x86_64-latest</code> <code>ubuntu</code> Ubuntu 24.04 <code>ghcr.io/openfaasltd/slicer-systemd-2404:5.10.240-x86_64-latest</code> <code>ubuntu</code> Rocky Linux 9 <code>ghcr.io/openfaasltd/slicer-systemd-rocky9:5.10.240-x86_64-latest</code> <code>slicer</code> Arch Linux <code>ghcr.io/openfaasltd/slicer-systemd-archlinux:6.1.90-x86_64-latest</code> <code>slicer</code> The most tested and stable images for x86_64 are based upon the <code>5.10.x</code> Kernel. Feel free to use the <code>6.9.x</code> Kernels and report any discrepancies you may find."},{"location":"reference/networking/","title":"Networking modes for Slicer microVMs","text":"<p>Table of contents:</p> <ul> <li>Bridge Networking</li> <li>CNI Networking</li> <li>Isolated Mode Networking</li> <li>Quick start</li> <li>Specifying an explicit range</li> <li>Drop and allow rules</li> <li>Firewall</li> <li>Additional configuration for Netplan</li> </ul> <p>Both Firecracker and Cloud Hypervisor microVMs use TAP devices for networking. A Linux TAP operates at Layer 2 (Data Link Layer), one end is attached to the microVM, and the other end is attached to the host system.</p> <p>Slicer supports multiple networking modes for microVMs, each with their own pros and cons. Generally, you should use the default option (Bridge Networking) unless you have specific requirements that need a different mode.</p> <p>See <code>slicer new --help</code> to generate a Slicer config with bridge-based networking. You can also provide a CIDR block to allocate IP addresses from, if you need to run Slicer across different machines and have them all be routable on the local network.</p>"},{"location":"reference/networking/#bridge-networking","title":"Bridge Networking","text":"<p>The default for Slicer is to use create a Linux bridge per Slicer daemon, and to attach all microVMs within that hostgroup to the bridge. This allows microVMs to communicate with each other and with the host system.</p> <p>The bridge also makes the microVMs routable from outside the host system on the local network, subject to the other machine having direct access to the machine where the Slicer daemon is running, and adding a route to the microVM subnet.</p> <p>Pros:</p> <ul> <li>Default mode, and best tested option</li> <li>Easy to route to microVMs from outside the host system</li> <li>Allocate a fixed set of IP addresses, or allocate from a larger subnet</li> </ul> <p>Cons:</p> <ul> <li>microVMs can communicate with the host, and the rest of the LAN, which may not be desirable in some use-cases</li> <li>There can be learning delays with a Linux bridge, so occasionally a microVM may boot up and not be able to resolve DNS for a few seconds</li> </ul> <p>Example with IPs allocated from the subnet automatically:</p> <pre><code>config:\n  host_groups:\n  - name: vm\n    count: 3\n    network:\n      bridge: brvm0\n      tap_prefix: vmtap\n      gateway: 192.168.137.1/24\n</code></pre> <p>To assign from a static list of IP addresses, use the <code>addresses</code> field, and make sure the <code>gateway</code> is in the same subnet:</p> <pre><code>config:\n  host_groups:\n  - name: vm\n    count: 3\n    network:\n      bridge: brvm0\n      tap_prefix: vmtap\n      gateway: 192.168.137.1/24\n      addresses:\n      - 192.168.137.2/24\n      - 192.168.137.3/24\n      - 192.168.137.4/24\n</code></pre>"},{"location":"reference/networking/#cni-networking","title":"CNI Networking","text":"<p>Container Network Interface (CNI) is a popular networking standard in the container ecosystem, and is used by OpenFaaS Edge (faasd), Kubernetes, and various other low-level projects.</p> <p>Slicer ships with a CNI configuration that acts in a similar way to the bridge mode. Just like with the bridge mode, microVMs can communicate with each other and with the host system, and are routable from outside the host system on the local network.</p> <p>The combination of <code>bridge</code>, and <code>firewall</code> CNI plugins are used in the default configuration, however you can also change this as needed for instance to bypass the <code>bridge</code> plugin and use something simpler like <code>ptp</code>.</p> <p>Pros:</p> <ul> <li>Network addresses are allocated from a large, fixed pool defined in a config file</li> </ul> <p>Cons:</p> <ul> <li>Reliance on CNI to manage networking</li> <li>microVMs can communicate with the host, and the rest of the LAN, which may not be desirable in some use-cases</li> <li>May also have learning delays like the bridge mode</li> </ul> <p>Example:</p> <pre><code>config:\n  host_groups:\n    - name: vm\n  network_name: \"slicer\"\n</code></pre> <p>To use CNI, leave off the <code>network</code> section of the hostgroup.</p> <p>The <code>slicer</code> network is defined at: <code>/etc/cni/net.d/51-slicer.conflist</code> and can be edited. Additional named configs can be created and selected by changing the <code>network_name</code> field in the hostgroup config.</p>"},{"location":"reference/networking/#isolated-mode-networking","title":"Isolated Mode Networking","text":"<p>In this mode, each microVM's TAP is created in a private network namespace, then connected to the host via a veth pair. Each VM is fully isolated from the others, from the host, and from the LAN.</p> <p>Pros:</p> <ul> <li>Built-in mechanism to block / drop all traffic to specific destinations</li> <li>microVMs cannot communicate with each other</li> <li>microVMs cannot communicate with the host system</li> <li>microVMs cannot communicate with the rest of the LAN</li> </ul> <p>Cons:</p> <ul> <li>Newer mode, less tested than bridge or CNI modes</li> <li>Additional complexity in managing the network namespaces and cleaning up all resources in error conditions or crashes</li> <li>Maximum node group name including the suffix <code>-</code> and a number, can't be longer than 15 characters. I.e. <code>agents-1</code> up to <code>agents-1000</code> is fine, but <code>isolated-agents-1</code> would not fit.</li> </ul>"},{"location":"reference/networking/#quick-start","title":"Quick start","text":"<p>The easiest way to use isolated mode is with <code>slicer new</code> \u2014 no IP range is needed:</p> <pre><code>slicer new sandbox --net=isolated\n</code></pre> <p>This generates a minimal config with no <code>range:</code> field. Slicer automatically derives a <code>/22</code> subnet within <code>169.254.0.0/16</code> from the hostgroup name. Each <code>/22</code> provides 256 usable <code>/30</code> blocks (one per VM).</p> <p>To add firewall rules at generation time:</p> <pre><code>slicer new sandbox --net=isolated --drop 192.168.1.0/24\n</code></pre> <p>The generated YAML looks like:</p> <pre><code>config:\n  host_groups:\n    - name: sandbox\n      network:\n        mode: \"isolated\"\n        drop: [\"192.168.1.0/24\"]\n</code></pre> <p>This is safe to use with multiple Slicer daemons running on the same host \u2014 each hostgroup name hashes to a different subnet, and Slicer checks for IP collisions on host interfaces before assigning each <code>/30</code> block, skipping any that are already in use.</p>"},{"location":"reference/networking/#specifying-an-explicit-range","title":"Specifying an explicit range","text":"<p>If you prefer to control the exact subnet, you can specify a <code>range:</code> field. This is useful when you want deterministic IP assignments or need to coordinate ranges across many host groups manually.</p> <pre><code>config:\n  host_groups:\n    - name: sandbox\n      network:\n        mode: \"isolated\"\n        range: \"169.254.100.0/22\"\n        drop: [\"192.168.1.0/24\"]\n</code></pre> <p>A <code>/22</code> range gives 256 usable <code>/30</code> blocks. Each <code>/30</code> contains 4 IPs: network, gateway, host (microVM), and broadcast.</p> <p>When using explicit ranges with multiple host groups or multiple Slicer daemons on the same host, use non-overlapping subnets:</p> <ul> <li><code>169.254.100.0/22</code></li> <li><code>169.254.104.0/22</code></li> <li><code>169.254.108.0/22</code></li> <li><code>169.254.112.0/22</code></li> </ul> <p>When Slicer is running on different hosts, you can re-use the same subnet ranges on different machines.</p> <p>You can also pass an explicit range via <code>slicer new</code>:</p> <pre><code>slicer new sandbox --net=isolated --isolated-range 169.254.100.0/22\n</code></pre>"},{"location":"reference/networking/#drop-and-allow-rules","title":"Drop and allow rules","text":"<p>The <code>drop</code> list contains CIDR blocks that should be blocked for all microVMs in this hostgroup. In the example above, all microVMs will have all traffic to the standard LAN network <code>192.168.1.0/24</code> dropped before it has a chance to leave the private network namespace.</p>"},{"location":"reference/networking/#firewall","title":"Firewall","text":"<p>There is both a <code>drop</code> and an <code>allow</code> list that can be given in the networking section.</p> <p>When only <code>drop</code> is given, all other traffic is allowed which hasn't been explicitly blocked.</p> <p>When only <code>allow</code> is given, all other traffic is blocked which hasn't been explicitly allowed.</p> <p>When neither <code>drop</code>, nor <code>allow</code> are given, then all traffic is allowed.</p>"},{"location":"reference/networking/#additional-configuration-for-netplan","title":"Additional configuration for Netplan","text":"<p>On Ubuntu 22.04 (Server), netplan can take over the veth pair that Slicer creates for the isolated network mode. NetworkManager doesn't tend to have this issue and ships with Ubuntu Desktop.</p> <p>If you run into issues (confirmed by <code>ip addr</code> showing no IP on the <code>ve-</code> interfaces), run the following:</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/systemd/network/00-veth-ignore.network &gt; /dev/null\n[Match]\nName=ve-* veth*\nDriver=veth\n\n[Link]\nUnmanaged=yes\n\n[Network]\nKeepConfiguration=yes\nEOF\n</code></pre> <p>Update <code>/etc/systemd/networkd.conf</code> as per the following:</p> <pre><code>cat &lt;&lt;EOF | sudo tee /etc/systemd/networkd.conf &gt; /dev/null\n[Network]\nKeepConfiguration=yes\nManageForeignRoutes=no\n#SpeedMeter=no\n#SpeedMeterIntervalSec=10sec\n#ManageForeignRoutingPolicyRules=yes\n#ManageForeignRoutes=yes\n#RouteTable=\n#IPv6PrivacyExtensions=no\n\n[DHCPv4]\n#DUIDType=vendor\n#DUIDRawData=\n\n[DHCPv6]\n#DUIDType=vendor\n#DUIDRawData=\nEOF\n</code></pre> <p>Then reload and restart Slicer and the isolated network mode microVMs:</p> <pre><code>sudo chmod 644 /etc/systemd/network/00-veth-ignore.network\nsudo systemctl restart systemd-networkd\n</code></pre>"},{"location":"reference/secrets/","title":"Secrets Management","text":"<p>Slicer provides a secure secrets management system that enables sharing of sensitive data between the host and guest VMs through a VSOCK-based secrets server. Secrets are stored on the host and synchronized to guest VMs via the <code>slicer-agent</code> service.</p>"},{"location":"reference/secrets/#common-use-cases","title":"Common Use Cases","text":"<p>The secrets system is ideal for securely sharing database credentials, API keys and tokens, TLS certificates and private keys, SSH keys, and other sensitive configuration data between the host and guest VMs.</p>"},{"location":"reference/secrets/#how-it-works","title":"How it works","text":"<ol> <li> <p>Secrets are created manually or through the Slicer CLI or API. Secrets are stored securely on the host filesystem in the <code>.secrets</code> folder.</p> </li> <li> <p>A HTTP secrets server listing on the host VSOCK is started when launching each VM.</p> </li> <li> <p>The <code>slicer-agent</code> service running in the guest VM connects to the host via VSOCK and synchronises secrets to the guest VM filesystem at <code>/run/slicer/secrets</code>.</p> </li> </ol>"},{"location":"reference/secrets/#create-secrets","title":"Create secrets","text":"<p>Create the <code>.secrets</code> directory if it does not exist and give it the correct permissions.</p> <pre><code>sudo mkdir .secrets\n# Ensure only root can read/write to the secrets folder.\nsudo chmod 700 .secrets\n</code></pre> <p>Create a new secret file:</p> <pre><code>echo \"my-api-key\" | sudo tee .secrets/api-key &gt; /dev/null\n</code></pre> <p>Permission and ownership are mapped 1:1 to the guest VM filesystem. Change permissions and ownership accordingly to manage who has access to secrets within the VM:</p> <pre><code>sudo chmod 600 .secrets/api-key\n\n# Optionally change ownership\nsudo chown 1000:1000 .secrets/api-key\n</code></pre> <p>It is recommended to change the permission so only the owner can read/write and nobody else can access the secret. In the example above we changed the owner and group to 1000 which means the secret will be readable by the default user in the VM.</p> <p>After launching a VM, the secrets will be available in the guest VM filesystem at <code>/run/slicer/secrets</code>.</p> <p>Laucnh a VM with the secrets from the example and list secrets:</p> <pre><code>ls -l /run/slicer/secrets/\n</code></pre> <p>Should give an output similar to:</p> <pre><code>total 4\n-rw------- 1 ubuntu ubuntu 11 Dec  8 11:12 api-key\n</code></pre>"},{"location":"reference/secrets/#managing-secrets-with-the-cli","title":"Managing Secrets with the CLI","text":"<p>The <code>slicer secret</code> command provides a complete interface for managing secrets.</p> <p>Note that the Slicer API needs to be running to manage secrets using the CLI.</p>"},{"location":"reference/secrets/#create-a-secret","title":"Create a secret","text":"<p>To create a secret from stdin run:</p> <pre><code>echo \"my secret value\" | slicer secret create my-secret\n</code></pre> <p>Or pipe the value from a file instead:</p> <pre><code>cat /path/to/secret.txt | slicer secret create my-secret\n</code></pre> <p>Create a secret from a literal value:</p> <pre><code>slicer secret create my-secret --from-literal=\"my secret value\"\n</code></pre> <p>Create a secret from a file:</p> <pre><code>slicer secret create my-secret --from-file=/path/to/secret.txt\n</code></pre> <p>Create a secret with custom permissions and ownership:</p> <pre><code>slicer secret create my-secret \\\n  --from-file=/path/to/secret.txt \\\n  --permissions=0600 \\\n  --uid=1000 \\\n  --gid=1000\n</code></pre> <p>Secrets can be assigned to specific users and groups using the <code>--uid</code> and <code>--gid</code> flags and permissions can be changed using the <code>--permissions</code> flag. This is useful when secrets need to be accessible by specific system users or services within the guest VM.</p> <p>By default secrets created using the CLI are owned by root and have permissions set to 0600.</p>"},{"location":"reference/secrets/#list-secrets","title":"List secrets","text":"<p>View all secrets:</p> <pre><code>slicer secret list\n</code></pre> <p>Output:</p> <pre><code>NAME\n----\nmy-secret\n</code></pre> <p>Use the <code>-v</code> flag to view more details like permissions and ownership:</p> <pre><code>slicer secret list -v\n</code></pre> <p>Output:</p> <pre><code>NAME                      SIZE       PERMISSIONS  UID      GID\n----                      ----       -----------  ---      ---\nmy-secret                 11         0600         0        0\n</code></pre>"},{"location":"reference/secrets/#update-a-secret","title":"Update a secret","text":"<p>Update a secret from a file:</p> <pre><code>slicer secret update my-secret --from-file=/path/to/newsecret.txt\n</code></pre> <p>Update permissions and ownership:</p> <pre><code>slicer secret update my-secret \\\n  --permissions=0644 \\\n  --uid=1001 \\\n  --gid=1001\n</code></pre>"},{"location":"reference/secrets/#remove-a-secret","title":"Remove a secret","text":"<p>Remove a secret by name:</p> <pre><code>slicer secret remove my-secret\n</code></pre>"},{"location":"reference/secrets/#scoped-secret-access","title":"Scoped secret access","text":"<p>You might not always want to sync all secrets to every VM. Slicer allows you to select which secrets should be synced when launching a VM using the CLI.</p> <p>For example, to make only the <code>my-secret</code> secret available but no other secrets run:</p> <pre><code>slicer vm launch --secret=my-secret\n</code></pre>"},{"location":"reference/secrets/#sync-secrets-after-vm-boot","title":"Sync secrets after VM boot.","text":"<p>If secrets have been added, updated or removed after a VM has been started, they can be re-synced by running the following command in the Slicer VM:</p> <pre><code>sudo slicer-agent secrets sync\n</code></pre>"},{"location":"reference/secrets/#guest-vm-requirements","title":"Guest VM Requirements","text":"<p>The <code>slicer-agent</code> daemon must be running within the guest VM to support secrets. This is included as a default in the various supported images.</p>"},{"location":"reference/sos/","title":"Slicer's Serial Over SSH console (SOS)","text":"<p>Slicer's Serial Over SSH (SOS) console allows you manage VMs without having an OpenSSH server running, or an active network connection. SOS works over VSOCK, which is a virtual serial console provided by Firecracker and Cloud Hypervisor.</p> <p>SSH can be convenient for remote access, but depends on an OpenSSH server being installed and running within the VM. On first boot, an operational OpenSSH server will need to go through a host-keys generation process which can take anywhere from a few hundred milliseconds to a few seconds, if entropy is low.</p> <p>SSH also depends on a network connection being available, and the system having fully booted.</p>"},{"location":"reference/sos/#set-up-sos-in-your-config-file","title":"Set up SOS in your config file","text":"<p>You'll need to specify the source for authorized SSH keys.</p> <p>This can be one of either, or both of:</p> <ul> <li><code>github_user</code> - a GitHub username to fetch public keys from</li> <li><code>ssh_keys</code> - a list of public keys to add to the VM</li> </ul> <pre><code>config:\n  github_user: &lt;your-github-username&gt;\n  ssh_keys:\n  - &lt;your-ssh-public-key&gt;\n  - &lt;another-ssh-public-key&gt;\n</code></pre> <p>Secondly, you need to bind a specific port and adapter for SOS to listen on.</p> <pre><code>config:\n  ssh:\n    bind_address: `127.0.0.1:`\n    port: 2222\n</code></pre> <p>To limit connections to only those coming from the Slicer host, use <code>127.0.0.1:</code>, but to enable remote SOS access from other hosts use <code>0.0.0.0:</code>.</p> <p>The port number can be anything, but it's suggested that you use a higher port i.e. <code>2221</code> or <code>2222</code>, etc to avoid conflicts.</p> <p>Each Slicer process can only bind to one port, so if you want to run multiple Slicer instances on the same host, you'll need to use different ports.</p>"},{"location":"reference/sos/#connect-to-a-vm-using-sos","title":"Connect to a VM using SOS","text":"<p>When connecting to the SOS, you'll be presented with a menu.</p> <p>This menu lists all running VMs, hit up or down arrows and then \"Enter\" on the target VM.</p> <pre><code>Select a VM (use \u2191\u2193 arrows, Enter to select):\n----------------------------------------\n&gt; vm-1 &lt;\n  Quit\n----------------------------------------\nPress Enter to select, q to quit\n</code></pre> <p>From within the VM sub-menu, you can:</p> <ul> <li>Connect - launch a shell like an SSH session as the root user, but without any need for OpenSSH</li> <li>Pause the VM - with Firecracker microVMs, the VM's vCPUs are stopped, but memory is still allocated</li> <li>Resume the VM - if the VM was paused, this will allow it to continue running from where it left off</li> <li>Shutdown the VM - this is a graceful shutdown, equivalent to running <code>sudo shutdown now</code> within the VM</li> </ul> <pre><code>Actions for VM: vm-1\n----------------------------------------\n&gt; Connect &lt;\n  Pause\n  Shutdown\n  Back\n----------------------------------------\nPress Enter to select, Esc or 'b' to go back\n</code></pre> <p>In order for the shell be be operational, you'll need to make sure the <code>slicer-ssh-agent.service</code> is running within the VM using systemd. This is pre-installed in all the official VM images, but can be disabled for a slightly faster boot time, if you do not need to use SOS.</p> <p>When you connect, you'll be logged in as the <code>root</code> user.</p> <pre><code>Connecting to: vm-1\nWelcome to Ubuntu 22.04.5 LTS (GNU/Linux 5.10.240 x86_64)\n\n * Documentation:  https://help.ubuntu.com\n * Management:     https://landscape.canonical.com\n * Support:        https://ubuntu.com/pro\n\nThis system has been minimized by removing packages and content that are\nnot required on a system that users do not log into.\n\nTo restore this content, you can run the 'unminimize' command.\nroot@vm-1:/root# \n</code></pre> <p>Type <code>exit</code> or press <code>Ctrl+D</code> to exit the shell session.</p> <p>To enter the menu again, reconnect with the SSH command you used earlier.</p>"},{"location":"reference/sos/#pause-and-resume-from-the-cli","title":"Pause and Resume from the CLI","text":"<p>In addition to pausing and resuming VMs through the SOS console, you can now use the CLI commands directly:</p> <pre><code># Pause a VM to stop CPU usage\nslicer vm pause vm-1\n\n# Resume a paused VM\nslicer vm resume vm-1\n</code></pre> <p>Pausing a VM stops all vCPU execution while keeping memory allocated. The VM will wake up instantly when resumed, preserving its state.</p> <p>You can view the pause status in the VM list:</p> <pre><code>slicer vm list\n</code></pre> <p>VMs that are paused will show <code>-</code> in each field of the <code>slicer vm top</code> output.</p> <p>Paused VMs cannot use the <code>cp</code>, or <code>exec</code> functionality until they are resumed.</p>"},{"location":"reference/ssh/","title":"Log into a VM using SSH","text":"<p>This page covers two concepts:</p> <ol> <li>SSH access to a running VM over the network (covered on this page)</li> <li>Serial Over SSH console (SOS)</li> </ol>"},{"location":"reference/ssh/#ssh-access-to-a-running-vm-over-the-network","title":"SSH access to a running VM over the network","text":"<p>Unless you have optimised an image to turn off the bundled OpenSSH server, then it will start when the VM boots.</p> <p>You can configure VMs within a hostgroup with your SSH keys in two ways.</p>"},{"location":"reference/ssh/#using-the-github_user-field","title":"Using the <code>github_user</code> field","text":"<p>The simplest option is to use GitHub. Set your SSH keys on your profile, then they'll be available at <code>https://github.com/USER.keys</code></p> <pre><code>config:\n  github_user: alexellis\n</code></pre> <p>Only one username can be specified within the <code>github_user</code> field.</p>"},{"location":"reference/ssh/#using-the-ssh_keys-field","title":"Using the <code>ssh_keys</code> field","text":"<p>The <code>ssh_keys</code> field removes the dependency on GitHub, and speeds up the boot by avoiding a call over the Internet to a remote server.</p> <p>This method supports multiple keys.</p> <pre><code>config:\n  ssh_keys:\n    - ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC3... user@host\n    # For a multi-line key, use YAML's pipe syntax\n    - |\n        ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAIB3... user@host\n</code></pre>"},{"location":"reference/ssh/#using-the-userdata-field-to-set-up-ssh-keys","title":"Using the <code>userdata</code> field to set up SSH keys","text":"<p>You can also use the <code>userdata</code> field to set up SSH keys, as shown in the Userdata for Slicer VMs page.</p> <pre><code>config:\n    host_groups:\n    - name: vm\n      userdata: |\n        #!/bin/bash\n        mkdir -p /home/ubuntu/.ssh\n        echo \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC3... user@host\" &gt;&gt; /home/ubuntu/.ssh/authorized_keys\n        chmod 600 /home/ubuntu/.ssh/authorized_keys\n        chown -R ubuntu:ubuntu /home/ubuntu/.ssh\n</code></pre> <p>If you're running a different OS image such as Rocky Linux, make sure you change the home directory and user/group name accordingly.</p> <p>The <code>TARGET_USER</code> environment variable will also be set within the context of the <code>userdata</code> script, so you could use that instead of hard-coding <code>/home/ubuntu</code> or <code>/home/slicer</code>.</p>"},{"location":"reference/troubleshooting/","title":"Troubleshooting Slicer","text":"<p>For Slicer plans with standard support, Discord is the best first support option. For paid-support plans, email support is available; check your welcome email for details.</p>"},{"location":"reference/troubleshooting/#doesnt-boot-right-or-has-a-networking-issue-perhaps-github-keys-arent-being-imported","title":"Doesn't boot right or has a networking issue - perhaps GitHub keys aren't being imported?","text":"<p>Look in the log file outputted from slicer. So if you are running i.e. <code>k3s</code> as the host group and VM 1 isn't booting:</p> <pre><code>sudo cat /var/log/slicer/k3s-1.txt\n</code></pre> <p>If you can get into the VM via the SOS console, then run the following:</p> <pre><code>sudo journalctl -u mount-config --no-pager\n</code></pre> <p>Look for networking issues, or a bad routing.</p> <p>If your networking equipment is forcing the microVMs to use IPv6, but you do not have IPv6 connectivity, then you could try to disable IPv6.</p> <p>The host is easier to fix and may mean you can leave the microVMs as they are.</p> <pre><code>sudo sysctl -w net.ipv6.conf.all.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.default.disable_ipv6=1\nsudo sysctl -w net.ipv6.conf.lo.disable_ipv6=1\n</code></pre> <p>To make it permanent:</p> <p>Then add the following lines to <code>/etc/sysctl.conf</code>:</p> <pre><code>net.ipv6.conf.all.disable_ipv6 = 1\nnet.ipv6.conf.default.disable_ipv6 = 1\nnet.ipv6.conf.lo.disable_ipv6 = 1\n</code></pre> <p>Alternatively, you could try disabling only within the microVM on first boot:</p> <pre><code>config:\n    host_groups:\n    - name: k3s\n      userdata: |\n        #!/bin/bash\n        sysctl -w net.ipv6.conf.all.disable_ipv6=1\n        sysctl -w net.ipv6.conf.default.disable_ipv6=1\n        sysctl -w net.ipv6.conf.lo.disable_ipv6=1\n\n        echo \"net.ipv6.conf.all.disable_ipv6 = 1\" |tee -a /etc/sysctl.conf\n        echo \"net.ipv6.conf.default.disable_ipv6 = 1\" |tee -a /etc/sysctl.conf\n        echo \"net.ipv6.conf.lo.disable_ipv6 = 1\" |tee -a /etc/sysctl.conf\n\n        # Cause the SSH keys to re re-imported from GitHub on the next boot\n\n        rm -rf /home/ubuntu/.ssh/github_keys_imported\n\n        # Optionally, reboot the VM to re-import the keys from GitHub\n        # reboot\n</code></pre> <p>If you're having issues reaching GitHub for your SSH keys, you can set them manually in the config or userdata.</p>"},{"location":"reference/troubleshooting/#the-problem-may-be-fixed-by-upgrading-slicer","title":"The problem may be fixed by upgrading Slicer","text":"<p>You can upgrade the Slicer binary by running the instructions at the end of the installation page.</p>"},{"location":"reference/troubleshooting/#i-cant-connect-over-ssh","title":"I can't connect over SSH","text":"<p>Have a look at the VM's serial console, written to: <code>/var/log/slicer/vm-1.txt</code> where <code>1</code> is the VM number, and vm is the host group name. </p>"},{"location":"reference/troubleshooting/#the-vm-cant-connect-to-the-internet","title":"The VM can't connect to the Internet","text":"<p>This can occur when there are old or stable routes in place, for instance, if you added a route for <code>192.168.137.0/24</code> on another host, but are now running Slicer on your own workstation.</p> <p>Check routes with:</p> <pre><code>sudo ip route\n</code></pre> <p>Then delete one with i.e. <code>sudo ip route del 192.168.137.0/24</code>.</p>"},{"location":"reference/troubleshooting/#ive-run-out-of-disk-space","title":"I've run out of disk space","text":"<p>There are three places to perform a prune/clean-up.</p> <ol> <li>Check there are not a lot of unused <code>.img</code> files from various launches of VMs</li> </ol> <pre><code>sudo find / -name *.img\n</code></pre> <p>Delete the ones you no longer require. Beware of deleting .img files for VMs that you still need.</p> <ol> <li>If you've been working with custom images, prune them from the containerd library:</li> </ol> <pre><code>sudo ctr -n slicer i ls\n\nsudo ctr -n slicer i ls -q |xargs sudo ctr -n slicer i rm\n</code></pre> <p>If you are using a snapshotter like ZFS or Devmapper, the above command will delete all images and their snapshots.</p> <p>So to be more selective, you can delete individual images by name:</p> <p>For instance:</p> <pre><code>sudo ctr -n slicer i rm docker.io/library/ubuntu:22.04\n</code></pre> <ol> <li>Remove the /var/run folder for Slicer</li> </ol> <p>The <code>/var/run</code> and <code>/var/log</code> folders contain logs, sockets, temporary disks, and extracted Kernel files. This can build up over time. <code>/var/run</code> is generally ephemeral, and removed on each reboot.</p> <pre><code>sudo rm -rf /var/run/slicer\nsudo rm -rf /var/log/slicer\n</code></pre> <p>For the nuclear option, delete all of the containerd's data, this will remove all images, snapshots, including any from containers that you run via Docker, if that's also installed.</p> <pre><code>sudo systemctl stop containerd\nsudo rm -rf /var/lib/containerd\nsudo systemctl restart containerd\n</code></pre> <p>If you're using image storage mode and need to resize a VM's disk, see the storage overview for instructions on resizing disk images.</p>"},{"location":"reference/vfio/","title":"VFIO Passthrough","text":""},{"location":"reference/vfio/#vfio-virtual-function-io-passthrough","title":"VFIO (Virtual Function I/O) Passthrough","text":"<p>VFIO (Virtual Function I/O) is a Linux kernel framework that allows a PCI device to be directly assigned to a virtual machine (VM). This enables the VM to have direct access to the hardware device, providing near-native performance and functionality.</p> <p>Any device such as a GPU or NIC that is passed through to a VM must first be unbound from its current driver on the host, and is exclusively bound to the VFIO driver.</p> <p>Slicer supports VFIO passthrough for Nvidia GPUs when using Cloud Hypervisor as the hypervisor as per the example Ollama with a GPU.</p> <p>Support for NICs for router appliances will be coming shortly.</p> <p>VFIO is limited to x86_64 systems with hardware support for IOMMU (Intel VT-d or AMD-Vi).</p>"},{"location":"reference/vfio/#enable-vfio-on-start-up","title":"Enable VFIO on start-up","text":"<p>You must edit the <code>cmdline</code> argument of your bootloader to include the following parameters.</p> <p>Intel:</p> <pre><code>intel_iommu=on iommu=pt\n</code></pre> <p>AMD:</p> <pre><code>amd_iommu=on iommu=pt\n</code></pre> <p>If you're using Grub as a bootloader, then edit the <code>/etc/default/grub</code> file and add the parameters to the <code>GRUB_CMDLINE_LINUX_DEFAULT</code> variable. For example:</p> <pre><code>GRUB_CMDLINE_LINUX_DEFAULT=\"quiet splash intel_iommu=on iommu=pt\"\n</code></pre> <p>Then update Grub:</p> <pre><code>sudo update-grub\n</code></pre> <p>Then update your initramfs:</p> <pre><code>sudo update-initramfs -c -k $(uname -r)\n</code></pre> <p>Now reboot your system.</p>"},{"location":"reference/vfio/#optional-bind-devices-to-vfio-on-start-up","title":"Optional: Bind devices to VFIO on start-up","text":"<p>We recommend unbinding and rebinding devices to VFIO using the <code>slicer pci</code> commands either as and when they're required, or via a systemd unit on start-up.</p> <p>But, you can also bind devices to VFIO via the <code>cmdline</code> argument in your bootloader configuration.</p> <p>Note: That this method only works at the vendor/device ID level, so if you have multiple GPUs, or multiple NICs of the same time, it's very unlikely that you'll want to bind them all to VFIO because they will not be accessible to the host.</p> <p>Use <code>sudo slicer pci list</code> to find the vendor and device IDs of the devices you want to bind to VFIO.</p> <p>Then add them to the <code>cmdline</code> argument as follows, replacing the example IDs below with your own:</p> <pre><code>vfio-pci.ids=10de:2204,10de:1aef\n</code></pre>"},{"location":"reference/vfio/#view-pci-devices-and-their-iommu-groups","title":"View PCI Devices and their IOMMU Groups","text":"<p>To view the PCI devices and their IOMMU groups, you can use the following command:</p> <pre><code>sudo slicer pci list\n</code></pre> <p>Example output:</p> <pre><code>ADDRESS      CLASS    VENDOR   DEVICE   DESCRIPTION                              IOMMU GROUP  VFIO GROUP   DRIVER\n-------      -----    ------   ------   ---------------------------------------- ----------   ----------   ------\n0000:0b:00.0 0300     10de     2204     VGA compatible controller: NVIDIA Cor... 28           28           vfio-pci\n0000:0b:00.1 0403     10de     1aef     Audio device: NVIDIA Corporation GA10... 28           28           vfio-pci\n0000:0c:00.0 0300     10de     2204     VGA compatible controller: NVIDIA Cor... 29           29           vfio-pci\n0000:0c:00.1 0403     10de     1aef     Audio device: NVIDIA Corporation GA10... 29           29           vfio-pci\n</code></pre>"},{"location":"reference/vfio/#bind-a-device-to-vfio","title":"Bind a device to VFIO","text":"<p>You can bind a device to VFIO in two ways:</p> <ol> <li>By specifying the device's vendor and device ID in the <code>cmdline</code> arguments for the Kernel.</li> <li>By using the <code>slicer pci</code> command to bind the device to VFIO.</li> </ol> <p>We recommend only using 2. because 1. is unable to differentiate between multiple devices of the same type such as two or more NICs or two or more GPUs. You often need at least one of these to be available for the host.</p> <p>View PCI devices via <code>sudo slicer pci list</code> and note the <code>ADDRESS</code> column.</p> <p>Then run <code>sudo slicer pci bind &lt;PCI_ADDRESS&gt;</code> to bind the device to VFIO. Replace <code>&lt;PCI_ADDRESS&gt;</code> with the actual PCI address of the device you want to bind.</p> <p>By default, this command will attempt to unbind the device from the original driver on your host system, but if it doesn't work, unbind it manually with the steps below.</p>"},{"location":"reference/vfio/#unbind-a-device-from-vfio","title":"Unbind a device from VFIO","text":"<p>To unbind a device from VFIO, you can use the following command:</p> <pre><code>sudo slicer pci unbind &lt;PCI_ADDRESS&gt;\n</code></pre>"},{"location":"reference/vfio/#rebind-a-device-to-its-original-driver","title":"Rebind a device to its original driver","text":"<p>Once unbound, you can use <code>bind</code> with the <code>--driver</code> flag to re-bind it to the original driver on your host system. Or reboot if that's easier.</p> <p>In the case of the GPU example above, you may want to allocate one of the bound GPUs back to the host for display purposes.</p> <pre><code>sudo slicer pci bind 0000:0b:00.0 --driver=nvidia\nsudo slicer pci bind 0000:0b:00.1 --driver=snd_hda_intel\n</code></pre>"},{"location":"reference/vfio/#troubleshooting","title":"Troubleshooting","text":"<p>It's unlikely that you'll need to load the modules for VFIO manually, but you can do so with:</p> <pre><code>sudo modprobe vfio\nsudo modprobe vfio-pci\n</code></pre> <ol> <li>Ensure that your CPU and motherboard support IOMMU and that it is enabled in the BIOS/UEFI settings.</li> <li>Also double-check your bootloader i.e. Grub configuration for the command line that's passed to the Linux Kernel. Did you skip <code>update-grub</code> or <code>update-initramfs</code>?</li> <li>Check the output of <code>sudo dmesg | grep -e DMAR -e IOMMU</code> for any errors related to IOMMU initialization.</li> <li>Ensure that the device you are trying to passthrough is not being used by the host system and that it's not already bound to a specific driver.</li> <li>Verify that the device is in its own IOMMU group using <code>sudo slicer pci list</code> - you typically have to bind every device within an IOMMU group otherwise they cannot be used in a VM.</li> <li>If you're having issues binding a device to VFIO, try explicitly unbinding it from the original driver with <code>sudo slicer pci unbind &lt;PCI_ADDRESS&gt;</code> first.</li> </ol> <p>After checking all of the above, if you find your devices are all mixed into the same IOMMU group, that means your system is not designed for VFIO.</p> <p>As an alternative, you can deploy/run the so called \"ACS patches\", but they may also have certain security or stability implications. Use at your own risk.</p>"},{"location":"storage/devmapper/","title":"Devmapper storage for Slicer","text":"<p>Devmapper is one of the options for storage with Slicer.</p> <p>We generally recommend using disk images for long-running VMs, or ZFS for launching many short-lived VMs.</p> <p>That said, you can install Devmapper as an alternative, with a backing drive set up for it, just like we do for ZFS.</p>"},{"location":"storage/devmapper/#installation","title":"Installation","text":"<p>To setup Devmapper you can run the installation script again, but this time, with additional flags.</p> <p>First, install a drive, or make a partition available for Devmapper to use.</p> <p>Run <code>lsblk</code> to identify your drives.</p> <pre><code>$ lsblk\nNAME                             MAJ:MIN RM   SIZE RO TYPE MOUNTPOINTS\nnvme1n1                          259:0    0 931.5G  0 disk \n\u251c\u2500nvme1n1p1                      259:1    0     1G  0 part /boot/efi\n\u251c\u2500nvme1n1p2                      259:2    0     2G  0 part /boot\n\u2514\u2500nvme1n1p3                      259:3    0 928.5G  0 part \n  \u2514\u2500ubuntu--vg-ubuntu--lv        253:0    0   500G  0 lvm  /\nnvme0n1                          259:4    0   1.8T  0 disk \n</code></pre> <p>In this instance, you can see that my 2TB NVMe SSD is called nvme0n1 and is currently not allocated.</p> <p>Run the installation script again and set the <code>--devmapper</code> flag:</p> <pre><code>curl -sLS https://get.slicervm.com | sudo bash -s -- \\\n  --devmapper /dev/nvme0n1 \\\n  --overwrite # Destroy any existing content on the disk\n</code></pre> <p>Be very careful that you specify the correct drive or partition. This operation cannot be reversed and will destroy any existing contents.</p> <p>The default size for any unpacked VM disk is <code>30GB</code>. To alter that size edit <code>/etc/containerd/config.toml</code> and restart containerd.</p> <pre><code>[plugins]\n  [plugins.\"io.containerd.snapshotter.v1.devmapper\"]\n    pool_name = \"slicer-thinpool\"\n    root_path = \"/var/lib/containerd/devmapper\"\n-   base_image_size = \"30GB\"\n+   base_image_size = \"50GB\"\n    discard_blocks = true\n</code></pre> <pre><code>sudo systemctl restart containerd\n</code></pre>"},{"location":"storage/devmapper/#configure-slicer-to-use-devmapper","title":"Configure Slicer to use Devmapper","text":"<pre><code>config:\n  host_groups:\n  - name: vm\n-   storage: image\n-   storage_size: 25G\n+   storage: devmapper\n</code></pre> <p>The <code>storage_size</code> field cannot be specified for Devmapper. So whatever size was setup for snapshots during the installation will be the size used for all VMs.</p>"},{"location":"storage/overview/","title":"Storage for Slicer","text":"<p>In the walkthrough, we show how to use disk images for storage.</p> <p>Disk images were not the first choice for Slicer, initially, only snapshotting filesystems were supported. With a snapshot, the initial filesystem is \"unpacked\" once - taking anywhere between 30 and 90 seconds, then can be cloned instantly.</p> <ul> <li>Disk Images - static files allocated on start-up, can take a few seconds to clone the initial disk</li> <li>Snapshotting / Copy On Write (CoW) - dynamic filesystems that only store changes and allow for instant cloning</li> </ul>"},{"location":"storage/overview/#disk-images","title":"Disk images","text":"<p>Disk images are similar to loopback filesystems which you may have created in the past via <code>fallocate</code> and <code>mkfs.ext4</code>.</p> <p>Pros:</p> <ul> <li>Convenient, with a custom disk size</li> <li>Easy to manage and migrate between systems</li> </ul> <p>Cons:</p> <ul> <li>No deduplication, as is possible with snapshots/Copy On Write (CoW) systems</li> <li>Launch/clone time slower than CoW when launching many VMs</li> </ul>"},{"location":"storage/overview/#resizing-disk-images","title":"Resizing disk images","text":"<p>If you need more storage space in a VM using image storage mode, resize the disk after shutting down the VM:</p> <ol> <li>Shutdown the VM:    <pre><code>slicer vm shutdown vm-hostname\n</code></pre></li> </ol> <p>Then close Slicer with Control + C, so you can boot the VM again on next start up.</p> <ol> <li> <p>Check the filesystem on the disk image:    <pre><code>sudo e2fsck -fy ./vm-hostname.img\n</code></pre></p> </li> <li> <p>Resize the filesystem to a given size i.e. 30G:    <pre><code>sudo resize2fs ./vm-hostname.img 30G\n</code></pre></p> </li> <li> <p>Start the VM again:    <pre><code>slicer up ./config.yaml\n</code></pre></p> </li> </ol> <p>Note: The VM must be using image storage mode (set with <code>storage: image</code> in your config). This assumes you've already increased the physical disk image size beforehand.</p> <p>Warning: Always run <code>e2fsck</code> before resizing and ensure the VM is completely shut down to avoid data corruption.</p>"},{"location":"storage/overview/#zfs","title":"ZFS","text":"<p>ZFS is an advanced filesystem that supports CoW snapshots.</p> <p>Pros:</p> <ul> <li>Built-in support for snapshots and deduplication</li> <li>Instant clone of base snapshot - ideal for launching many VMs</li> <li>Easier to troubleshoot and understand than devmapper</li> </ul> <p>Cons:</p> <ul> <li>More complex to set up and manage than disk images</li> <li>Requires an additional disk or partition</li> <li>Additional setup required</li> <li>No custom sizing - must match the base snapshot</li> </ul> <p>Setup ZFS storage for Slicer</p>"},{"location":"storage/overview/#devmapper","title":"Devmapper","text":"<p>Devmapper is available for Slicer, but not set up by default and requires additional setup.</p> <p>Generally, we'd recommend using ZFS instead of devmapper unless you have a specific need for it.</p> <p>Pros:</p> <ul> <li>Reasonably well known from the Docker space</li> <li>Instant clone of base snapshot</li> </ul> <p>Cons:</p> <ul> <li>Requires an additional disk or partition</li> <li>No custom size for VMs - the size of any VM must match the base snapshot</li> <li>Difficult to debug and troubleshoot - it's easier to recreate the whole storage pool</li> </ul> <p>Setup devmapper storage for Slicer</p>"},{"location":"storage/zfs/","title":"ZFS Storage Pools for Slicer","text":"<p>ZFS is an advanced filesystem that was originally developed at Sun Microsystems. The modern equivalent for Linux was ported as the OpenZFS project and has a license that is incompatible with the Linux Kernel, for that reason, it is kept out of tree, and must be installed separately.</p> <p>The containerd project already has a ZFS snapshotter, however it is unsuitable for use for VMs, therefore we implemented our own snapshotter plugin which can present ZFS volumes as block devices to a microVM.</p>"},{"location":"storage/zfs/#installation","title":"Installation","text":"<p>To setup ZFS you can run the installation script again, but this time, with additional flags.</p> <p>Install a drive, or make a partition available for zfs to use. The installation script will automatically creatre a loopback device if no device is provided.</p> <p>While using a loopback file is supported, it is absolutely not recommended to use a loopback file for ZFS with Slicer.</p> <p>Run the installation script again and set the <code>--zfs</code> flag:</p> <pre><code>curl -sLS https://get.slicervm.com | sudo bash -s -- \\\n  --zfs /dev/nvme0n1 \\\n  --overwrite # Destroy any existing content on the disk\n</code></pre> <p>Be very careful that you specify the correct drive or partition. This operation cannot be reversed and will destroy any existing contents.</p> <p>The default size for any unpacked VM disk is <code>30GB</code>. See adjust the base snapshot size to change this.</p>"},{"location":"storage/zfs/#use-zfs-for-vm-storage","title":"Use ZFS for VM storage","text":"<p>Let's customise the walkthrough example for ZFS.</p> <p>1) Change the <code>storage</code> type from <code>image</code> to <code>zfs</code>:</p> <pre><code>config:\n  host_groups:\n  - name: vm\n-   storage: image\n+   storage: zfs\n-   storage_size: 20G\n</code></pre> <p>See the note below on <code>storage_size</code>.</p> <p>2) Customise the <code>storage_size</code></p> <p>The <code>storage_size</code> field is optional for ZFS.</p> <p>If not specified, the default size of the base snapshot will be used. A custom size can be given, so long as it is equal to or larger than the base snapshot size.</p> <pre><code>config:\n  host_groups:\n  - name: vm\n    storage: zfs\n+   storage_size: 40G\n</code></pre> <p>If the base snapshot size is large for any existing VMs, then you can find its lease and remove it before having it re-created with the new settings for the vzol-snapshotter.</p> <pre><code>$ sudo ctr -n slicer leases ls\nID            CREATED AT           LABELS \nslicer/k3s-1  2025-09-04T13:53:07Z -      \n</code></pre> <p>Then, find the lease ID for the VM in question, and delete it. The lease ID is the hostname of the VM i.e.<code>k3s-1</code>.</p> <pre><code>sudo ctr -n slicer leases rm slicer/k3s-1\n</code></pre> <p>To delete all leases:</p> <pre><code>sudo ctr -n slicer leases ls -q | xargs -n1 sudo ctr -n slicer leases rm\n</code></pre>"},{"location":"storage/zfs/#adjust-the-base-snapshot-size","title":"Adjust the base snapshot size","text":"<p>The base snapshot size can be changed by updating the snapshooter configuration file and restarting the zvol snapshotter service.</p> <p>Edit <code>/etc/containerd-zvol-grpc/config.toml</code> and replace the volume size with the desired value, e.g <code>40G</code>:</p> <pre><code>root_path=\"/var/lib/containerd-zvol-grpc\"\ndataset=\"your-zpool/snapshots\"\n-volume_size=\"30G\"\n+volume_size=\"40G\"\nfs_type=\"ext4\"\n</code></pre> <p>Finally restart the zvol-snapshotter service:</p> <pre><code>sudo systemctl restart zvol-snapshotter\n</code></pre>"},{"location":"tasks/copy-files/","title":"Copy files to and from a VM","text":"<p>You can copy files to and from a VM using the following methods:</p> <ul> <li>SSH - sftp, scp, rsync, restic</li> <li>Slicer's native mechanism - <code>slicer vm cp</code> powered by the <code>slicer-agent</code> service</li> </ul> <p>The first approach is fairly standard, but requires SSH to be started, and installed. Any client must have a route and direct access to the VM via the LAN or over some form of VPN/overlay network.</p> <p>The second approach can be run from anywhere, so long as the client has access to Slicer's REST API, making it more powerful and easier to use.</p>"},{"location":"tasks/copy-files/#copy-files-or-directories-between-a-client-and-a-vm","title":"Copy files or directories between a client and a VM","text":"<p>The copy command enables bidirectional file transfer between the host and guest VMs without the need of mounting any folders to VMs.</p> <p>For example, to copy a file named <code>local-file.txt</code> from the host to a VM named <code>vm-1</code> use the command:</p> <pre><code>slicer vm cp example.txt vm-1:.\n</code></pre> <p>To copy a file from the instance named <code>vm-1</code> run:</p> <pre><code>slicer vm cp vm-1:/home/ubuntu/example.txt .\n</code></pre> <p>It is also possible to copy an entire directory between host and a VM:</p> <pre><code># Copy a directory from local machine to VM\nslicer vm cp /etc vm-1:/tmp/etc\n\n# Copy a directory from VM to local machine\nslicer vm cp vm-1:/etc /tmp/etc\n</code></pre> <p>Slicer supports two copy modes. The default tar mode supports directories and preserves file structure using compression for efficient transfer. Binary mode provides file-by-file transfer and supports custom permissions setting, making it ideal for single files with specific permission requirements:</p> <pre><code># Use tar mode explicitly (default)\nslicer vm cp --mode tar vm-1:/home/ubuntu/documents ./documents\n\n# Use binary mode with custom permissions\nslicer vm cp --mode binary --permissions 0755 ./script.sh vm-1:/usr/local/bin/script.sh\n</code></pre>"},{"location":"tasks/custom-image/","title":"Build a custom root filesystem","text":"<p>You can customise a Slicer VM in two ways:</p> <ol> <li>Via userdata on first boot (a bash script included via the config file)</li> <li>By extending an existing root filesystem with Docker and adding various <code>COPY</code> and <code>RUN</code> statements</li> </ol> <p>When building within Docker, you must bear in mind that not all of the files shown to you will persist into the final image.</p> <p>Additionally, if you're copying and pasting commands from installation guides, certain <code>systemctl</code> commands will need to be adapted.</p> <p>If a guide gave you <code>systemctl enable --now bind</code> for a custom DNS server, you'd simply remove the <code>--now</code> flag. The meaning of <code>--now</code> is to instruct the build container to start a systemd service immediately - that's not possible or desirable during a build.</p> <p>In short, if you are trying to extend a Slicer image, treat it as if it were a Docker container, and only run commands that you know work in a regular Dockerfile.</p> <p>For anything that does not run or work within a Dockerfile, either use userdata to run the steps on first boot, or add a one-shot systemd unit file that will run your custom steps on first boot.</p>"},{"location":"tasks/custom-image/#build-a-custom-image","title":"Build a custom image","text":"<p>First, refer to the image you want to customise - whether it's for aarch64 or x86_64.</p> <p>Then create a Dockerfile with a <code>FROM</code> line specifying the base image you want to use. For example:</p> <pre><code>FROM ghcr.io/openfaasltd/slicer-systemd:5.10.240-x86_64-latest\n</code></pre> <p>If you wanted to install Nginx and have it start automatically, with a website you've created, you could add the following lines to your Dockerfile.</p> <p>Nginx will run in systemd, we should not try to change the CMD instruction.</p> <pre><code>+RUN apt-get update &amp;&amp; apt-get install -y nginx\n+RUN systemctl enable nginx\n+COPY ./my-website /var/www/html\n</code></pre> <p>Then build and publish the image to your own registry:</p> <pre><code>docker build -t docker.io/alexellis2/slicer-nginx:5.10.240-x86_64 .\ndocker push docker.io/alexellis2/slicer-nginx:5.10.240-x86_64\n</code></pre> <p>Then edit your Slicer YAML and replace the <code>image:</code> with <code>docker.io/alexellis2/slicer-nginx:5.10.240-x86_64</code>.</p> <p>If you wanted Docker to be pre-installed into all your VMs, with the default user already set up for access, you could write:</p> <pre><code>+RUN curl -sLS https://get.docker.com | sh\n+RUN usermod -aG docker ubuntu\n</code></pre> <p>If you want to run a local registry, without TLS authentication enabled, you can do so with the following within your YAML file:</p> <pre><code>config:\n  insecure_registry: true\n</code></pre> <p>Then if you want to run a temporary Docker registry on another machine on your network:</p> <pre><code>docker run -d -p 5000:5000 --restart always \\\n  --name registry registry:3\n</code></pre>"},{"location":"tasks/execute-commands-with-sdk/","title":"Execute Commands in VM via SDK","text":"<p>The Slicer SDK for Go enables programmatic management of Slicer VMs. It supports the full VM lifecycle, including creation, deletion of VMs,and provides methods for file transfer and executing commands within the VM. This allows you to automate Slicer to run ephemeral or isolated workloads and retrieve the results efficiently.</p> <p>Some common use cases could include:</p> <ul> <li>Running AI agents or inference workloads</li> <li>Executing cluster nodes (Kubernetes, Nomad, etc.)</li> <li>Building or running untrusted customer code in isolation</li> <li>Media conversion and metadata extraction</li> <li>Headless browser automation</li> <li>Batch workloads with dynamic scaling</li> </ul>"},{"location":"tasks/execute-commands-with-sdk/#example-video-conversion-workflow","title":"Example: Video Conversion Workflow","text":"<p>In this example we are going to use the SDK to provision a short-lived Linux VM, install <code>ffmpeg</code>, and use it to convert a mkv video to MP4. After the conversion is done the result is copied out of the VM before it is teared down.</p>"},{"location":"tasks/execute-commands-with-sdk/#run-slicer","title":"Run slicer","text":"<p>Before creating and running the SDK example script ensure that a Slicer API server is running.</p> <p>Create a basic Slicer configuration using <code>slicer new</code>. <pre><code>slicer new sdk \\\n  --count=0 \\\n  --graceful-shutdown=false \\\n  &gt; sdk.yaml\n</code></pre></p> <p>By using the <code>--count=0</code> flag we create a host group with no pre\u2011allocated VMs, the SDK will be used to provision VMs on demand. Disabling graceful shutdown reduces teardown latency for short\u2011lived workloads. In this example we also set the flag <code>--persistent=false</code> to ensure the VM storage is always cleared after the VM is destroyed.</p> <p>For the fastest possible boot times, use ZFS for storage.</p> <p>If you have ZFS set up, you can simply replace the storage flags with something like:</p> <pre><code>--storage=zfs\n</code></pre> <p>Start the Slicer API using the generated configuration:</p> <pre><code>sudo slicer up sdk.yaml\n</code></pre>"},{"location":"tasks/execute-commands-with-sdk/#create-a-sample-program","title":"Create a sample program","text":""},{"location":"tasks/execute-commands-with-sdk/#step-1-initialize-sdk-client-and-create-a-vm","title":"Step 1: Initialize SDK client and create a VM","text":"<p>The SDK client handles authentication and communication with the SlicerVM API. VM resource requirements such as CPU count and memory are specified at creation time. <pre><code>package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"os\"\n    \"time\"\n\n    sdk \"github.com/slicervm/sdk\"\n)\n\nfunc main() {\n    if err := run(); err != nil {\n        log.Fatal(err)\n    }\n}\n\nfunc run() error {\n    // Initialize SDK client with authentication\n    baseURL := os.Getenv(\"SLICER_URL\")\n    token := os.Getenv(\"SLICER_TOKEN\")\n    userAgent := \"video-converter/1.0\"\n\n    client := sdk.NewSlicerClient(baseURL, token, userAgent, nil)\n    ctx := context.Background()\n\n    // Define VM specifications\n    createReq := sdk.SlicerCreateNodeRequest{\n        RamBytes:    8 * 1024 * 1024 * 1024, // 8GB RAM\n        CPUs:        4, // 4 CPU cores for video processing\n    }\n\n    // Create VM in the 'sdk' host group\n    hostGroupName := \"sdk\"\n    node, err := client.CreateVM(ctx, hostGroupName, createReq)\n    if err != nil {\n        return fmt.Errorf(\"failed to create VM: %s\", err)\n    }\n\n    log.Printf(\"Created VM: %s\", node.Hostname)\n\n    // Ensure VM cleanup on program exit\n    defer func() {\n        log.Printf(\"Cleaning up VM: %s\", node.Hostname)\n        if err := client.DeleteNode(hostGroupName, node.Hostname); err != nil {\n            log.Printf(\"Failed to delete VM: %s\", err)\n        }\n    }()\n}\n</code></pre></p>"},{"location":"tasks/execute-commands-with-sdk/#step-2-wait-for-agent-readiness","title":"Step 2: Wait for agent readiness","text":"<p>After the VM is created, the SlicerVM agent must be initialized before commands or file transfers can be executed. Agent readiness should always be verified before trying to execute commands or transfer files.</p> <pre><code>    // Wait for SlicerVM agent to become ready\n    log.Println(\"Waiting for slicer agent to initialize...\")\n\n    agentReady := false\n    for attempt := 1; attempt &lt;= 30; attempt++ {\n        _, err := client.GetAgentHealth(ctx, node.Hostname, false)\n        if err == nil {\n            agentReady = true\n            log.Println(\"Slicer agent is ready\")\n            break\n        }\n\n        log.Printf(\"Agent not ready (attempt %d/30): %s\", attempt, err)\n        time.Sleep(100 * time.Millisecond)\n    }\n\n    if !agentReady {\n        return fmt.Errorf(\"timeout waiting for slicer agent to become ready\")\n    }\n</code></pre>"},{"location":"tasks/execute-commands-with-sdk/#step-3-install-required-packages","title":"Step 3: Install required packages","text":"<p>Use <code>Exec()</code> to install ffmpeg into the VM.</p> <pre><code>    // Install ffmpeg for video processing\n    log.Println(\"Installing ffmpeg...\")\n\n    installCmd, err := client.Exec(ctx, node.Hostname, sdk.SlicerExecRequest{\n        Command: \"apt update &amp;&amp; apt install -y ffmpeg\",\n        Shell:   \"/bin/bash\",\n    })\n    if err != nil {\n        return fmt.Errorf(\"failed to start ffmpeg installation: %s\", err)\n    }\n\n    // Process command output in real-time\n    for response := range installCmd {\n        if len(response.Error) &gt; 0 {\n            return fmt.Errorf(\"ffmpeg installation failed: %s\", response.Error)\n        }\n\n        // Log command output\n        if response.Stdout != \"\" {\n            log.Printf(\"[install ffmpeg] %s\", response.Stdout)\n        }\n        if response.Stderr != \"\" {\n            log.Printf(\"[install ffmpeg] %s\", response.Stderr)\n        }\n\n        // Check final exit status\n        if response.ExitCode != 0 {\n            return fmt.Errorf(\"ffmpeg installation failed with exit code: %d\", response.ExitCode)\n        }\n    }\n</code></pre> <p>To speed up the workflow, instead of installing required packages each time, you could derive a custom image with ffmpeg pre-installed. See Build a custom image for more info.</p>"},{"location":"tasks/execute-commands-with-sdk/#step-4-transfer-files-and-run-exec","title":"Step 4: Transfer files and run Exec","text":"<p>An input video file is copied into the VM, processed with ffmpeg, and then the result is copied back out. The SDK has a <code>CpToVM()</code> and <code>CpFromVM()</code> method for file transfer. File ownership and permissions can be explicitly controlled. Commands can be executed inside the VM using the <code>Exec()</code> method. Output is streamed in real time, allowing progress monitoring and failure detection.</p> <pre><code>// Transfer input file to VM\n    log.Println(\"Copying input file to VM...\")\n    err = client.CpToVM(ctx, node.Hostname, \"./input.mkv\", \"/home/ubuntu/input.mkv\", 1000, 1000, \"0664\", \"binary\")\n    if err != nil {\n        return fmt.Errorf(\"failed to copy input file: %s\", err)\n    }\n\n    // Execute video conversion\n    log.Println(\"Converting video...\")\n\n    convertCmd, err := client.Exec(ctx, node.Hostname, sdk.SlicerExecRequest{\n        Cwd:     \"/home/ubuntu\",\n        Command: \"ffmpeg -i input.mkv -vf scale=-2:720 -c:v libx264 -preset medium -crf 23 -c:a aac output.mp4\",\n        Shell:   \"/bin/bash\",\n        UID:     1000,\n        GID:     1000,\n    })\n    if err != nil {\n        return fmt.Errorf(\"failed to start conversion: %s\", err)\n    }\n\n    // Monitor conversion progress\n    for response := range convertCmd {\n        if len(response.Error) &gt; 0 {\n            return fmt.Errorf(\"conversion failed: %s\", response.Error)\n        }\n\n        if response.Stdout != \"\" {\n            log.Printf(\"[ffmpeg] %s\", response.Stdout)\n        }\n\n        if response.Stderr != \"\" {\n            log.Printf(\"[ffmpeg] %s\", response.Stderr)\n        }\n\n        if response.ExitCode != 0 {\n            return fmt.Errorf(\"conversion failed with exit code: %d\", response.ExitCode)\n        }\n    }\n\n    // Copy converted file back to host\n    log.Println(\"Copying result back to host...\")\n    err = client.CpFromVM(ctx, node.Hostname, \"/home/ubuntu/output.mp4\", \"./output.mp4\", \"0664\", \"binary\")\n    if err != nil {\n        return fmt.Errorf(\"failed to copy output file: %s\", err)\n    }\n\n    log.Println(\"Video conversion completed successfully!\")\n</code></pre> <p>After copying in the source file we execute ffmpeg on the VM to convert the <code>mkv</code> file into an <code>mp4</code> file.</p> <pre><code>ffmpeg -i input.mkv -vf scale=-2:720 -c:v libx264 -preset medium -crf 23 -c:a aac output.mp\n</code></pre> <ul> <li><code>-i input.mkv</code> \u2013 input video file</li> <li><code>-vf scale=-2:720</code> \u2013 scales video to 720px height while keeping aspect ratio (-2 auto-calculates width, divisible by 2)</li> <li><code>-c:v libx264</code> \u2013 encodes video using H.264</li> <li><code>-preset medium</code> \u2013 balances encoding speed and compression efficiency</li> <li><code>-crf 23</code> \u2013 sets video quality (lower = better quality, larger file)</li> <li><code>-c:a aac</code> \u2013 encodes audio using AAC</li> <li><code>output.mp4</code> \u2013 output file in MP4 format</li> </ul> <p>The input file used in this example can be found in the GitHub repository.</p> <p>In step 1 we used a <code>defer</code> to delete the VM via the API after a conversion is complete.</p> <p>It is also possible to trigger a shutdown from within the VM by executing the <code>sudo reboot</code> command. If the VM has persistent storage turned off the disk is automatically deleted after the VM is shutdown.</p>"},{"location":"tasks/execute-commands-with-sdk/#run-the-example","title":"Run the example","text":"<p>Configure environment variables for our example program. These are used to connect to the Slicer API. You can connect to a local slicer instance running on the same host, like what we are doing here, or to a remote instance.</p> <pre><code>export SLICER_URL=\"http://127.0.0.1:8080\"\nexport SLICER_TOKEN=\"$(sudo cat /var/lib/slicer/auth/token)\"\n</code></pre> <p>If you are running the slicer API on a remote host you could get the token over SSH:</p> <pre><code>export SLICER_TOKEN=\"$(ssh user@remote-host 'sudo cat /var/lib/slicer/auth/token')\"\n</code></pre> <p>Download an example input video file:</p> <pre><code>curl -L -o input.mkv https://github.com/welteki/slicer-sdk-example/raw/refs/heads/main/input.mkv\n</code></pre> <p>Full <code>main.go</code> file:</p> <pre><code>package main\n\nimport (\n    \"context\"\n    \"fmt\"\n    \"log\"\n    \"os\"\n    \"time\"\n\n    sdk \"github.com/slicervm/sdk\"\n)\n\nfunc main() {\n    if err := run(); err != nil {\n        log.Fatal(err)\n    }\n}\n\nfunc run() error {\n    // Initialize SDK client with authentication\n    baseURL := os.Getenv(\"SLICER_URL\")\n    token := os.Getenv(\"SLICER_TOKEN\")\n    userAgent := \"video-converter/1.0\"\n\n    client := sdk.NewSlicerClient(baseURL, token, userAgent, nil)\n    ctx := context.Background()\n\n    // Define VM specifications\n    createReq := sdk.SlicerCreateNodeRequest{\n        RamGB:    8, // 8GB RAM\n        CPUs:     4, // 4 CPU cores for video processing\n    }\n\n    // Create VM in the 'sdk' host group\n    hostGroupName := \"sdk\"\n    node, err := client.CreateVM(ctx, hostGroupName, createReq)\n    if err != nil {\n        return fmt.Errorf(\"failed to create VM: %s\", err)\n    }\n\n    log.Printf(\"Created VM: %s\", node.Hostname)\n\n    // Ensure VM cleanup on program exit\n    defer func() {\n        log.Printf(\"Cleaning up VM: %s\", node.Hostname)\n        if err := client.DeleteNode(hostGroupName, node.Hostname); err != nil {\n            log.Printf(\"Failed to delete VM: %s\", err)\n        }\n    }()\n\n    // Wait for SlicerVM agent to become ready\n    log.Println(\"Waiting for slicer agent to initialize...\")\n\n    agentReady := false\n    for attempt := 1; attempt &lt;= 30; attempt++ {\n        _, err := client.GetAgentHealth(ctx, node.Hostname, false)\n        if err == nil {\n            agentReady = true\n            log.Println(\"Slicer agent is ready\")\n            break\n        }\n\n        log.Printf(\"Agent not ready (attempt %d/30): %s\", attempt, err)\n        time.Sleep(100 * time.Millisecond)\n    }\n\n    if !agentReady {\n        return fmt.Errorf(\"timeout waiting for slicer agent to become ready\")\n    }\n\n    // Install ffmpeg for video processing\n    log.Println(\"Installing ffmpeg...\")\n\n    installCmd, err := client.Exec(ctx, node.Hostname, sdk.SlicerExecRequest{\n        Command: \"apt update &amp;&amp; apt install -y ffmpeg\",\n        Shell:   \"/bin/bash\",\n    })\n    if err != nil {\n        return fmt.Errorf(\"failed to start ffmpeg installation: %s\", err)\n    }\n\n    // Process command output in real-time\n    for response := range installCmd {\n        if len(response.Error) &gt; 0 {\n            return fmt.Errorf(\"ffmpeg installation failed: %s\", response.Error)\n        }\n\n        // Log command output\n        if response.Stdout != \"\" {\n            log.Printf(\"[install ffmpeg] %s\", response.Stdout)\n        }\n        if response.Stderr != \"\" {\n            log.Printf(\"[install ffmpeg] %s\", response.Stderr)\n        }\n\n        // Check final exit status\n        if response.ExitCode != 0 {\n            return fmt.Errorf(\"ffmpeg installation failed with exit code: %d\", response.ExitCode)\n        }\n    }\n\n    // Transfer input file to VM\n    log.Println(\"Copying input file to VM...\")\n    err = client.CpToVM(ctx, node.Hostname, \"./input.mkv\", \"/home/ubuntu/input.mkv\", 1000, 1000, \"0664\", \"binary\")\n    if err != nil {\n        return fmt.Errorf(\"failed to copy input file: %s\", err)\n    }\n\n    // Execute video conversion\n    log.Println(\"Converting video...\")\n\n    convertCmd, err := client.Exec(ctx, node.Hostname, sdk.SlicerExecRequest{\n        Cwd:     \"/home/ubuntu\",\n        Command: \"ffmpeg -i input.mkv -vf scale=-2:720 -c:v libx264 -preset medium -crf 23 -c:a aac output.mp4\",\n        Shell:   \"/bin/bash\",\n        UID:     1000,\n        GID:     1000,\n    })\n    if err != nil {\n        return fmt.Errorf(\"failed to start conversion: %s\", err)\n    }\n\n    // Monitor conversion progress\n    for response := range convertCmd {\n        if len(response.Error) &gt; 0 {\n            return fmt.Errorf(\"conversion failed: %s\", response.Error)\n        }\n\n        if response.Stdout != \"\" {\n            log.Printf(\"[ffmpeg] %s\", response.Stdout)\n        }\n\n        if response.Stderr != \"\" {\n            log.Printf(\"[ffmpeg] %s\", response.Stderr)\n        }\n\n        if response.ExitCode != 0 {\n            return fmt.Errorf(\"conversion failed with exit code: %d\", response.ExitCode)\n        }\n    }\n\n    // Copy converted file back to host\n    log.Println(\"Copying result back to host...\")\n    err = client.CpFromVM(ctx, node.Hostname, \"/home/ubuntu/output.mp4\", \"./output.mp4\", \"0664\", \"binary\")\n    if err != nil {\n        return fmt.Errorf(\"failed to copy output file: %s\", err)\n    }\n\n    log.Println(\"Video conversion completed successfully!\")\n    return nil\n}\n</code></pre> <p>The full code sample is also available on GitHub</p> <p>Run the program:</p> <pre><code>go run main.go\n</code></pre> <p>In this example we created an ephemeral VM that is destroyed after the file conversion completes and the result is extracted. However it is entirely up to your implementation how you want to handle the VM lifecycle strategy.</p> <p>Possible VM lifecycle strategies:</p> <ul> <li>Single use: Create a VM per task and destroy it immediately after completion (as shown in this example). Maximizes isolation.</li> <li>Reusable VMs: Reuse a VM to execute multiple tasks in sequence or in parallel to reduce startup overhead.</li> </ul>"},{"location":"tasks/execute-commands-with-sdk/#next-steps","title":"Next Steps","text":"<ul> <li>Review the SDK reference</li> <li>Explore the REST API documentation for direct integrations</li> </ul>"},{"location":"tasks/execute-commands/","title":"Execute commands in a VM","text":"<p>Just like copying files in and out of a VM, executing commands can be done in several ways:</p> <ul> <li>Initially, via a userdata script or userdata file specified in the host group or via the API/CLI create command after Slicer has been started.</li> <li>Through SSH assuming direct network access is available to the VM via the LAN or a VPN.</li> <li>Through Slicer's REST API - using your own client, the SDK or the <code>slicer</code> CLI.</li> </ul> <p>The exec command allows you to run commands remotely on VMs. The first argument is the instance to run the commands on, <code>--</code> optionally separates the slicer CLI options from the command to run on the VM.</p> <p><pre><code># Run a command as root (default)\nslicer vm exec vm-1 whoami\n</code></pre> Slicer allows you to control the command execution context, including user permissions and working directory.</p> <pre><code># Run command as non-root user (UID 1000)\nslicer vm exec vm-1 --uid 1000 whoami\n</code></pre> <p>Change the working directory and run commands with arguments:</p> <pre><code># Execute command in specific directory\nslicer vm exec vm-1 --cwd /var/log -- ls -la\n</code></pre> <p>Control shell interpretation for complex commands:</p> <pre><code># Use default bash shell for complex commands with pipes\nslicer vm exec vm-1 -- \"ls -l | sort\"\n\n# Use custom shell interpreter\nslicer vm exec vm-1 --shell /bin/zsh -- \"echo $SHELL\"\n\n# Execute directly without shell (faster for simple commands)\nslicer vm exec vm-1 --shell \"\" whoami\n</code></pre> <p>Combine with local commands using pipes and STDIO:</p> <pre><code># Pipe local file content to VM command\ncat /etc/hostname | slicer vm exec vm-1 -- base64 --wrap 9999\n</code></pre>"},{"location":"tasks/expose-ports/","title":"Expose ports and services from Slicer VMs","text":"<p>There are serveral ways to access services listening on TCP ports within a Slicer VM.</p>"},{"location":"tasks/expose-ports/#direct-access-via-the-vms-ip","title":"Direct access via the VM's IP","text":"<p>The easiest way to access a service running on a VM, is to simply use its IP address. This requires any user to be on the same LAN and to have a route to the VM itself.</p> <p>If you installed the <code>nginx</code> package within the VM, you could open a web browser and navigate to <code>http://192.168.137.2</code> to access the service.</p> <p>SSH is already installed on VMs, so if a VM had an IP address of <code>192.168.137.2</code>, you could run <code>ssh ubuntu@192.168.137.2</code> to access it.</p> <p>The drawbacks of direct VM IP are that users must have a route to the Slicer host, and the VM's network CIDR. If you're away from the LAN, you will have to use a VPN to get access to the VM which can be complex.</p>"},{"location":"tasks/expose-ports/#port-forwarding-over-ssh-to-the-vm","title":"Port-forwarding over SSH to the VM","text":"<p>SSH tunnelling can be used to access a service that is only listening on 127.0.0.1 within a VM, for instance 127.0.0.1:3000 in the VM, can be accessed via:</p> <pre><code>ssh -L 3000:127.0.0.1:3000 ubuntu@192.168.137.2\n\ncurl http://localhost:3000\n</code></pre>"},{"location":"tasks/expose-ports/#port-forwarding-over-slicers-rest-api","title":"Port forwarding over Slicer's REST API","text":"<p>If you expose or tunnel Slicer's REST API over the Internet using inlets (below), then you can access any service within a VM without any routes or VPNs.</p> <p>Let's say you've exposed Slicer at <code>https://slicer-n100-1.example.com</code> and want to access port 3000 within vm-1:</p> <pre><code>slicer vm forward vm-1 127.0.0.1:3000\n\ncurl http://localhost:3000\n</code></pre> <p>You can also remap the local port i.e. from Nginx on 80 to 8080 locally</p> <pre><code>slicer vm forward vm-1 8080:127.0.0.1:80\n\ncurl http://localhost:8080\n</code></pre> <p>And you can make forwarded ports available to other machines on your local network this way:</p> <pre><code>slicer vm forward vm-1 0.0.0.0:8080:127.0.0.1:80\n</code></pre> <p>Then you'll be able to access the forwarded service with your own machine's IP address.</p>"},{"location":"tasks/expose-ports/#unix-socket-forwarding","title":"UNIX Socket Forwarding","text":"<p>Slicer also supports UNIX socket forwarding, allowing you to forward sockets from the VM to your local system. This is useful for accessing services like Docker, containerd, or other socket-based services.</p> <p>Forward a remote UNIX socket to your local system:</p> <pre><code>slicer vm forward vm-1 -L /Users/alex/docker.sock:/run/docker.sock --token \"...\" --url \"...\"\n\nDOCKER_HOST=unix:///Users/alex/docker.sock docker ps\n</code></pre> <p>In this example, we assume you've installed Docker into the microVM, and want to access it via a socket from your local machine - perhaps a Linux or MacOS client.</p> <p>Port forwarding works in any combination of: UNIX:UNIX, UNIX:TCP, TCP:UNIX, TCP:TCP.</p>"},{"location":"tasks/expose-ports/#public-access-with-inlets","title":"Public access with Inlets","text":"<p><code>inlets-pro</code> is a self-hosted tunnel that's easy to use, gives you full privacy and control over security and networking.</p> <p>You can use inlets-pro or inlets-cloud to expose a service directly from within a VM, or to expose Slicer's REST API for VM management, or port forwarding.</p>"},{"location":"tasks/expose-ports/#expose-a-tcp-service","title":"Expose a TCP service","text":"<p>You can start a TCP tunnel to expose a service at L4. IP whitelists/ACLs can also be added on top, as well as preserving the source IP address via PROXY protocol.</p> <p>You can set up the <code>inlets-pro</code> server manually on a public cloud VM which has a public IP address. Or, for ease of use, <code>inletsctl</code> can fully automate the process for you and return a connection string for the client.</p> <p>TCP tunnels are ideal for exposing things that already have TLS or encryption, or which cannot work over HTTP:</p> <ul> <li>The Kubernetes API</li> <li>SSH</li> <li>Reverse proxies like Caddy and Nginx</li> <li> <p>Databases (with self-signed certs)</p> </li> <li> <p>Automate a TCP tunnel server</p> </li> </ul>"},{"location":"tasks/expose-ports/#expose-a-http-service-with-tls","title":"Expose a HTTP service with TLS","text":"<p><code>inlets-pro</code> also supports automated TLS termination, and add-on authentication options like static API keys, or OAuth via GitHub or Gmail.</p> <p>When you expose a local HTTP endpoint i.e. 127.0.0.1:3000 it can be accessed via a DNS record such as <code>https://wordpress.example.com</code> and will obtain a TLS certificate from Let's Encrypt.</p> <ul> <li>Automate a HTTP tunnel server</li> <li>How to authenticate your HTTP tunnels with inlets and OAuth</li> <li>Authentication options for HTTPS tunnels</li> </ul>"},{"location":"tasks/expose-ports/#inlets-cloud","title":"Inlets Cloud","text":"<p>Inlets Cloud is a completely managed tunnel service available to inlets-pro subscribers at no extra cost.</p>"},{"location":"tasks/expose-ports/#expose-a-http-service-with-tls-and-a-custom-domain","title":"Expose a HTTP service with TLS and a custom domain","text":"<p>You can create a HTTPS tunnel with a custom domain, and the control plane will terminate TLS for you.</p> <p>This is the quickest and simplest way to expose a HTTP endpoint on the Internet with TLS.</p> <ul> <li>Managed HTTPS tunnels in one-click with inlets cloud</li> </ul>"},{"location":"tasks/expose-ports/#expose-kubernetes-or-ssh-for-various-hosts","title":"Expose Kubernetes or SSH for various hosts","text":"<p>Inlets Cloud also supports tunnel that expose Kubernetes or SSH for various hosts, by following a separate guide.</p> <ul> <li>SSH Into Any Private Host With Inlets Cloud</li> </ul>"},{"location":"tasks/monitoring/","title":"Logs &amp; Monitoring","text":""},{"location":"tasks/monitoring/#logs","title":"Logs","text":"<p>The output of the serial console which contains boot-up messages and output from the init system is available at <code>/var/log/slicer</code>.</p> <p>So for the quickstart, with 1x VM in a hostgroup named vm, you can view the log with:</p> <pre><code>sudo tail -f /var/log/slicer/vm-1.txt\n</code></pre> <p>These logs are also available via the REST API or the CLI:</p> <pre><code>sudo slicer logs vm-1               # Tail the last 20 lines (default)\nsudo slicer logs vm-1 --lines 50    # Tail the last 50 lines\nsudo slicer logs vm-1 --lines 0     # Print the whole file\n</code></pre>"},{"location":"tasks/monitoring/#monitoring","title":"Monitoring","text":"<p>When the <code>slicer-vmmeter.service</code> is loaded and running, then system utilization data can be collected via the <code>slicer vm top</code> command.</p> <p>If you need more monitoring that is available, feel free to let us know what you're looking for.</p>"},{"location":"tasks/monitoring/#prometheus-metrics-via-metrics","title":"Prometheus metrics via <code>/metrics</code>","text":"<p>Slicer exposes Prometheus metrics at the <code>/metrics</code> endpoint on the API server. This endpoint uses the same auth as the rest of the API. If auth is enabled, supply a bearer token.</p> <p>Example curl (local token file):</p> <pre><code>curl -H \"Authorization: Bearer $(sudo cat /var/lib/slicer/auth/token)\" \\\n  http://127.0.0.1:8080/metrics\n</code></pre> <p>Example curl (explicit token value):</p> <pre><code>curl -H \"Authorization: Bearer TOKEN_VALUE\" \\\n  http://127.0.0.1:8080/metrics\n</code></pre> <p>Common metric names:</p> <ul> <li><code>slicer_vm_launch_total</code> (labels: <code>host_group</code>, <code>code</code>)</li> <li><code>slicer_vm_running</code> (labels: <code>host_group</code>, <code>api</code>, <code>state</code>)</li> <li><code>slicer_vm_gpu_count</code> (labels: <code>host_group</code>)</li> <li><code>slicer_api_requests_total</code> (labels: <code>method</code>, <code>code</code>)</li> <li><code>slicer_api_request_duration_seconds</code> (labels: <code>method</code>, <code>code</code>)</li> <li><code>slicer_system_load_avg_1</code></li> <li><code>slicer_system_load_avg_5</code></li> <li><code>slicer_system_load_avg_15</code></li> <li><code>slicer_system_memory_total_bytes</code></li> <li><code>slicer_system_memory_available_bytes</code></li> <li><code>slicer_system_egress_tx</code></li> <li><code>slicer_system_egress_rx</code></li> <li><code>slicer_system_open_files</code></li> <li><code>slicer_system_open_connections</code></li> </ul> <p>Note: <code>slicer_system_egress_*</code> uses the primary egress adapter detected at runtime. You can override it with <code>SLICER_EGRESS_ADAPTER</code>.</p> <p>Prometheus scrape config (with token file):</p> <pre><code>scrape_configs:\n  - job_name: slicer\n    metrics_path: /metrics\n    scheme: http\n    bearer_token_file: /var/lib/slicer/auth/token\n    static_configs:\n      - targets:\n          - 127.0.0.1:8080\n</code></pre>"},{"location":"tasks/monitoring/#view-utilization-via-slicer-vm-top","title":"View utilization via <code>slicer vm top</code>","text":"<pre><code>$ slicer vm top\n\nHOST   IP             CPU  L1    L5    L15   MEM_USED  MEM_FREE  DISK_USED%  DISK_FREE  NET_RX/s  NET_TX/s  DR/s  DW/s  UP\nk3s-2  192.168.136.3  2    0.00  0.00  0.00  159MB     3669MB    2.2         29.2GB     0B/s      0B/s      0B/s  0B/s  7m45s\nk3s-1  192.168.136.2  2    0.00  0.00  0.00  154MB     3674MB    2.2         29.2GB     0B/s      0B/s      0B/s  0B/s  7m45s\n</code></pre> <p>Breakdown:</p> <ul> <li>CPU - vCPU allocated</li> <li>L1, L5, L15 - Load average over 1, 5 and 15 minutes</li> <li>MEM_USED - Memory used by the VM</li> <li>MEM_FREE - Memory free in the VM</li> <li>DISK_USED% - Percentage of disk used in the VM</li> <li>DISK_FREE - Disk space free in the VM</li> <li>NET_RX/s - Network received per second</li> <li>NET_TX/s - Network transmitted per second</li> <li>DR/s - Disk read per second</li> <li>DW/s - Disk write per second</li> <li>UP - Uptime of the VM</li> </ul> <p>To simulate resource usage, you could download Geekbench 6 and run a benchmark. Note that the Arm preview for Geekbench 6 may not fully complete on an Arm system. </p> <p>If slicer is running remotely:</p> <pre><code>$ slicer vm top --api http://192.168.1.114:8080\n</code></pre> <p>If slicer is running remotely but requires authentication:</p> <pre><code>$ slicer vm top --api http://192.168.1.114:8080 --token TOKEN_VALUE\n$ slicer vm top --api http://192.168.1.114:8080 --token-file TOKEN_VALUE\n</code></pre> <p>When slicer is running locally, and authentication is enabled, use <code>sudo</code> and slicer will attempt to read the auth token from the default location of <code>/var/lib/slicer/auth/token</code>.</p> <pre><code>$ sudo slicer vm top\n</code></pre>"},{"location":"tasks/monitoring/#automated-monitoring-with-node_exporter","title":"Automated monitoring with <code>node_exporter</code>","text":"<p>The Open Source node_exporter project from the Prometheus project can be used to collect system metrics from each VM, you can install the agent as a systemd unit file through userdata, or a custom base image.</p> <p>Prometheus or the Grafana Agent can then be run on the host, or somewhere else to collect the metrics and store them in a time-series database.</p>"},{"location":"tasks/nested-virtualization/","title":"Nested Virtualization with Slicer","text":"<p>Nested virtualization refers to running a virtual machine within another.</p> <p>Generally, you should always aim to run microVMs directly on bare-metal for the best performance and lowest overheads. There are a few use-cases where that is not possible, so nested virtualization provides an alternative at the trade-off of some additional latency.</p> <p>There are three main use-cases for nested virtualization with Slicer:</p> <ol> <li>Your primary OS is MacOS or Windows, which means you have to run Slicer within a Linux VM.</li> <li>You only have access to cloud VMs from DigitalOcean, Azure, or Google Cloud Platform (GCP) rather than bare-metal servers.</li> <li>You want to run Slicer within Slicer for testing and experimentation.</li> </ol> <p>For the first two use-cases, there's nothing extra for you to do. You're simply running all the Slicer commands within an existing VM.</p> <p>The rest of this guide covers the third use-case: running Slicer within Slicer. This requires attention to IP addresses and routing configurations if you want VMs to be accessible from outside the server.</p>"},{"location":"tasks/nested-virtualization/#slicer-within-slicer","title":"Slicer within Slicer","text":""},{"location":"tasks/nested-virtualization/#topology","title":"Topology","text":"<p>This guide covers the following topology. The workstation and server IPs used below are examples, replace them with the actual IPs of your machines:</p> <ul> <li>Workstation (<code>192.168.1.10</code>) - Your local machine from which you want to access VMs.</li> <li>Server (<code>192.168.1.11</code>) - A bare-metal server or VM with KVM support running Slicer (e.g., Intel N100, Intel NUC).</li> <li>slicer0 (<code>192.168.130.0/24</code>) - The Slicer instance running directly on the server.</li> <li>slicer1 (<code>192.168.137.0/24</code>) - A Slicer instance running within a slicer0 VM.</li> </ul> <p>The following diagram shows what runs on the server:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Slicer daemon \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Host Group: slicer0                     \u2502\n\u2502  CIDR: 192.168.130.0/24                  \u2502\n\u2502                                          \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Slicer VM \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  slicer0-1 (192.168.130.2)        \u2502   \u2502\n\u2502  \u2502                                   \u2502   \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Slicer daemon \u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502   \u2502\n\u2502  \u2502  \u2502  Host Group: slicer1        \u2502  \u2502   \u2502\n\u2502  \u2502  \u2502  CIDR: 192.168.137.0/24     \u2502  \u2502   \u2502\n\u2502  \u2502  \u2502                             \u2502  \u2502   \u2502\n\u2502  \u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500 Slicer VM \u2500\u2500\u2500\u2500\u2500\u2510  \u2502  \u2502   \u2502\n\u2502  \u2502  \u2502  \u2502  slicer1-1            \u2502  \u2502  \u2502   \u2502\n\u2502  \u2502  \u2502  \u2502  192.168.137.2        \u2502  \u2502  \u2502   \u2502\n\u2502  \u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u2502   \u2502\n\u2502  \u2502  \u2502                             \u2502  \u2502   \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502   \u2502\n\u2502  \u2502                                   \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502                                          \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"tasks/nested-virtualization/#prerequisites","title":"Prerequisites","text":"<ul> <li>The slicer0 instance must use a different IP range than slicer1. This example uses <code>192.168.130.0/24</code> for slicer0 and <code>192.168.137.0/24</code> (the default) for slicer1.</li> <li>Use different host group names for each Slicer instance. MAC addresses are derived from the group name, so using the same name could result in duplicate MAC addresses and network conflicts.</li> </ul>"},{"location":"tasks/nested-virtualization/#step-1-install-slicer-on-the-server","title":"Step 1: Install Slicer on the server","text":"<p>On the server, follow the instructions to install Slicer.</p> <p>Generate a configuration file with a custom CIDR:</p> <pre><code># Use this if you only want to acces via 'slicer vm shell' and not via SSH\nslicer new slicer0 --cidr 192.168.130.0/24 &gt; slicer0.yaml\n\n# Download SSH public keys from a GitHub profile\nslicer new slicer0 --cidr 192.168.130.0/24 --github alexellis &gt; slicer0.yaml\n\n# Specify an SSH public key file directly\nslicer new slicer0 --cidr 192.168.130.0/24 --ssh-key ~/.ssh/id_ed25519.pub &gt; slicer0.yaml\n</code></pre> <p>Start Slicer:</p> <pre><code>sudo slicer up ./slicer0.yaml\n</code></pre>"},{"location":"tasks/nested-virtualization/#step-2-configure-the-slicer0-vm-for-nested-virtualization","title":"Step 2: Configure the slicer0 VM for nested virtualization","text":"<p>Copy the Slicer license from the server into the slicer0 VM:</p> <pre><code>sudo slicer vm cp ~/.slicer/LICENSE slicer0-1:/home/ubuntu/.slicer/LICENSE\n</code></pre> <p>Connect to the slicer0 VM using <code>slicer vm shell</code>:</p> <pre><code>sudo slicer vm shell --uid 1000 slicer0-1\n</code></pre> <p>Enable KVM by loading the kernel modules:</p> <pre><code># For AMD CPUs\nsudo modprobe kvm &amp;&amp; sudo modprobe kvm_amd\n\n# For Intel CPUs\nsudo modprobe kvm &amp;&amp; sudo modprobe kvm_intel\n</code></pre>"},{"location":"tasks/nested-virtualization/#step-3-install-slicer-within-the-slicer0-vm","title":"Step 3: Install Slicer within the slicer0 VM","text":"<p>Perform an installation of Slicer within the slicer0 VM.</p> <p>Generate a configuration file for slicer1:</p> <pre><code># Use this if you only want to acces via 'slicer vm shell' and not via SSH\nslicer new slicer1 --cidr 192.168.137.0/24 &gt; slicer1.yaml\n\n# Download SSH public keys from a GitHub profile\nslicer new slicer1 --cidr 192.168.137.0/24 --github alexellis &gt; slicer1.yaml\n\n# Specify an SSH public key file directly\nslicer new slicer1 --cidr 192.168.137.0/24 --ssh-key ~/.ssh/id_ed25519.pub &gt; slicer1.yaml\n</code></pre> <p>Start Slicer within the slicer0 VM:</p> <pre><code>sudo slicer up ./slicer1.yaml\n</code></pre>"},{"location":"tasks/nested-virtualization/#connect-to-vms","title":"Connect to VMs","text":"<p>You can connect to using <code>slicer vm shell</code>.</p> <p>Connect to a slicer0 VM:</p> <pre><code>sudo slicer vm shell slicer0-1 --uid 1000 \n</code></pre> <p>Next run the shell command again from with the slicer0 shell to connect the any slicer1 nested VMs:</p> <pre><code>sudo slicer vm shell slicer1-1 --uid 1000\n</code></pre>"},{"location":"tasks/nested-virtualization/#routing-to-nested-vms-over-the-lan","title":"Routing to nested VMs over the LAN","text":"<p>By adding static routes you can make VMs accessible from any machine on the LAN.</p> <pre><code>\u250c\u2500\u2500 Workstation \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  192.168.1.10                      \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n                  \u2502 LAN\n                  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500 Server \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  192.168.1.11                                       \u2502\n\u2502                                                     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500 Slicer daemon \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502  Host Group: slicer0                          \u2502  \u2502\n\u2502  \u2502  CIDR: 192.168.130.0/24                       \u2502  \u2502\n\u2502  \u2502                                               \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500 Slicer VM \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  \u2502\n\u2502  \u2502  \u2502  slicer0-1 (192.168.130.2)              \u2502  \u2502  \u2502\n\u2502  \u2502  \u2502                                         \u2502  \u2502  \u2502\n\u2502  \u2502  \u2502  \u250c\u2500\u2500 Slicer daemon \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  \u2502  \u2502\n\u2502  \u2502  \u2502  \u2502  Host Group: slicer1              \u2502  \u2502  \u2502  \u2502\n\u2502  \u2502  \u2502  \u2502  CIDR: 192.168.137.0/24           \u2502  \u2502  \u2502  \u2502\n\u2502  \u2502  \u2502  \u2502                                   \u2502  \u2502  \u2502  \u2502\n\u2502  \u2502  \u2502  \u2502  \u250c\u2500\u2500 Slicer VM \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502  \u2502  \u2502  \u2502\n\u2502  \u2502  \u2502  \u2502  \u2502  slicer1-1 (192.168.137.2)  \u2502  \u2502  \u2502  \u2502  \u2502\n\u2502  \u2502  \u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u2502  \u2502  \u2502\n\u2502  \u2502  \u2502  \u2502                                   \u2502  \u2502  \u2502  \u2502\n\u2502  \u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u2502  \u2502\n\u2502  \u2502  \u2502                                         \u2502  \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502  \u2502\n\u2502  \u2502                                               \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2502                                                     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Two sets of routes are needed, one on the server and one on the workstation. In the commands below, replace <code>192.168.1.11</code> with the IP of your server running Slicer nested VMs.</p> <p>On the server</p> <p>Add a route so the server knows how to reach the nested slicer1 network through the slicer0 VM:</p> <pre><code># Route to the slicer1 VM network via the slicer0 VM\nsudo ip route add 192.168.137.0/24 via 192.168.130.2\n</code></pre> <p>On the workstation</p> <p>Add routes so the workstation knows to reach both VM networks via the server:</p> <pre><code># Route to the slicer0 VM network via the server\nsudo ip route add 192.168.130.0/24 via 192.168.1.11\n\n# Route to the slicer1 VM network via the server\nsudo ip route add 192.168.137.0/24 via 192.168.1.11\n</code></pre> <p>Both workstation routes use the server (<code>192.168.1.11</code>) as the gateway. The server then forwards traffic for the slicer1 network to the slicer0 VM (<code>192.168.130.2</code>), which forwards it to the nested VMs.</p> <p>Once routing is configured, you can access VMs directly from your workstation. For example using SSH:</p> <pre><code># Connect to a slicer0 VM\nssh ubuntu@192.168.130.2\n\n# Connect to a slicer1 VM\nssh ubuntu@192.168.137.2\n</code></pre> <p>Any other network traffic works the same way. For example, if a slicer1 VM runs a web server on port 8080, you can reach it directly from the workstation at <code>http://192.168.137.2:8080</code>.</p>"},{"location":"tasks/pvm/","title":"Run slicer without KVM on cloud VMs","text":"<p>PVM (Pagetable Virtual Machine) enables Slicer microVMs to run on cloud VMs without nested virtualization enabled or missing the <code>/dev/kvm</code> device.</p> <p>This is particularly useful for AWS EC2 instances, where bare-metal options are significantly more expensive than regular VMs, or other cloud providers where nested virtualization isn't available.</p> <p>Architecture Support</p> <p>PVM currently supports x86_64 architecture only. Arm64 support is not available.</p> <p>Running Slicer with PVM requires a custom kernel on the host and compatible guest images. Additionally, a patched Firecracker version is require to support PVM.</p>"},{"location":"tasks/pvm/#install-pvm-kernel-on-the-host","title":"Install PVM kernel on the host","text":"<p>Create a new VM instance on your cloud and select Ubuntu 24.04 as the operating system.</p> <p>PVM has been tested on Ubuntu 24.04 on Amazon EC2 and Hetzner Cloud.</p> <p>SSH into the host and install the pre-built PVM kernel packages.</p> <p>Install arkade if it is not on your system already:</p> <pre><code>curl -sLS https://get.arkade.dev | sudo sh\n</code></pre> <p>Download the kernel packages:</p> <pre><code>arkade oci install ghcr.io/openfaasltd/actuated-kernel-pvm-host:x86_64-latest \\\n  --path .\n</code></pre> <p>Install the downloaded packages:</p> <pre><code>sudo dpkg -i *.deb\n</code></pre> <p>Configure the kernel boot parameters. PVM requires Page Table Isolation (PTI) to be disabled:</p> <pre><code># Configure GRUB to append pti=off to existing kernel parameters\nsudo sed -i '/^GRUB_CMDLINE_LINUX_DEFAULT=/ { s/pti=off//g; s/\"$/ pti=off\"/; s/  */ /g }' /etc/default/grub\nsudo sed -i '/^GRUB_CMDLINE_LINUX=/ { s/pti=off//g; s/\"$/ pti=off\"/; s/  */ /g }' /etc/default/grub\n</code></pre> <p>Configure GRUB to use the new kernel:</p> <pre><code># Enable an override to the saved default\nsudo sed -i 's/^GRUB_DEFAULT=.*/GRUB_DEFAULT=saved/' /etc/default/grub\nsudo update-grub\n\n# View available kernel options\nsudo grep -n \"menuentry '\" /boot/grub/grub.cfg | sed \"s/.*menuentry '\\(.*\\)'.*/\\1/\"\n\n# Set the new PVM kernel as default\nsudo grub-set-default \"Advanced options for Ubuntu&gt;Ubuntu, with Linux 6.12.33\"\nsudo update-grub\n\n# Update all initramfs images\nsudo update-initramfs -u -k all\n</code></pre> <p>Reboot the system to boot into the PVM kernel:</p> <pre><code>sudo reboot\n</code></pre> <p>After reboot, verify the new kernel is running and load the PVM module:</p> <pre><code># Verify PVM kernel is active\nuname -a\n\n# Load the PVM kernel module\nsudo modprobe kvm_pvm\n</code></pre> <p>You can also use <code>lsmod</code> to verify the kvm_pvm module is loaded:</p> <pre><code>Module                  Size  Used by\nkvm_pvm                53248  0\nkvm                  1400832  1 kvm_pvm\n</code></pre>"},{"location":"tasks/pvm/#install-slicer","title":"Install Slicer","text":"<p>Follow the standard installation instructions for slicer. The installation script automatically downloads a forked Firecracker binary with PVM support.</p> <p>When creating VMs on a PVM-enabled host, use the PVM-compatible base image.</p> <p>Update the <code>image</code> field in slicer config files:</p> <pre><code>  image: \"ghcr.io/openfaasltd/slicer-systemd-2204-pvm:x86_64-latest\"\n</code></pre> <p>Note that at the moment PVM is only supported when using Firecracker as the hypervisor.</p> <p>Ensure the correct hypervisor is selected in slicer config files:</p> <pre><code>  hypervisor: \"firecracker\"\n</code></pre>"},{"location":"tasks/pvm/#performance-considerations","title":"Performance considerations","text":"<p>PVM adds virtualization overhead compared to bare-metal KVM. Based on testing:</p> <ul> <li>I/O intensive workloads may see significant performance impact</li> <li>CPU-bound tasks typically see lower overhead</li> <li>Long-running services and HTTP workloads are less affected</li> <li>Cold start times remain fast</li> </ul> <p>For optimal performance, consider bare-metal hosts with hardware virtualization when possible. PVM is ideal when bare-metal isn't available or cost-effective.</p>"},{"location":"tasks/share-files-with-nfs/","title":"Share files between host and VM with NFS","text":"<p>Neither Firecracker nor Cloud Hypervisor support \"shared folders\" natively, but it is on the immediate roadmap to add this capability to the <code>slicer-agent</code>.</p> <p>For the time being, one of the most convenient options is to use NFS (Network File System) to share files between your host machine and Slicer VMs. This is particularly useful for development workflows, data processing, or when you need persistent storage that survives VM restarts.</p> <p>You should also consider <code>slicer cp</code> as an alternative to NFS, along with a self-hosted S3 server and SSH-based solutions like sshfs, sftp, scp and restic.</p> <p>This example shows you how to set up an NFS server on your host machine and mount it from within a Slicer VM.</p>"},{"location":"tasks/share-files-with-nfs/#prerequisites","title":"Prerequisites","text":"<p>You'll need a running Slicer VM. Follow the walkthrough to create one if you haven't already.</p>"},{"location":"tasks/share-files-with-nfs/#set-up-the-nfs-server-on-the-host","title":"Set up the NFS server on the host","text":"<p>Install the NFS kernel server on your host machine:</p> <pre><code>sudo apt update &amp;&amp; sudo apt install -y nfs-kernel-server\n</code></pre> <p>Create a directory to share with your VMs:</p> <pre><code>sudo mkdir -p /srv/slicer_share\nsudo chown nobody:nogroup /srv/slicer_share\n</code></pre> <p>Configure the NFS exports by editing <code>/etc/exports</code>:</p> <pre><code>sudo nano /etc/exports\n</code></pre> <p>Add the following line to allow your Slicer network access to the share. Replace the network CIDR with your actual Slicer network configuration (the default usded in the walkthrough is <code>192.168.137.0/24</code>):</p> <pre><code>/srv/slicer_share 192.168.137.0/24(rw,sync,no_subtree_check)\n</code></pre> <p>Apply the configuration changes:</p> <pre><code>sudo exportfs -ra\n</code></pre> <p>Start and enable the NFS server:</p> <pre><code>sudo systemctl enable --now nfs-server\n</code></pre> <p>You can verify the export is active:</p> <pre><code>sudo exportfs -v\n</code></pre>"},{"location":"tasks/share-files-with-nfs/#mount-the-nfs-share-in-your-vm","title":"Mount the NFS share in your VM","text":"<p>This section provides manual setup instructions. For an automated setup using userdate, see Automate NFS setup with userdata</p> <p>Connect to your VM via SSH:</p> <pre><code>ssh ubuntu@192.168.137.2\n</code></pre> <p>Inside the VM, install the NFS client utilities:</p> <pre><code>sudo apt update &amp;&amp; sudo apt install -y nfs-common\n</code></pre> <p>Create a mount point:</p> <pre><code>sudo mkdir -p /mnt/slicer_share\n</code></pre> <p>Mount the NFS share from your host (replace <code>192.168.137.1</code> with your actual host IP if different):</p> <pre><code>sudo mount -t nfs 192.168.137.1:/srv/slicer_share /mnt/slicer_share\n</code></pre> <p>Test the connection by creating a file:</p> <pre><code>echo \"Hello from VM!\" | sudo tee /mnt/slicer_share/test.txt\n</code></pre> <p>You should be able to see this file on your host at <code>/srv/slicer_share/test.txt</code>.</p>"},{"location":"tasks/share-files-with-nfs/#make-the-mount-persistent","title":"Make the mount persistent","text":"<p>To automatically mount the NFS share when the VM boots, add it to <code>/etc/fstab</code>:</p> <pre><code>echo \"192.168.137.1:/srv/slicer_share /mnt/slicer_share nfs defaults 0 0\" | sudo tee -a /etc/fstab\n</code></pre> <p>Test the fstab entry:</p> <pre><code>sudo umount /mnt/slicer_share\nsudo mount -a\n</code></pre>"},{"location":"tasks/share-files-with-nfs/#automate-nfs-setup-with-userdata","title":"Automate NFS setup with userdata","text":"<p>You can automate the NFS client setup by adding userdata to your VM configuration.</p> <pre><code>config:\n  host_groups:\n  - name: vm\n    userdata: |\n      #!/bin/bash\n      # Install NFS client\n      apt update &amp;&amp; apt install -y nfs-common\n\n      # Create mount point\n      mkdir -p /mnt/slicer_share\n\n      # Add to fstab for persistent mounting\n      echo \"192.168.137.1:/srv/slicer_share /mnt/slicer_share nfs defaults 0 0\" &gt;&gt; /etc/fstab\n\n      # Mount immediately\n      mount -a\n</code></pre> <p>With this configuration, you can increase the VM count in your host group and all VMs will automatically have the NFS share mounted.</p>"},{"location":"tasks/userdata/","title":"Userdata for Slicer VMs","text":"<p>Userdata can be used to customise VMs on first boot. Another option is to build a custom image.</p> <p>This can include anything from installing packages, configuring services, or setting up user accounts.</p> <p>If you wanted to run AI agents within Slicer, you could use userdata to install a custom AI framework, library, or agent for Remote Procedure Control (RPC).</p>"},{"location":"tasks/userdata/#inline-userdata-in-the-config-file","title":"Inline userdata in the config file","text":"<pre><code>config:\n  host_groups:\n  - name: agent\n+   userdata: |\n+     # Install Ollama\n+     curl -fsSL https://ollama.com/install.sh | sh\n+\n+     # Install a hard-coded SSH key for remote administration\n+\n+     echo \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC3... user@host\" | tee -a /home/ubuntu/.ssh/authorized_keys\n+     chmod 600 /home/ubuntu/.ssh/authorized_keys\n+     chown ubuntu:ubuntu /home/ubuntu/.ssh/authorized_keys\n</code></pre> <p>You could also install something like Docker like this:</p> <pre><code>config:\n  host_groups:\n  - name: agent\n+   userdata: |\n+     # Install Docker\n+     curl -fsSL https://get.docker.com | sh\n</code></pre>"},{"location":"tasks/userdata/#userdata-via-a-local-file","title":"Userdata via a local file","text":"<p>Instead of inlining long scripts in the config file, you can reference a local file.</p> <pre><code>config:\n  host_groups:\n  - name: agent\n+   userdata_file: ./userdata.sh\n</code></pre> <p>Then simply write the script in <code>userdata.sh</code>, here's one to set up Docker:</p> <pre><code>#!/bin/bash\n# Install Docker\ncurl -fsSL https://get.docker.com | sh\n# Add user to docker group\nusermod -aG docker ubuntu\n</code></pre>"},{"location":"tasks/userdata/#userdata-via-the-cli","title":"Userdata via the CLI","text":"<p>You can add and remove VMs via Slicer's API directly using HTTP, or using the Slicer CLI itself as a client.</p> <p>If you had a hostgroup named <code>agents</code>, you could run:</p> <pre><code>USERDATA=$(echo '# Install Ollama\\ncurl -fsSL https://ollama.com/install.sh | sh' | jq -Rs .)\n\ncurl -sLSf http://127.0.0.1:8080/hostgroup/agents/nodes \\\n    -H \"Content-Type: application/json\" \\\n    -d \"{\n      \\\"userdata\\\": $USERDATA\n    }\"\n</code></pre> <p>For longer userdata scripts, you can save them to a file and reference it:</p> <pre><code># Create userdata.sh\ncat &gt; userdata.sh &lt;&lt; EOF\n#!/bin/bash\n\n# Install Docker\ncurl -fsSL https://get.docker.com | sh\n\n# Add user to docker group\nusermod -aG docker ubuntu\nEOF\n\n# Use the file in your API call\ncurl -sLSf http://127.0.0.1:8080/hostgroup/agents/nodes \\\n    -H \"Content-Type: application/json\" \\\n    -d \"{\n      \\\"userdata\\\": $(cat userdata.sh | jq -Rs .)\n    }\"\n</code></pre> <p>Or you can use the CLI for more convenience:</p> <pre><code>sudo slicer vm add \\\n    --userdata ./userdata.sh\n</code></pre>"}]}